{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"posts","text":"<p>Subscribe to a digest of social &amp; writing content (preview)</p>"},{"location":"welcome/","title":"home","text":"<p>Welcome to my digital garden.</p> <p>This will include content I frequently update or take notes on. It's unrefined and in process.</p> <p>I seperated this out from my main blog at sheldonhull.com to allow for more organic updates in my daily workflow.</p> <p>Subscribe to a weekly digest of social &amp; writing content (preview)</p>"},{"location":"career/communication/","title":"Communication","text":"<p>Selling Past the Close</p> <p>\"Don't sell past the close\" simply means that, once the customer has agreed to the points being made, the person or team making the pitch should immediately move onto the next aspect of the deal. [Don't sell past the close][dont-sell-past-the-close]</p> <p>This is great, as I've found trying to win someone and finally having a concession can result in continuing the \"sell\" beyond. In an effort to convince, it's important to ensure you accept a close and move on.</p> <p>Reflective Listening<sup>1</sup></p> <p>The skill of reflective listening involves two steps:</p> <p>Understand what the speaker is saying, and then repeat it back to them to confirm you understood it correctly.</p> <ol> <li> <p>See Reflection Section \u21a9</p> </li> </ol>","tags":["communication"]},{"location":"career/engineering-culture/","title":"Engineering Culture","text":"","tags":["career","culture"]},{"location":"career/engineering-culture/#reading","title":"Reading","text":"<ul> <li>The Pragmatic Engineer Test: 12 Questions on Engineering Culture - The Pragmatic Engineer</li> <li>A Software Engineering Culture Test - The Pragmatic Engineer</li> <li>The Joel Test: 12 Steps to Better Code - Joel on Software</li> </ul>","tags":["career","culture"]},{"location":"career/engineering-culture/#things-to-look-into-writing-on-here-or-on-blog","title":"Things To Look Into Writing On Here or On Blog","text":"<ul> <li> Balancing \"Guardrails\" with \"Autonomy\" on an organizational level</li> <li> Applying SRE Principles - I did write on this!</li> </ul>","tags":["career","culture"]},{"location":"career/eponymous-laws/","title":"Eponymous Laws","text":""},{"location":"career/eponymous-laws/#links","title":"Links","text":"<ul> <li>The Eponymous Laws of Tech - daverupert.com</li> </ul>"},{"location":"career/management-dynamics/","title":"Management Dynamics","text":"","tags":["management","career","communication"]},{"location":"career/management-dynamics/#reading","title":"Reading","text":"Reading Why Help Your Boss Help You Assuming the motivations and purpose of your management is the same as you as an engineer is wrong. An interesting read for understanding more the motivations and factors that go into management dynamics.","tags":["management","career","communication"]},{"location":"career/management-dynamics/#notes-help-your-boss-help-you","title":"Notes - Help Your Boss Help You","text":"<p>It's an interesting book, especially as the world of management is far different than the world an engineer is immersed in.</p> <p>Overall, I found the book interesting albeit biased in discussions towards traditional corporate management structure. Since this is often the environment enterprise development operates in, some of these generalization are useful for thinking differently when things aren't clicking.</p> <p>Proposals - Introverts vs Sensors <sup>1</sup></p> <p>From Help Your Boss Help You:</p> <p>Introverts appreciate having time to prepare themselves for interpersonal interactions, even if it's just a few moments. Extraverts look for opportunities to talk to someone directly.</p> <p>Likewise, an extraverted manager will want to call a meeting to resolve any issue, which gets everyone together to talk. Introverts prefer to set up a wiki, with the goal of avoiding any interpersonal contact entirely.</p> <p>Again, that is an exaggeration. Most people do not fall on one extreme or another, but the extreme cases help define the difference in preferences and are intended to make the actual situations easier to diagnose.<sup>1</sup></p> <p>This means the order in which you present a new proposal is reversed.</p> <ul> <li>Sensor: You focus on the aging stack, the steps to achieve migration, and only talk through big picture if it was asked for.</li> <li>Intuitive: You'd focus on the big picture and then individual steps.</li> </ul> <p>Proposals - Feeling vs Thinking</p> <p>Lead with facts or a demo in general. One is impression based, and the other is more dealing with whitepapers and facts.</p> <p>Proposals - Judging Vs Perceiving</p> <p>Of all the Myers Brigg values, this one is furthest from the actual word definitions we expect.</p> <p>If the thinking versus feeling scale involves how people make decisions, the judging versus perceiving scale measures when they prefer to make decisions. A person with a strong judging preference prefers to finish existing tasks, while a person with a strong perceiving preference enjoys starting new tasks.<sup>1</sup></p> <p>The Keirsey Temperament Sorter<sup>1</sup></p> <p>There's some interesting information about the various temperaments: Guardians, Rationals, Idealists, and Artisans.</p> <p>Generalizing:</p> <ul> <li>Guardians: checklist oriented.   Can ask nebelous questions, which shouldn't be disregarded, instead still make an effort (timeboxed) to resolve quickly.   Until requested, consider minimizing the effort until it's recognized as worth it. (I can tend to way overthink the effort expected)</li> <li>Rationals: Systems thinkers with a strong desire for process improvement, optimization, and improving architecture.</li> <li>Idealists: values harmony with a tendency to dislike conflict.</li> <li>Artisans: tend to try and enjoy work, optimistic, spontaneous. Tend to not be as prevelant in traditional enterprise worlds due to the lack of compability outside roles like sales, marketing, etc.</li> </ul> <p>The difficulty when dealing with an Idealist is that they want to feel inspired by whatever they're working on, and if they aren\u2019t, they have a hard time staying motivated.</p> <ul> <li>A Guardian has a job to do and does it.</li> <li>A Rational likes designing and optimizing the systems that make up the job.</li> <li>An Idealist has to feel that what they're doing matters, either for themselves or for others, or they're not interested.</li> </ul> <ol> <li> <p>See Reflection Section \u21a9\u21a9\u21a9\u21a9</p> </li> </ol>","tags":["management","career","communication"]},{"location":"development/","title":"development","text":"<p>Content related to various programming languages and their toolchains.</p> Language Thoughts Go When you like checking errors, no syntactic sugar, and want to be your own car mechanic in code.<sup>1</sup> PowerShell When you get tired of parsing text as strings and want to move to the 21st century with objects \ud83d\ude00 Shell When you need some adhoc magic that you don't care about testing, cross-platform consistency, and readability <ol> <li> <p>Good luck finding an \"is-even\" package in the Go ecosystem. No hand holding here!\u00a0\u21a9</p> </li> </ol>"},{"location":"development/level-up/","title":"Level-Up","text":""},{"location":"development/level-up/#languages","title":"Languages","text":"<p>Tracking my progress.</p> <ul> <li> VBA<sup>1</sup></li> <li> SQL</li> <li> Batch</li> <li> PowerShell<ul> <li> pester/TDD</li> <li> module creation</li> <li> DSC</li> </ul> </li> <li> C#<ul> <li> exercism course finished</li> <li> pulumi<ul> <li> basic c# deployment</li> <li> moderately complex c# deployment</li> </ul> </li> <li> concepts<ul> <li> grok async/await</li> </ul> </li> <li> get language server working in jetbrains and Visual Studio Code</li> </ul> </li> <li> Python<ul> <li> Pandas &amp; Datascience</li> <li> Build a web server</li> <li> Build a TUI</li> </ul> </li> <li> Go<ul> <li> Automation &amp; DevOps</li> <li> Lambda/Serverless</li> <li> Charm TUI<ul> <li> Partial components</li> <li> Full reactive \"Elm\" based design</li> </ul> </li> <li> CLI helper tools</li> <li> DDD (Domain Driven Design) for a complex project</li> <li> API<ul> <li> design versioning support (such as v1/v2 running concurrently)</li> </ul> </li> <li> concurrency<ul> <li> expertise in goroutines</li> <li> expertise in channels</li> </ul> </li> <li> observability<ul> <li> leverage OTEL</li> <li> logging with context</li> <li> logging middleware</li> </ul> </li> <li> slackbot<ul> <li> build a slackbot</li> <li> figure out interaction model for UI elements</li> <li> openai integration</li> <li> slash command</li> </ul> </li> </ul> </li> <li> Kubernetes Operator<ul> <li> contribute to one<sup>3</sup></li> <li> build one from scratch</li> <li> build one that interacts with more security lifecycle operations</li> </ul> </li> <li> Shell<sup>2</sup></li> <li> Rust - someday!</li> <li> Zig</li> </ul> things I've decided to archive <p>Things I'll wait on, or am not as interested in anymore.</p> <ul> <li> Typescript</li> <li> Build a raycast extension</li> <li> Build a VSCode extension</li> </ul>"},{"location":"development/level-up/#infra","title":"Infra","text":"<ul> <li> Use Pulumi to deploy an entire cluster with backups and everything from zero.</li> <li> Pulumi kubernetes fully done in apis (no calling helm)</li> </ul>"},{"location":"development/level-up/#knowledge-roadmap","title":"Knowledge Roadmap","text":"<ul> <li> Microservices</li> <li> DDD</li> <li> TDD (partially completed)</li> </ul> <ol> <li> <p>We all start somewhere!\u00a0\u21a9</p> </li> <li> <p>I don't write many bash scripts though, prefer Go/PowerShell for complex stuff.\u00a0\u21a9</p> </li> <li> <p>dsv-k8s and dsv-k8s-sidecar public projects on github \ud83c\udf89\u00a0\u21a9</p> </li> </ol>"},{"location":"development/ai/","title":"AI","text":"<p>For tracking machine learning, AI, and other bits of interesting stuff on this popular topic.</p>"},{"location":"development/ai/chat/","title":"chat","text":"","tags":["chatgpt","coding"]},{"location":"development/ai/chat/#examples-where-chatgpt-failed","title":"Examples Where ChatGPT Failed","text":"Help Me With Using Goldmark For Markdown Parsing <p>This failed repeatedly. The code examples were promising but made up quite a few methods and approaches that were non-existent in the actual code base and had no code examples matching in the repo. Goldmark doesn't have a lot of use examples compared to many projects, so I think the quality of the suggestions degraded with a broader question.</p> <p>It looked great, but with improper method signatures it really suffered.</p> <p></p> <p>I think for this to succeed I'm going to have to it focus on a specific code block and not a function in it's entirety.</p>","tags":["chatgpt","coding"]},{"location":"development/ai/chat/#examples-where-chatgpt-has-been-amazing","title":"Examples Where ChatGPT Has Been Amazing","text":"<p>\ud83d\udc49 See anonymous server</p> Help Me Jump Start an API Server With TDD Support and Zerolog <p>What's a good bootstrap template for golang rest api server. I want to include TDD approach that abstracts the http server into a cmd file and allows me to mock http for tests. Use zerolog for logging with context.</p> <p>Key points I think this did well with.</p> <ol> <li>It structured it reasonsably.</li> <li>It provided a jump start test file to do TDD with based on <code>httptest</code>.</li> <li>It was intelligent enough to add <code>zerolog</code> url handling logging using my preferred package.</li> </ol> <p>I could see how this could really help jump start intelligently templating out a project much better than just raw boilerplate.</p> <p>Instead of having to maintain cookiecutter style templates, you might see the future allowing being really specific and generating customized code along with infra to support a quicker iteration on development. Of course, everything should be vetted and scanned, but it's a pretty incredible experience and worth using copilot/chatGPT to occasionally accelerate development.</p> <p>ChatGPT's answer:</p> <p>There are several good bootstrap templates available for Go REST API servers. One popular option is the Go kit template, which provides a clean architecture for building microservices in Go. Another option is the Gin Gonic template, which is a lightweight and fast HTTP web framework. For your specific requirements, I would recommend using the Gin Gonic template, which provides a simple and easy-to-use web framework. You can use the gin package to create an HTTP server and handle requests, and use the zerolog package to log requests and responses.</p> <p>Here's an example of how you can structure your project using the Gin Gonic template and implement TDD:</p> <ol> <li>Start by creating a new project using the Gin Gonic template: <code>go get github.com/gin-gonic/gin</code></li> <li>Create a new cmd directory in your project root and create a main.go file inside it:</li> <li>This file sets up a basic HTTP server using the <code>api.NewServer</code> function, which we'll define in the next step.    It also initializes a logger using the <code>logging.NewLogger</code> function.</li> <li>Create a new api directory and create a <code>server.go</code> file inside it.    This file defines a Server struct that holds a reference to our logger.    We'll use this struct to handle HTTP requests in the ServeHTTP method.</li> <li>Now you can implement your TDD approach by writing tests and mocking the HTTP server using the <code>httptest</code> package:</li> <li>This test creates a new instance of our Server and mocks an HTTP request using the <code>httptest</code> package.    It then calls the <code>ServeHTTP</code> method and checks that the response status code is <code>200</code> OK.</li> <li>Finally, you can use the zerolog package to log requests and responses in the ServeHTTP method.</li> </ol> cmd/main.goapi/server.goapi/server_test.go cmd/main.go<pre><code>package main\n\nimport (\n    \"net/http\"\n    \"os\"\n    \"your_project/api\"\n    \"your_project/logging\"\n)\n\nfunc main() {\n  logger := logging.NewLogger()\n  defer logger.Sync()\n\n  // Initialize the HTTP server\n  server := api.NewServer(logger)\n\n  // Start the server\n  port := os.Getenv(\"PORT\")\n  if port == \"\" {\n      port = \"8080\"\n  }\n\n  logger.Info().Msgf(\"Starting server on port %s...\", port)\n  if err := http.ListenAndServe(\":\"+port, server); err != nil {\n      logger.Fatal().Err(err).Msg(\"Failed to start server\")\n  }\n}\n</code></pre> api/server.go<pre><code>package api\n\nimport (\n    \"net/http\"\n    \"your_project/logging\"\n)\n\ntype Server struct {\n    logger *logging.Logger\n}\n\nfunc NewServer(logger *logging.Logger) *Server {\n    return &amp;Server{logger: logger}\n}\n\nfunc (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    // Handle the request\n}\n\nfunc (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    s.logger.Info().\n        Str(\"method\", r.Method).\n        Str(\"path\", r.URL.Path).\n}\n</code></pre> api/server_test.go<pre><code>package api_test\n\nimport (\n    \"net/http/httptest\"\n    \"testing\"\n    \"your_project/api\"\n    \"your_project/logging\"\n)\n\nfunc TestServer_ServeHTTP(t *testing.T) {\n    logger := logging.NewLogger()\n    defer logger.Sync()\n\n    server := api.NewServer(logger)\n\n    req := httptest.NewRequest(\"GET\", \"/\", nil)\n    w := httptest.NewRecorder()\n\n    server.ServeHTTP(w, req)\n\n    resp := w.Result()\n    if resp.StatusCode != http.StatusOK {\n        t.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n    }\n}\n</code></pre>","tags":["chatgpt","coding"]},{"location":"development/ai/copilot/","title":"copilot","text":"","tags":["copilot","coding"]},{"location":"development/ai/copilot/#examples-where-copilot-failed","title":"Examples Where Copilot Failed","text":"","tags":["copilot","coding"]},{"location":"development/ai/copilot/#examples-where-copilot-has-been-amazing","title":"Examples Where Copilot Has Been Amazing","text":"ASCII color codes in bash scripts <p>This has always messed with me, and I spend more time trying to find the pattern of \"background is blue with white text and bold\" to add a little bit of polish. Copilot nailed this with a comment in a <code>.envrc</code> file stating: <code># variable for setting terminal output with light yellow background and black text that is bold</code> resulting in a perfect match.</p> <p></p>","tags":["copilot","coding"]},{"location":"development/aws/","title":"AWS","text":""},{"location":"development/aws/aws-cli/","title":"aws-cli","text":""},{"location":"development/aws/aws-cli/#aws-cli-metadata","title":"AWS CLI &amp; Metadata","text":""},{"location":"development/aws/aws-cli/#retrieve-instance-region","title":"Retrieve Instance Region","text":"<p>Looks like the metadata service uses tokens now, so this requires an additional step.</p> <pre><code>TOKEN=`curl --silent --show-error --fail -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nRESPONSE=`curl --silent  --show-error --fail -H \"X-aws-ec2-metadata-token: $TOKEN\"  http://169.254.169.254/latest/meta-data/placement/region`\necho \"Current Region is: [$RESPONSE]\"\n</code></pre>"},{"location":"development/aws/aws-cli/#list-matching-instances","title":"List Matching Instances","text":"<p>You can use output with <code>--output text</code> but for this example I used json and <code>jq</code>.</p> <pre><code>aws ec2 describe-instances --filters \"Name=tag:Name,Values={{ .EC2_NAME_FILTER }}\" --output json \\\n--query 'Reservations[*].Instances[*].{Instance:InstanceId}' | jq --compact-output '.[][].Instance'\n</code></pre>"},{"location":"development/aws/aws-cli/#list-standard-users","title":"List Standard Users","text":"<pre><code>getent passwd {1000..60000}\n</code></pre>"},{"location":"development/aws/aws-powershell/","title":"AWS PowerShell","text":""},{"location":"development/aws/aws-powershell/#aws-tools","title":"AWS Tools","text":""},{"location":"development/aws/aws-powershell/#install-awstools","title":"Install AWS.Tools","text":"<p>Going forward, use AWS.Tools modules for newer development. It's much faster to import and definitely a better development experience in alignment with .NET SDK namespace approach.</p> <p>Use their installer module to simplify versioning and avoid conflicts with automatic cleanup of prior SDK versions.</p> <pre><code>install-module 'AWS.Tools.Installer' -Scope CurrentUser\n\n$modules = @(\n    'AWS.Tools.Common'\n    'AWS.Tools.CostExplorer'\n    'AWS.Tools.EC2'\n    'AWS.Tools.Installer'\n    'AWS.Tools.RDS'\n    'AWS.Tools.S3'\n    'AWS.Tools.SecretsManager'\n    'AWS.Tools.SecurityToken'\n    'AWS.Tools.SimpleSystemsManagement'\n)\n\nInstall-AWSToolsModule $modules -Cleanup -Force\n</code></pre>"},{"location":"development/aws/aws-powershell/#using-systems-manager-parameters-ssm-to-create-a-pscredential","title":"Using Systems Manager Parameters (SSM) To Create A PSCredential","text":"<pre><code>$script:SqlLoginName = (Get-SSMParameterValue -Name $SSMParamLogin -WithDecryption $true).Parameters[0].Value\n$script:SqlPassword = (Get-SSMParameterValue -Name $SSMParamPassword -WithDecryption $true).Parameters[0].Value | ConvertTo-SecureString -AsPlainText -Force\n$script:SqlCredential = [pscredential]::new($script:SqlLoginName, $script:SqlPassword)\n</code></pre>"},{"location":"development/aws/aws-powershell/#using-aws-secrets-manager-to-create-a-pscredential","title":"Using AWS Secrets Manager To Create a PSCredential","text":"<p>Note that this can vary in how you read it based on the format. The normal format for entries like databases seems to be: <code>{\"username\":\"password\"}</code> or similar.</p> <pre><code>$Secret = Get-SECSecretValue -SecretId 'service-accounts/my-secret-id' -ProfileName $ProfileName\n</code></pre>"},{"location":"development/aws/aws-powershell/#generate-a-temporary-key","title":"Generate a Temporary Key","text":"<p>Useful for needing to generate some time sensitive access credentials when connected via SSM Session and needing to access another account's resources.</p> <pre><code>Import-Module aws.tools.common, aws.tools.SecurityToken\nSet-AWSCredential -ProfileName 'ProfileName' -scope Global\n$cred = Get-STSSessionToken -DurationInSeconds ([timespan]::FromHours(8).TotalSeconds)\n@\"\n`$ENV:AWS_ACCESS_KEY_ID = '$($cred.AccessKeyId)'\n`$ENV:AWS_SECRET_ACCESS_KEY = '$($cred.SecretAccessKey)'\n`$ENV:AWS_SESSION_TOKEN  = '$($cred.SessionToken)'\n\"@\n</code></pre>"},{"location":"development/aws/aws-powershell/#install-ssm-agent-manually","title":"Install SSM Agent Manually","text":"<p>This is based on the AWS install commands, but with a few enhancements to better work on older Windows servers.</p> <pre><code># https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-win.html\n$ProgressPreference = 'SilentlyContinue'\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\nWrite-Host \"Downloading installer\"\n$InstallerFile = Join-Path $env:USERPROFILE 'Downloads\\SSMAgent_latest.exe'\n$invokeWebRequestSplat = @{\n    Uri = 'https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/windows_amd64/AmazonSSMAgentSetup.exe'\n    OutFile = $InstallerFile\n}\nInvoke-WebRequest @invokeWebRequestSplat\n\nWrite-Host \"Installing SSM Agent\"\n$startProcessSplat = @{\n    FilePath     = $InstallerFile\n    ArgumentList = '/S'\n}\nStart-Process @startProcessSplat\n\nWrite-Host \"Cleaning up ssmagent download\"\nRemove-Item $InstallerFile -Force\nRestart-Service AmazonSSMAgent\n</code></pre>"},{"location":"development/aws/aws-powershell/#aws-powershell-specific-cheatsheets","title":"AWS PowerShell Specific Cheatsheets","text":""},{"location":"development/aws/s5cmd/","title":"Syncing Files Using S5Cmd","text":""},{"location":"development/aws/s5cmd/#syncing-files-using-s5cmd","title":"Syncing Files Using S5Cmd","text":""},{"location":"development/aws/s5cmd/#get-s5cmd","title":"Get S5Cmd","text":"<p>Older versions of PowerShell (4.0) and the older AWSTools don't have the required ability to sync down a folder from a key prefix. This also performs much more quickly for a quick sync of files like backups to a local directory for initiating restores against.</p> <p>In testing down by the tool creator, they showed this could saturate a network for an EC2 with full download speed and be up to 40x faster than using CLI with the benefit of being a small self-contained Go binary as well.</p> <p>Once the download of the tooling is complete you can run a copy from s3 down to a local directory by doing something similar to this, assuming you have rights to the bucket. If you don't have rights, you'll want to set environment variables for the access key and secret key such as:</p> <pre><code>Import-Module aws.tools.common, aws.tools.SecurityToken\nSet-AWSCredential -ProfileName 'MyProfileName' -Scope Global\n$cred = Get-STSSessionToken -DurationInSeconds ([timespan]::FromHours(8).TotalSeconds)\n@\"\n`$ENV:AWS_ACCESS_KEY_ID = '$($cred.AccessKeyId)'\n`$ENV:AWS_SECRET_ACCESS_KEY = '$($cred.SecretAccessKey)'\n`$ENV:AWS_SESSION_TOKEN  = '$($cred.SessionToken)'\n\"@\n</code></pre> <p>You can copy that string into your remote session to get the access tokens recognized by the s5cmd tool and allow you to grab files from another AWS account S3 bucket.</p> <p>NOTE: To sync a full \"directory\" in s3, you need to leave the asteriks at the end of the key as demonstrated.</p>"},{"location":"development/aws/s5cmd/#windows","title":"Windows","text":""},{"location":"development/aws/s5cmd/#install-s5cmd-for-windows","title":"Install S5Cmd For Windows","text":"<pre><code>[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n$ToolsDir = 'C:\\tools'\n$ProgressPreference = 'SilentlyContinue'\n$OutZip = Join-Path $ToolsDir 's5cmd.zip'\nInvoke-WebRequest -Uri 'https://github.com/peak/s5cmd/releases/download/v1.2.1/s5cmd_1.2.1_Windows-64bit.zip' -UseBasicParsing -OutFile $OutZip\n\n# Not available on 4.0 Expand-Archive $OutZip -DestinationPath $ToolsDir\n\nAdd-Type -assembly 'system.io.compression.filesystem'\n[io.compression.zipfile]::ExtractToDirectory($OutZip, $ToolsDir)\n$s5cmd = Join-Path $ToolsDir 's5cmd.exe'\n&amp;$s5cmd version\n</code></pre>"},{"location":"development/aws/s5cmd/#windows-sync-a-directory-from-to-local","title":"Windows Sync A Directory From To Local","text":"<pre><code>$ErrorActionPreference = 'Stop'\n$BucketName = ''\n$KeyPrefix = 'mykeyprefix/anothersubkey/*'\n\n$Directory = \"C:\\temp\\adhoc-s3-sync-$(Get-Date -Format 'yyyy-MM-dd')\\$KeyPrefix\"\nNew-Item $Directory -ItemType Directory -Force -ErrorAction SilentlyContinue\n$s5cmd = Join-Path $ToolsDir 's5cmd.exe'\nWrite-Host \"This is what is going to run: `n&amp;$s5cmd cp `\"s3://$bucketname/$KeyPrefix`\" $Directory\"\nRead-Host 'Enter to continue if this makes sense, or cancel (ctrl+c)'\n\n\n&amp;$s5cmd cp \"s3://$bucketname/$KeyPrefix\" $Directory\n</code></pre>"},{"location":"development/build-release-engineering/","title":"Build &amp; Release Engineering","text":""},{"location":"development/build-release-engineering/azure-pipelines/","title":"Azure Pipelines","text":""},{"location":"development/build-release-engineering/azure-pipelines/#azure-pipelines-tips","title":"Azure Pipelines Tips","text":""},{"location":"development/build-release-engineering/azure-pipelines/#dynamic-link-to-a-pipeline-run","title":"Dynamic Link to a Pipeline Run","text":"<p>Create a link to a pipeline for your chatops.</p> <pre><code>$button = \"${ENV:SYSTEM_TEAMFOUNDATIONCOLLECTIONURI}${ENV:SYSTEM_TEAMPROJECT}/_build/results?buildId=$($ENV:BUILD_BUILDID)&amp;view=logs\"\n</code></pre>"},{"location":"development/build-release-engineering/azure-pipelines/#helper-function-for-reporting-progress-on-task","title":"Helper Function for Reporting Progress on Task","text":"<p>This is an ugly function, but hopefully useful as a jump start when in a hurry. \ud83d\ude01</p> <p>This primarily allows you to quickly report:</p> <ul> <li>ETA</li> <li>Percent complete on a task</li> <li>Log output on current task</li> </ul> <p>Needs more refinement and probably should just be an invoke build function, but for now it's semi-useful for some long-running tasks</p> <p>Setup up your variables like demonstrated. This doesn't handle nested task creation, so parentid won't be useful for anything other than local progress bar nesting.</p> <pre><code>$ProgressId =  [int]$RANDOM.Next(1, 1000)\n$ProgressCounter = 0\n$ProgressTotalToProcess = 1000\n$ProgressStopwatch =  [diagnostics.stopwatch]::new()\n$ProgressStopwatch.Start()\nforeach ($i in $List.Items)\n{\n    $ProgressCounter++\n        $ProgressSplat = @{\n        Activity       =  '&gt;&gt;&gt; Tacos'\n        StatusMessage  =  \"$($i.Foo).$($i.Bar)\"\n        ProgressId     =  $ProgressId\n        Counter        =  $Counter\n        TotalToProcess =  $ProgressTotalToProcess\n        Stopwatch      =  $ProgressStopwatch\n        # ParentId       =  $ParentProgressId\n        BuildOutput    = $true # for write-host special commands to show task progress in pipelines\n    }\n    Write-BuildProgressInfo @ProgressSplat\n}\n</code></pre> <p>Include this function in InvokeBuild job. If not using InvokeBuild, you'll need to change <code>Write-Build</code> to <code>Write-Host</code> and remove the color attribute.</p> <pre><code>function Write-BuildProgressInfo\n{\n    [cmdletbinding()]\n    param(\n        [string]$Activity='doing stuff',\n        [string]$StatusMessage,\n        [int]$ProgressId,\n        [int]$Counter,\n        [int]$TotalToProcess,\n        $StopWatch,\n        $ParentId,\n        [switch]$Complete,\n        [switch]$BuildOutput\n    )\n    [hashtable]$writeProgressSplat = @{}\n    $writeProgressSplat.Add('Id', $ProgressId)\n    $writeProgressSplat.Add('Activity'  , $Activity)\n    if ($ParentId)\n    {\n        $writeProgressSplat.Add('ParentId', $ParentId )\n    }\n\n    # Write-Debug($PSBoundParameters.GetEnumerator() | Format-Table -AutoSize| Out-String)\n    if ($Counter -lt $TotalToProcess -and $Complete -eq $false)\n    {\n        #still processing\n        [int]$PercentComplete = [math]::Ceiling(($counter/$TotalToProcess)*100)\n        try { [int]$AvgTimeMs = [math]::Ceiling(($StopWatch.ElapsedMilliseconds / $counter)) } catch { [int]$AvgTimeMs=0 } #StopWatch from beginning of process\n\n        [int]$RemainingTimeSec = [math]::Ceiling( ($TotalToProcess - $counter) * $AvgTimeMs/1000)\n        [string]$Elapsed = '{0:hh\\:mm\\:ss}' -f $StopWatch.Elapsed\n\n        $writeProgressSplat.Add('Status', (\"Batches: ($counter of $TotalToProcess) | Average MS: $($AvgTimeMs)ms | Elapsed Secs: $Elapsed | $($StatusMessage)\"))\n        $writeProgressSplat.Add('PercentComplete', $PercentComplete)\n        $writeProgressSplat.Add('SecondsRemaining', $RemainingTimeSec)\n        Write-Progress @writeProgressSplat\n        if ($BuildOutput)\n        {\n            Write-Build DarkGray (\"$Activity | $PercentComplete | ETA: $('{0:hh\\:mm\\:ss\\.fff}' -f [timespan]::FromSeconds($RemainingTimeSec)) |  Batches: ($counter of $TotalToProcess) | Average MS: $($AvgTimeMs)ms | Elapsed Secs: $Elapsed | $($StatusMessage)\")\n            Write-Build DarkGray \"##vso[task.setprogress value=$PercentComplete;]$Activity\"\n        }\n    }\n    else\n    {\n        [int]$PercentComplete = 100\n        try { [int]$AvgTimeMs = [math]::Ceiling(($StopWatch.ElapsedMilliseconds / $TotalToProcess)) } catch { [int]$AvgTimeMs=0 } #StopWatch from beginning of process\n        [string]$Elapsed = '{0:hh\\:mm\\:ss}' -f $StopWatch.Elapsed\n\n        $writeProgressSplat.Add('Completed', $true)\n        $writeProgressSplat.Add('Status', (\"Percent Complete: 100% `nAverage MS: $($AvgTimeMs)ms`nElapsed: $Elapsed\"))\n        Write-Progress @writeProgressSplat\n        if ($BuildOutput)\n        {\n            Write-Build DarkGray (\"COMPLETED | $Activity | $PercentComplete | ETA: $('{0:hh\\:mm\\:ss\\.fff}' -f $RemainingTimeSec) |  Batches: ($counter of $TotalToProcess) | Average MS: $($AvgTimeMs)ms | Elapsed Secs: $Elapsed | $($StatusMessage)\")\n        }\n    }\n}\n</code></pre>"},{"location":"development/build-release-engineering/renovate/","title":"renovate","text":"","tags":["dependency-management"]},{"location":"development/build-release-engineering/renovate/#run-locally-via-docker","title":"Run Locally Via Docker","text":"","tags":["dependency-management"]},{"location":"development/build-release-engineering/renovate/#for-azure-devops","title":"For Azure DevOps","text":"<p>Git Safe Directory</p> <p>This doesn't mount to the host <code>/tmp</code>, unlike examples in GitHub, because it flags the directory as owned by the docker user (likely root), which causes Git's safe directory feature to block.</p> <p>Easier to just disable mounting to the host in this scenario, as <code>git config --global --add safe.directory /tmp/renovate/repos/*</code> didn't seem to work.</p> <p>Set the environment variables: <code>export AZURE_DEVOPS_ORG=foo</code>, and the other <code>AZURE_DEVOPS_EXT_PAT</code>, and finally replace <code>PROJECTNAME/REPO</code>.</p> <pre><code>docker run --rm -it \\\n    -e RENOVATE_PLATFORM=\"azure\" \\\n    -e RENOVATE_ENDPOINT=\"https://dev.azure.com/${AZURE_DEVOPS_ORG}/\" \\\n    -e GITHUB_COM_TOKEN=$(gh auth token) \\\n    -e SYSTEM_ACCESSTOKEN=$AZURE_DEVOPS_EXT_PAT \\\n    -e RENOVATE_TOKEN=$AZURE_DEVOPS_EXT_PAT \\\n    -e RENOVATE_DRY_RUN=full \\\n    -e LOG_LEVEL=debug \\\n    -v ${PWD}/config.js:/usr/src/app/config.js \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    renovate/renovate:latest --include-forks=false --dry-run=full PROJECTNAME/REPO\n</code></pre>","tags":["dependency-management"]},{"location":"development/build-release-engineering/renovate/#centralizing-config","title":"Centralizing Config","text":"<p>Create a <code>config.js</code> in the <code>renovate-config</code> repo you create. Based on the docs for renovate, this type of config can support nuget, npm, and github auth.</p> <p>Change the various options by reviewing the renovate docs. Each option such as creation of closed pull requests can be configured by flags or environment variables.</p> <p>For Azure DevOps Pipelines, see the example below. For GitHub, you'll benefit from using the Renovatebot GitHub app as a much smoother integration (it also checks by default every 3 hours).</p> local debuggingconfig.jsrenovate.azure-pipelines.yml <pre><code># Change recreation if you are testing and need to abandon, then recreate\ndocker run --rm \\\n    -it \\\n    -e RENOVATE_RECREATE_CLOSED=false \\\n    -e LOG_LEVEL=debug \\\n    -e RENOVATE_DRY_RUN=true \\\n    -e GITHUB_COM_TOKEN=$GITHUB_TOKEN \\\n    -e RENOVATE_TOKEN=$AZURE_DEVOPS_EXT_PAT \\\n    -v \"${PWD}/config.js:/usr/src/app/config.js\" \\\n    renovate/renovate --include-forks=false\n</code></pre> <pre><code>const pipelineToken = process.env.RENOVATE_TOKEN;\nconst patTokenForFeed = process.env.RENOVATE_TOKEN;\n\nmodule.exports = {\n  platform: 'azure',\n  endpoint: 'https://dev.azure.com/{myorg}/',\n  token: pipelineToken,\n  hostRules: [\n    {\n      hostType: 'npm',\n      matchHost: 'pkgs.dev.azure.com',\n      username: 'apikey',\n      password: patTokenForFeed,\n    },\n    {\n      hostType: 'npm',\n      matchHost: '{myorg}.pkgs.visualstudio.com',\n      username: 'apikey',\n      password: patTokenForFeed,\n    },\n    {\n      matchHost: 'https://pkgs.dev.azure.com/{myorg}/',\n      hostType: 'nuget',\n      username: 'renovate', // doesn't matter for azure\n      password: patTokenForFeed,\n    },\n    {\n      matchHost: 'github.com',\n      token: process.env.GITHUB_COM_TOKEN,\n    },\n  ],\n  repositories: [\n    // 'Project/reponame',\n    'Project/reponame',\n  ],\n};\n</code></pre> <pre><code>---\nname: renovate.$(Build.Reason)-$(Date:yyyyMMdd)-$(Rev:.r)\npr: none\ntrigger:\n  batch: true\n  branches:\n    include:\n      - main\nschedules:\n  - cron: 0 07 * * Mon\n    displayName: Mon7am\n    branches:\n      include: [main]\n    always: true\njobs:\n  - job: renovate\n    displayName: renovate-repos\n    timeoutInMinutes: 15\n    pool:\n      name: Azure Pipelines\n      vmImage: ubuntu-latest\n    steps:\n      - checkout: self\n      - bash: |\n          git config --global user.email 'bot@renovateapp.com'\n          git config --global user.name 'Renovate Bot'\n          npx --userconfig .npmrc renovate\n        displayName: npx-renovate\n        env:\n          RENOVATE_TOKEN: $(System.AccessToken)\n          GITHUB_COM_TOKEN: $(GITHUB_COM_TOKEN)\n</code></pre>","tags":["dependency-management"]},{"location":"development/front-end/yarn/","title":"Yarn","text":"","tags":["front-end","yarn","package-management"]},{"location":"development/front-end/yarn/#setup","title":"Setup","text":"<ul> <li><code>npm install -g yarn</code></li> <li>Now set the version to latest in the project by running: <code>yarn set version berry</code>.</li> </ul>","tags":["front-end","yarn","package-management"]},{"location":"development/front-end/yarn/#gitignore","title":"Gitignore<sup>1</sup>","text":"Normal usageZero Installs Normal Usage<pre><code>pnp.*\n.yarn/*\n!.yarn/patches\n!.yarn/plugins\n!.yarn/releases\n!.yarn/sdks\n!.yarn/versions\n</code></pre> Zero Installs aka Plug &amp; Play<pre><code>.yarn/*\n!.yarn/cache\n!.yarn/patches\n!.yarn/plugins\n!.yarn/releases\n!.yarn/sdks\n!.yarn/versions\n</code></pre> <ol> <li> <p>https://yarnpkg.com/getting-started/qa/#which-files-should-be-gitignored\u00a0\u21a9</p> </li> </ol>","tags":["front-end","yarn","package-management"]},{"location":"development/go/","title":"Go","text":"<p>A no-frills language that maximizes productivity and readability for both individuals and teams.</p> <p>My \"Go To\" for solving problems!</p> <p></p>"},{"location":"development/go/code-coverage/","title":"Code Coverage","text":""},{"location":"development/go/code-coverage/#code-coverage-report","title":"Code Coverage Report","text":""},{"location":"development/go/code-coverage/#codecov","title":"Codecov","text":"<p>Using codecov works great for Github repos.</p>"},{"location":"development/go/code-coverage/#use-gopherbadge","title":"Use gopherbadge","text":"<pre><code>go install github.com/jpoles1/gopherbadger@master\n</code></pre> <ul> <li>Set <code>![gopherbadger-tag-do-not-edit]()</code> in the readme, and then this gets replaced with a code coverage percentage badge.</li> <li>Generate the required code coverage reports using:</li> </ul> <pre><code>go test ./... -coverprofile ./artifacts/cover.out\ngo tool cover -html=./artifacts/cover.out -o ./artifacts/coverage.html\ngopherbadger -md=\"README.md,coverage.md\" -tags 'unit'\n</code></pre>"},{"location":"development/go/dagger/","title":"Dagger","text":"<p>Containerized magic with Go and BuildKit</p> <p>Will put some experience notes here soon as I've successfully built angular and nginx containers with it and overall was a great experience. With the upcoming services support, I can see a whole lot more usage cases too.</p> Example Building An Angular Project <p>Using mage, here's an example of how to invoke Mage to build an angular project without any angular tooling installed locally.</p> <pre><code>const AngularVersion = \"15\"\n\n// Build runs the angular build via Dagger.\nfunc (Dagger) Build(ctx context.Context) error {\n  client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stdout))\n  if err != nil {\n    pterm.Error.Printfln(\"unable to connect to dagger: %s\", err)\n    return err\n  }\n  defer client.Close()\n\n  homedir, err := os.UserHomeDir()\n  if err != nil {\n    return err\n  }\n  npm := client.Container().From(\"node:lts-alpine\")\n  npm = npm.WithMountedDirectory(\"/src\", client.Host().Directory(\".\")).\n    WithWorkdir(\"/src\")\n\n  path := \"dist/\"\n  npm = npm.WithExec([]string{\"npm\", \"install\", \"-g\", fmt.Sprintf(\"@angular/cli@%s\", AngularVersion)})\n  npm = npm.WithExec([]string{\"ng\", \"config\", \"-g\", \"cli.warnings.versionMismatch\", \"false\"})\n  npm = npm.WithExec([]string{\"ng\", \"v\"})\n  npm = npm.WithExec([]string{\"npm\", \"ci\"})\n  npm = npm.WithExec([]string{\"ng\", \"build\", \"--configuration\", \"production\"})\n\n  // Copy \"dist/\" from container to host.\n  _, err = npm.Directory(path).Export(ctx, path)\n  if err != nil {\n    return err\n  }\n  return nil\n}\n</code></pre> example of handling both local and CI private npm auth <p>Here you can handle both running in a CI context or a remote context by evaluting for a CI variable that would provide back a CI system generated <code>.npmrc</code>.   If this isn't provided, mount the file from the home directory into the build container.</p> <p>Note this container isn't for publishing, it's a build container copying the <code>dist/</code> contents back to the project directory.</p> <pre><code>npmrcFile := &amp;dagger.Secret{}\n\n// bypassing any mounting of npmrc, as CI tooling should update any private  inline with current file here\nif os.Getenv(\"NPM_CONFIG_USERCONFIG\") != \"\" {\n  pterm.Info.Printfln(\"[OVERRIDE] NPM_CONFIG_USERCONFIG: %s\", os.Getenv(\"NPM_CONFIG_USERCONFIG\"))\n  npmrcDir := filepath.Dir(os.Getenv(\"NPM_CONFIG_USERCONFIG\"))\n} else {\n  // [DEFAULT] NPM config set from home/.npmrc\n  npmrcFile = client.Host().Directory(homedir, dagger.HostDirectoryOpts{Include: []string{\".npmrc\"}}).File(\".npmrc\").Secret()\n\n  // if npmrcFile doesn't exist output error\n  if _, err := os.Stat(filepath.Join(homedir, \".npmrc\")); os.IsNotExist(err) {\n    return errors.New(\"missing npmrc file\")\n  }\n  npm = npm.WithMountedSecret(\"/root/.npmrc\", npmrcFile)\n}\n</code></pre> Building a Go App with Caching <p>Using Mage and the excellent chainguard go builder image, this shows building a binary for the current platform and architecture, while wrapping up the entire build process inside the Dagger engine. The output goes to the standard <code>.artifacts</code> directory I include in all my projects, which should be ignored by git.</p> <pre><code>package main\n\nimport (\n  \"context\"\n  \"os\"\n  \"path/filepath\"\n  \"runtime\"\n\n  \"dagger.io/dagger\"\n  \"github.com/magefile/mage/mg\"\n  \"github.com/pterm/pterm\"\n)\n\n// Build contains all the build related mage targets.\ntype Build mg.Namespace\n\n\nconst (\n  // ArtifactDirectory is a directory containing artifacts for the project and shouldn't be committed to source.\n  ArtifactDirectory = \".artifacts\"\n\n  // PermissionUserReadWriteExecute is the permissions for the artifact directory.\n  PermissionUserReadWriteExecute = 0o0700\n\n\n)\n// TargetBuildDirectory is the directory where the build artifacts will be placed.\nvar TargetBuildDirectory = filepath.Join(ArtifactDirectory, \"builds\")\n\n\n// \ud83d\udd28 MyAppName builds the service using Dagger for the current system architecture.\n//\n// Development notes: This is a fully containerized build, using Dagger. Requires Docker.\nfunc (Build) MyAppName() error {\n\n  ctx := context.Background()\n  pterm.DefaultHeader.Println(\"Building with Dagger\")\n\n  buildThis := \"./myApp/main.go\" // this is the specific file to build, could be an input variable/slice though\n  appName := \"myApp\"\n  // make the target directory\n  if err := os.MkdirAll(filepath.Join(TargetBuildDirectory, appName), PermissionUserReadWriteExecute); err != nil {\n    return err\n  }\n  // initialize Dagger client\n  client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stdout))\n  if err != nil {\n    return err\n  }\n  defer client.Close()\n\n  // get reference to the local project\n  src := client.Host().Directory(\".\")\n  cachedBuild := client.CacheVolume(\"go-build-cache\")\n  cachedMod := client.CacheVolume(\"go-mod-cache\")\n\n  modcache := \"/nonroot/.cache/go-mod-cache\"\n  buildcache := \"/nonroot/.cache/go-build-cache\"\n\n  // get `golang` image\n  golang := client.Container().From(\"cgr.dev/chainguard/go:latest\").\n    WithEnvVariable(\"CGO_ENABLED\", \"0\").\n    WithEnvVariable(\"GOOS\", runtime.GOOS).\n    WithEnvVariable(\"GOARCH\", runtime.GOARCH).\n    WithEnvVariable(\"GOMODCACHE\", modcache). // attempt to optimize mod and build caching\n    WithEnvVariable(\"GOCACHE\", buildcache)\n\n  // mount cloned repository into `golang` image\n  golang = golang.WithMountedDirectory(\"/src\", src).\n    WithWorkdir(\"/src\").\n    WithMountedCache(modcache, cachedMod).\n    WithMountedCache(buildcache, cachedBuild)\n\n  // define the application build command\n  outputDirectory := filepath.Join(TargetBuildDirectory, appName)\n  outputFile := filepath.Join(outputDirectory, fmt.Sprintf(\"%s-service\",appName))\n  golang = golang.WithExec([]string{\"build\", \"-o\", outputFile, \"-ldflags\", \"-s -w\", \"-trimpath\", buildThis}) // NOTE: target is preset for now\n\n  // get reference to build output directory in container\n  output := golang.Directory(outputDirectory).File(fmt.Sprintf(\"%s-service\",appName)) // this is the specific  file rather than a mounted directory as we just want this one artifact\n\n  // write contents of container build/ directory to the host\n  _, err = output.Export(ctx, outputFile)\n  if err != nil {\n    return err\n  }\n\n  return nil\n}\n</code></pre>","tags":["containers","go","build-release-engineering"]},{"location":"development/go/effective-go/","title":"Effective Go","text":""},{"location":"development/go/effective-go/#top-reference-material","title":"Top Reference Material","text":"<p>Here's the reference material I use to help guide me on basic style, design, and general idiomatic Go practices.</p> Resource Description Go Style Guide A guide to writing Go code. Uber Go Styleguide A thorough stylistic guide (opinionated, but great explanations on why) Practical Go - Dave Cheney A great practical guide from a well-respected authority in the Go community"},{"location":"development/go/effective-go/#effective-go","title":"Effective Go","text":"<p>Principles I've gleaned over-time and am quoting or bookmarking.</p>"},{"location":"development/go/effective-go/#dont-hide-the-cost","title":"Don't hide the cost","text":"<p>Source: Bill Kennedy in Ultimate Go [^readability]</p> <p>If we are doing construction to a variable, we use value construction. Avoid pointer semantic construction if not in the return.</p> <p>Example:</p> <pre><code>// clear visible cost of the allocation by value construction and passing of pointer back up the call stack\nfunc createSomething() *something {\n  u := something{\n    name: \"example\",\n  }\n  return &amp;u // &lt;--- This makes clear the cost and allocation back up the callstack.\n}\n// cost is obscured by construction being a pointer\n// and returning a value that is not clear to reader if value or pointer\nfunc createSomething()*something {\n  u := &amp;something{\n    name: \"example\",\n  }\n  return u // &lt;--- Not good. Hides the cost, and require reading function further to find that this is a pointer.\n}\n</code></pre> <p>Making cost obvious and visible is a big priority for readable maintainable code with a team.</p>"},{"location":"development/go/go/","title":"go","text":"","tags":["development","golang"]},{"location":"development/go/go/#starter-template","title":"Starter Template","text":"<p>I saved this as a snippet for vscode to get up and running quickly with something better than the defaults for handling <code>func main</code> isolation. I've been working on modifying this a bit as I don't really like using args, but am trying not to overcomplicate things as a new gopher.</p> <p>I tend to like better flag parsing than using args, but it's still a better pattern to get functions isolated from main to easily test.</p> <p>The gist that I've taken from this and discussions in the community is ensure that <code>main</code> is where program termination is dedicated instead of handling this in your functions. This isolation of logic from main ensures you can more easily setup your tests as well, since <code>func main()</code> isn't testable.</p> <pre><code>package main\n\n// package template from:\nimport (\n    \"errors\"\n    \"fmt\"\n    \"io\"\n    \"os\"\n)\n\nconst (\n    // exitFail is the exit code if the program\n    // fails.\n    exitFail = 1\n)\n\nfunc main() {\n    if err := run(os.Args, os.Stdout); err != nil {\n        fmt.Fprintf(os.Stderr, \"%s\\n\", err)\n        os.Exit(exitFail)\n    }\n}\n\nfunc run(args []string, stdout io.Writer) error {\n    if len(args) == 0 {\n        return errors.New(\"no arguments\")\n    }\n    for _, value := range args[1:] {\n        fmt.Fprintf(stdout, \"Running %s\", value)\n    }\n    return nil\n}\n</code></pre>","tags":["development","golang"]},{"location":"development/go/go/#running-external-commands","title":"Running External Commands","text":"","tags":["development","golang"]},{"location":"development/go/go/#repos","title":"Repos","text":"","tags":["development","golang"]},{"location":"development/go/goroutines/","title":"goroutines","text":"","tags":["go"]},{"location":"development/go/goroutines/#resources","title":"Resources","text":"Resource Notes Goroutines &amp; Closures Important caveat on shadowing variable in loops included Using uiprogress Notes I wrote about trying threadsafe progress bar package","tags":["go"]},{"location":"development/go/goroutines/#using-goroutines-with-cli-tools","title":"Using Goroutines with CLI Tools","text":"<p>Running cli tools via goroutines can allow speeding up slow actions like code generation. I prefer to run these type of actions with a buffered channel to allow throttling the requests to avoid my laptop from catching on fire. \ud83d\udd25</p> <p>Here's an example using Pterm output for reporting progress (no progress bar)<sup>1</sup>.</p> <p>Playground - Go </p> Invoking CLI With Buffered Channels<pre><code>package main\n\nimport (\n    \"sync\"\n\n    \"github.com/bitfield/script\"\n    \"github.com/pterm/pterm\"\n)\n\nfunc main() {\n    pterm.DisableColor()\n    concurrentLimit := 4\n    type runMe struct {\n        title   string\n        command string\n    }\n    runCommands := []runMe{\n        {title: \"commandtitle\", command: \"echo 'foo'\"},\n    }\n    var wg sync.WaitGroup\n    buffChan := make(chan struct{}, concurrentLimit)\n    wg.Add(len(runCommands))\n    pterm.Info.Printfln(\"running cli [%d]\", len(runCommands))\n    for _, r := range runCommands {\n        r := r\n        go func(r runMe) {\n            buffChan &lt;- struct{}{}\n            defer wg.Done()\n            if _, err := script.Exec(r.command).Stdout(); err != nil {\n                pterm.Error.Printfln(\"[%s] unable to run: %s, err: %s\", r.title, r.command, err)\n            } else {\n                pterm.Success.Printfln(\"[%s]\", r.title)\n            }\n            &lt;-buffChan\n        }(r)\n    }\n    wg.Wait()\n}\n</code></pre> <ol> <li> <p>Since things are running concurrently a single bar isn't quite accurate. There are libraries out there that report correctly with goroutines, but pterm as of 2023-03 isn't one of them yet. It's being worked on.\u00a0\u21a9</p> </li> </ol>","tags":["go"]},{"location":"development/go/http/","title":"HTTP","text":"","tags":["api","rest","http"]},{"location":"development/go/http/#anonymous-server","title":"Anonymous Server","text":"<p>MacOS will throw firewall alerts to allow connectivity everytime a binary is run that doesn't provide the server address.</p> <ul> <li>\u274c What throws the error: <code>http.ListenAndServe(\":\"+port, nil)</code></li> <li>\u2714\ufe0f What doesn't throw an error: <code>http.ListenAndServe(\"127.0.0.1:\"+port, nil)</code></li> </ul> <p>While that might be problematic to iron out as a variable if running insider a container for docker needing to talk to the host, this works well for running the binary locally without much change. If you are running inside docker and need 127.0.0.1 to resolve to the host directly, then you'll need to handle that differently. See I want to connect from a container to a service on the host.</p> <p>I ran this problem through chatGPT and it provided a nice explanation.<sup>1</sup></p> chatgpt answers: macOS and anonymous server for local testing <p>The reason why you are seeing a pop-up alert on macOS when using http.ListenAndServe(\":\"+port, nil) is that when you use a port number without specifying an IP address, the Go HTTP server listens on all available network interfaces on the local machine, including the loopback address (127.0.0.1), which is used for communication within the same machine.</p> <p>When a process listens on a network interface, it is bound to that interface and can receive incoming connections from other machines on the network. In the case of the loopback interface, however, it can only receive connections originating from the same machine, and macOS security policies require user approval before allowing incoming connections to loopback addresses.</p> <p>When you specify an IP address like 127.0.0.1 explicitly, you are telling the server to only listen on that specific network interface, which macOS recognizes as a safe and expected behavior and does not generate an alert.</p> <p>To avoid the pop-up alert when using http.ListenAndServe(\":\"+port, nil), you can add your application to the list of trusted applications in the macOS Firewall settings or use http.ListenAndServe(\"127.0.0.1:\"+port, nil) to explicitly listen on the loopback interface.</p> <ol> <li> <p>This is an incredible example of it jump starting debugging efforts. The level of effort to get a quick answer was much faster than Google. Will it may \"hallucinate\" this is a good example of quick information that can help and let you move on.\u00a0\u21a9</p> </li> </ol>","tags":["api","rest","http"]},{"location":"development/go/logging/","title":"Go Logging","text":""},{"location":"development/go/logging/#logging","title":"Logging","text":"<p>At this stage, I'm using zerolog as I found it very easy to get moving with structured logs.</p> <p>The output of this demo looks pretty good!</p> <p></p> <p>Here's a functional demo that can be used to bootstrap a new project with this.</p>"},{"location":"development/go/mage/","title":"Mage","text":""},{"location":"development/go/mage/#using-go-for-task-running-automation","title":"Using Go for Task Running &amp; Automation","text":"<p>My preferred tool at this time is Mage.</p> <p>Mage replaces the need for Bash or PowerShell scripts in your repo for core automation tasks, and provides the benefits of Go (cross-platform, error handling paradigm, readability, performance, etc).</p>"},{"location":"development/go/mage/#getting-started-with-mage","title":"Getting Started With Mage","text":""},{"location":"development/go/mage/#use-go","title":"Use Go","text":"<ul> <li>Run <code>go install github.com/magefile/mage@latest</code></li> <li>Run <code>go install github.com/iwittkau/mage-select@latest</code></li> <li>asdf: <code>asdf plugin-add mage &amp;&amp; asdf install mage latest &amp;&amp; asdf local mage latest</code></li> </ul>"},{"location":"development/go/mage/#intialize-a-new-project","title":"Intialize a New Project","text":"<ul> <li>Scripts-To-Rule-Them-All-Go: A repo I've setup as quick start template for a Mage enabled repository with linting and core structure already in place.</li> <li>Magetools: Reusable packages that can be pulled in to jump start common tasks or utilities.</li> <li>Examples:<ul> <li>Enhanced go formatter with <code>mage go:wrap</code>.</li> <li>Preinstall common Go tools such as the language server, dlv, gofumpt, golangci-lint, and more with <code>mage go:init</code>.</li> <li>Provide a github repo for a go binary and use in tasks. If the binary isn't found, it will automatically grab it when invoked.</li> <li>Pre-commit registration and tooling.</li> <li>Install Git Town, Bit, and other cool git helpers with <code>mage gittools:init</code>.</li> <li>Chain together all your core tasks with <code>mage init</code> to allow for a fully automated dev setup.</li> </ul> </li> </ul>"},{"location":"development/go/mage/#why-should-i-care-about-mage","title":"Why Should I Care About Mage?","text":"<ul> <li>I've never felt my automation was as robust, stable, and easy to debug as when I've used Mage.</li> <li>I've done a lot of experimenting with others, and had primarily relied on <code>InvokeBuild</code> (powershell based) in the past.</li> <li>Mage takes the prize for ease of use.</li> <li>You can migrate a make file relatively easily if you want to just call tools directly.</li> <li>You can benefit from using Go packages directly as you up your game.</li> <li>Example: instead of calling kubectl directly, I've used a helm Go library that does actions like validation, linting, and templating directly from the same core code that kubectl itself uses.</li> </ul>"},{"location":"development/go/mage/#mage-basics","title":"Mage Basics","text":"<ul> <li>Mage is just Go code.</li> <li>It does a little \"magic\" by simplying matching some functions that match basic signature such as <code>error</code> output, like <code>func RunTests(dir string) error {...}</code>.</li> <li>You can get around needing mage by creating Go files, but you'd have to add basic args handling for the <code>main()</code> entry point, and help generation.</li> <li>Mage basically tries to simplify the cli invocation by auto-discovering all the matched functions in your <code>magefiles</code> directory and providing as tasks.</li> <li>Mage does not currently support flags, though this is actively being looked at.</li> <li>This means you are best served by keeping tasks very very simple. Ie <code>mage deploy project dev</code> is about as complex as I'd recommend.</li> <li>Normally, you'd invoke with <code>mytool -project ProjectName -env dev</code> and positions wouldn't matter. With mage, it's positional for simplicity so best to keep simple!</li> </ul>"},{"location":"development/go/mage/#my-mage-tips","title":"My Mage Tips","text":"<ul> <li>Use the pattern shown in my template repo above.</li> <li>Use <code>magefiles</code> directory.</li> <li>Provide a single <code>magefile.go</code> that does your imports and basic commands. If it's a big project then just have it import and put all your tasks in subdirectories that it imports.</li> <li>Provide a <code>magefiles/constants/constants.go &amp;&amp; vars.go</code> rather than being worried about globals.   This is for build automation, and having a configured file with standards that shouldn't change or global variables is a nice alternative to needing more yaml files.</li> <li>Use Pterm for enchanced logging experience, provides some beautiful output for users.</li> <li>For extra benefit, standardize with a <code>mage doctor</code> command in your project that validates issues experienced and gets added to over time.   This can help troubleshooting any environment or project issues if you maintain and add a list of checks being run.   Using Pterm you can make this into a nice table output like this:</li> </ul>"},{"location":"development/go/modules/","title":"Go Modules","text":""},{"location":"development/go/modules/#what-are-go-modules","title":"What are Go Modules?","text":"<p>Go Modules are primarily a dependency management solution.</p> <p>A module:</p> <ul> <li>Is primarily a dependency management tool, not a project organization tool.</li> <li>Is imported to get access to public exported members in your own project.</li> <li>One module can have <code>n</code> binaries produced.</li> <li>A module can be used in a monorepo or single CLI tool.</li> </ul> <p>A module doesn't:</p> <ul> <li>Handle build or binary path metadata.</li> <li>Have any relationship to the produced artifacts.</li> </ul>"},{"location":"development/go/modules/#module-tips","title":"Module Tips","text":"<ul> <li>Use canconical import path (aka) <code>github.com/sheldonhull/mygomod</code> if you want to support <code>go install</code> commands.</li> <li>Use <code>mymod.local</code> if no need to support remote imports or installs.</li> <li>This allows <code>gofumpt</code> and tooling to correctly sort the imports from standard library apart from your own imports, without requiring canonical name format.</li> <li>Stick with one module in the repo if possible, to simplify tooling, linting, and testing. This is important in monorepos as much of the tooling that uses paths like <code>go test ./...</code> will not work with multi-module repos in a project.</li> </ul>"},{"location":"development/go/modules/#project-build-tooling","title":"Project &amp; Build Tooling","text":"<ul> <li>Use <code>devtools.go</code> to create a list of cli tools that should be installed with Mage.</li> <li>Use <code>tools.go</code> to put <code>_ \"remotemodulename\"</code> in, and identify clearly that a tool such as Stringer or linters are not dependencies for the primary module, but instead are tooling dependencies.</li> </ul>"},{"location":"development/go/pre-commit/","title":"Pre-Commit","text":"","tags":["go"]},{"location":"development/go/pre-commit/#using-pre-commit-tooling","title":"Using Pre-Commit Tooling","text":"<p>Here's how to setup pre-commit for Go projects.</p> <ol> <li>Install pre-commit for macOS: <code>brew install pre-commit</code> or see directions for curl/other options for WSL, Windows, Linux, etc.</li> <li>Use the template from TekWizely/pre-commit-golang: Pre-Commit hooks for Golang with support for Modules</li> <li>Several options are provided for <code>fmt</code> oriented commands.       Comment out any duplicates that don't apply.</li> <li>Finally initialize the pre-commit hooks in your repo by running: <code>pre-commit install</code></li> </ol> <p>Validate everything is working by running: <code>pre-commit run --all-files</code></p> <p>Periodically, you can run <code>pre-commit autoupdate</code> to ensure the latest version of the pre-commit hooks are upgraded.</p>","tags":["go"]},{"location":"development/go/testing/","title":"Go Testing","text":""},{"location":"development/go/testing/#testing","title":"Testing","text":"<ul> <li>Go test will automatically ignore directories and files starting with <code>.</code> or <code>_</code>, see go command - cmd/go - pkg.go.dev.</li> </ul>"},{"location":"development/go/testing/#gotestsum","title":"Gotestsum","text":"<p>Install with: <code>go install gotest.tools/gotestsum@latest</code>.</p> <p>Then run like this: <code>gotestsum</code> or try the alternative formats like: <code>gotestsum --format dots-v2</code> or <code>--format pkgname</code>, or <code>--format testname</code>.</p>"},{"location":"development/go/using-go-tools/","title":"Using Go Tools","text":"<p>This is primarily focused on folks who don't use Go tooling everday, but want to use Go tools. Maybe you need help getting up and running?</p>"},{"location":"development/go/using-go-tools/#go-binaries","title":"Go Binaries","text":"<p>Tools that can compile to a Go binary such as CLI tools or a web server can be installed from source easily, by running <code>go install</code>.</p> <p>There's a few things you need though to do this.</p> <ul> <li>Go installed \ud83d\ude00</li> <li>Using aqua makes this easy.</li> <li>The path the binaries are dropped into isn't in your <code>PATH</code> by default, so you need to ensure your shell of choice has this path added so the binaries can be found globally.</li> <li>Ensure binaries can be found. <code>export PATH=\"$(go env GOPATH)/bin:${PATH}\"</code></li> <li>The right way to invoke it.</li> </ul>"},{"location":"development/go/using-go-tools/#setup-path-variables-so-go-tooling-can-be-found","title":"Setup Path Variables So Go Tooling Can Be Found","text":"linux/darwinwindows linux (.zshenv, .bashrc, etc)<pre><code># If using private, then you can set something like this: export GOPRIVATE=dev.azure.com\nexport GOPATH=\"${HOME}/go\"\nexport GOBIN=\"${HOME}/go/bin\"\nexport PATH=\"${GOBIN}:${PATH}\"\n</code></pre> windows<pre><code>  if ($PSVersionTable.PSEdition -ne 'Core') {\n    Write-Warning \"Please use PowerShell Core 7+ for this to work\"\n    return\n  }\n  # If using private, then you can set something like this: [Environment]::SetEnvironmentVariable('GOPRIVATE', 'dev.azure.com', 'Machine')\n  [Environment]::SetEnvironmentVariable('GOPATH', (Join-Path $Home 'go'), 'Machine')\n  [Environment]::SetEnvironmentVariable('GOBIN', (Join-Path $Home 'go' 'bin'), 'Machine')\n  [Environment]::SetEnvironmentVariable('PATH', ((Join-Path $Home 'go' 'bin'), $ENV:PATH -join [IO.Path]::PathSeparator), 'Machine')\n  New-Item -Path (Join-Path $Home 'go') -ItemType Directory -Force -EA 0\n  Write-Host \"Shutdown Terminal and reopen for this to take effect \ud83d\ude4f\" -ForegroundColor Green\n</code></pre>"},{"location":"development/go/vscode-configuration/","title":"VSCode Configuration for Go","text":""},{"location":"development/go/vscode-configuration/#vscode","title":"VSCode","text":""},{"location":"development/go/vscode-configuration/#custom-tasks","title":"Custom Tasks","text":""},{"location":"development/go/vscode-configuration/#default-shells","title":"Default Shells","text":"<p>This can ensure default behavior is processed on each OS, customizing the shell to use.</p> <pre><code>{\n  \"version\": \"2.0.0\",\n  \"presentation\": {\n    \"echo\": false,\n    \"reveal\": \"always\",\n    \"focus\": false,\n    \"panel\": \"dedicated\",\n    \"showReuseMessage\": true\n  },\n  \"linux\": {\n    \"options\": {\n      \"shell\": {\n        \"executable\": \"/usr/local/bin/zsh\",\n        \"args\": [\"-l\", \"-c\"]\n      }\n    },\n    \"type\": \"shell\"\n  },\n  \"windows\": {\n    \"options\": {\n      \"shell\": {\n        \"executable\": \"pwsh\"\n      }\n    },\n    \"type\": \"shell\"\n  },\n  \"osx\": {\n    \"options\": {\n      \"shell\": {\n        \"executable\": \"/usr/local/bin/zsh\",\n        \"args\": [\"-l\", \"-c\"]\n      }\n    },\n    \"type\": \"shell\"\n  },\n  \"tasks\": []\n}\n</code></pre>"},{"location":"development/go/vscode-configuration/#run-lint","title":"Run Lint","text":"<p>Add this to your <code>.vscode/tasks.json</code> file and you'll get the full linting output in your problems pane.</p> <p>By default, the <code>golangci-lint</code> config should include <code>--fast</code> to avoid impact to your editing.</p> <p>This will ensure all tasks that a pre-commit check or CI check will be run and provided in the problems panel.</p> <pre><code>\"tasks\": [\n    {\n      \"label\": \"go-lint-all\",\n      \"detail\": \"This runs the full range of checks and the VSCode problem matcher will pull all of them in. Without this, the default behavior of VSCode is to run with --fast to reduce impact to IDE.\",\n      \"type\": \"shell\",\n      \"command\": \"golangci-lint\",\n      \"args\": [\n        \"run\",\n        \"--out-format\",\n        \"colored-line-number\"\n      ],\n      \"problemMatcher\": [\n        \"$go\"\n      ],\n      \"presentation\": {\n        \"echo\": true,\n        \"reveal\": \"always\",\n        \"focus\": true,\n        \"panel\": \"dedicated\",\n        \"showReuseMessage\": true,\n        \"clear\": true\n      }\n    },\n</code></pre>"},{"location":"development/go/vscode-configuration/#run-nicely-formatted-test-output","title":"Run Nicely Formatted Test Output","text":""},{"location":"development/go/vscode-configuration/#tparse","title":"Tparse","text":"<p>While the testing extension is great, sometimes I just want to see a console summary. This task uses Tparse and provides a nicely formatted summary (including coverage numbers, cached tests, and more).</p> <p>Install tparse with: <code>go install github.com/mfridman/tparse@latest</code>.</p> <p>Run manually like this: <code>GOTESTS='slow' go test ./... -v -cover -json | tparse -all</code></p> <pre><code>{\n  \"label\": \"go-test-formatted-output\",\n  \"type\": \"shell\",\n  \"command\": \"go\",\n  \"options\": {\n    \"env\": {\n      \"GOTEST\": \"slow integration\",\n    }\n  },\n  \"args\": [\n    \"test\",\n    \"./...\",\n    \"-v\",\n    \"-cover\",\n    \"-json\",\n    \"|\",\n    \"tparse\",\n    \"-all\"\n  ],\n  \"problemMatcher\": []\n},\n</code></pre>"},{"location":"development/kubernetes/kubernetes/","title":"kubernetes","text":"","tags":["kubernetes"]},{"location":"development/kubernetes/kubernetes/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>What imagePullPolicy should I use? <p>The caching semantics of the underlying image provider make even imagePullPolicy: Always efficient, as long as the registry is reliably accessible.image-policy</p> </li> </ul>","tags":["kubernetes"]},{"location":"development/kubernetes/kubernetes/#troubleshooting","title":"Troubleshooting","text":"Error: ImagePullBackOff <p>Thanks to a Stack Overflow answer I was pointed to the answer on pre-pulled-images.</p> <p>This is a great example of the challenges of abstraction with tools. The declared image policies have more impact on the behavior than I originally thought.</p> <p>By default, the kubelet tries to pull each image from the specified registry. However, if the imagePullPolicy property of the container is set to IfNotPresent or Never, then a local image is used (preferentially or exclusively, respectively).</p> <p>To load an image you can run a command like this: <code>minikube image load --profile myprofile 'image:latest'</code>.</p>","tags":["kubernetes"]},{"location":"development/observability/observability/","title":"observability","text":"","tags":["observability","operations"]},{"location":"development/observability/observability/#datadog","title":"Datadog","text":"","tags":["observability","operations"]},{"location":"development/observability/observability/#evaluate-impact-on-pricing-based-on-contributors-in-a-repository","title":"Evaluate Impact on Pricing Based On Contributors In A Repository","text":"<p>Some of the pricing models reflect based on the number of active contributors in a repository. For example, the CI/Pipeline integration is based on a model of who's contributing.</p> <p>This snippet will assess unique contributors in the repo and based on a rough On-Demand pricing you can find on the website, roughly approx the impact to enabling this feature directly against this repository.</p> Figuring Out Pricing Based On Contributors in Git CalcDatadogPricing.ps1<pre><code>    $DatadogPerContributorPricing = 8 # Get from datadog pricing site\n    $LookBack = -6\n    $month = Get-Date -Format 'MM'\n    $year = Get-Date -Format 'yyyy'\n    $since = \"$((Get-Date -Day 1 -Month $month -Year $year).AddMonths($LookBack).AddDays(-1).ToString('yyyy-MM'))\"\n    $until = \"$((Get-Date).AddMonths(1).AddDays(-1).ToString('yyyy-MM')))\"\n\n    # $contributors =\n    $stats = &amp;git log --date=\"format-local:%Y-%m\" --pretty=format:\"%ad %ae\" --since=\"$since\" --until=\"$until\" |\n    ConvertFrom-Csv -Delimiter ' ' -Header 'month', 'contributor' |\n    Sort-Object -Property 'month', 'contributor' -Descending -Unique\n\n    $report = $stats | Group-Object -Property 'month' | ForEach-Object {\n        [pscustomobject]@{\n            month            = $_.Name\n            ContributorCount = $_.Count\n            approxCost       = ($_.Count * $DatadogPerContributorPricing).ToString('C0')\n        }\n    }\n\n    Write-Host '==== Pricing Consideration for Observability on Repo Based on Contributors Per Month ====' -ForegroundColor Green\n    Write-Host \"$($report | Format-Table -AutoSize -Wrap | Out-String)\" -ForegroundColor DarkGray\n</code></pre>","tags":["observability","operations"]},{"location":"development/powershell/","title":"PowerShell","text":"<p>With magic like this, why use bash!</p> <p></p>"},{"location":"development/powershell/dbatools/","title":"dbatools","text":"<p>Requests or Suggestions</p> <p>If you have any requests or improvements for this content, please comment below. It will open a GitHub issue for chatting further. I'd be glad to improve with any additional quick help and in general like to know if anything here in particular was helpful to someone. Cheers! \ud83d\udc4d</p>","tags":["sql-server","development","powershell","dbatools"]},{"location":"development/powershell/dbatools/#setup","title":"Setup","text":"<pre><code>install-module 'dbatools' -Scope CurrentUser\n</code></pre>","tags":["sql-server","development","powershell","dbatools"]},{"location":"development/powershell/dbatools/#database-corruption","title":"Database Corruption","text":"","tags":["sql-server","development","powershell","dbatools"]},{"location":"development/powershell/dbatools/#configure-the-database-default-path","title":"Configure the Database Default Path","text":"","tags":["sql-server","development","powershell","dbatools"]},{"location":"development/powershell/dbatools/#install-dbatools-and-restore-a-directory-of-backups","title":"Install Dbatools And Restore A Directory of Backups","text":"","tags":["sql-server","development","powershell","dbatools"]},{"location":"development/powershell/pester/","title":"Pester","text":""},{"location":"development/powershell/pester/#pester","title":"Pester","text":"<p>Many changes occurred after version 5. This provides a few examples on how to leverage Pester for data driven tests with this new format.</p>"},{"location":"development/powershell/pester/#beforeall-and-beforediscovery","title":"BeforeAll And BeforeDiscovery","text":"<p>One big change was the two scopes. Read the Pester docs for more details.</p> <p>The basic gist is that BeforeAll is in the \"run\" scope, while the test generation is BeforeDiscovery. While older versions of Pester would allow a lot more <code>foreach</code> type loops, this should be in the discovery phase now, and then <code>-Foreach</code> (aka <code>-TestCases</code>) hashtable can be used to iterate more easily now through result sets.</p>"},{"location":"development/powershell/pester/#pester-container-to-help-setup-data-driven-tests","title":"Pester Container To Help Setup Data Driven Tests","text":"<p>Example of setting up inputs for the test script from your InvokeBuild job.</p> <pre><code>$pc = New-PesterContainer -Path (Join-Path $BuildRoot 'tests\\configuration.tests.ps1') -Data @{\n    credential_user1 = Get-PSFConfigValue \"Project.$ENV:GROUP.credential.user1\" -NotNull\n    credential_user2 = Get-PSFConfigValue \"Project.$ENV:GROUP.credential.user2\" -NotNull\n    sql_instance     = Get-PSFConfigValue \"Project.$ENV:GROUP.instance_address\" -NotNull\n    database_list    = $DatabaseList\n}\n</code></pre>"},{"location":"development/powershell/pester/#pester-configuration-object","title":"Pester Configuration Object","text":"<p>Now, you'd add this <code>PesterContainer</code> object to the <code>PesterConfiguration</code>.</p> <p>Explore PesterConfiguration</p> <p>If you want to explore the pester configuration try navigating through it with: <code>PesterConfiguration]::Default</code> and then explore sub-properties with actions like: <code>[PesterConfiguration]::Default.Run | Get-Member</code>.</p> <pre><code>$configuration = [PesterConfiguration]@{\n    Run        = @{\n        Path        = (Join-Path $BuildRoot 'tests\\configuration.tests.ps1')\n        ExcludePath = '*PSFramework*', '*_tmp*'\n        PassThru    = $True\n        Container   = $pc\n    }\n    Should     = @{\n        ErrorAction = 'Continue'\n    }\n    TestResult = @{\n        Enabled      = $true\n        OutputPath   = (Join-Path $ArtifactsDirectory 'TEST-configuration-results.xml')\n        OutputFormat = 'NUnitXml'\n\n    }\n    Output     = @{\n        Verbosity = 'Diagnostic'\n    }\n}\n</code></pre> <p>This pester configuration is a big shift from the parameterized arguments provided in version &lt; 5.</p>"},{"location":"development/powershell/pester/#invoke-pester","title":"Invoke Pester","text":"<p>Run this with: <code>Invoke-Pester -Configuration $Configuration</code></p> <p>To improve the output, I took a page from <code>PSFramework</code> and used the summary counts here, which could be linked to a chatops message. Otherwise the diagnostic output should be fine.</p> <pre><code>$testresults = @()\n$testresults +=  Invoke-Pester -Configuration $Configuration\n\n\nWrite-Host '======= TEST RESULT OBJECT ======='\n\nforeach ($result in $testresults)\n{\n    $totalRun += $result.TotalCount\n    $totalFailed += $result.FailedCount\n    # -NE 'Passed'\n    $result.Tests | Where-Object Result | ForEach-Object {\n        $testresults += [pscustomobject]@{\n            Block   = $_.Block\n            Name    = \"It $($_.Name)\"\n            Result  = $_.Result\n            Message = $_.ErrorRecord.DisplayErrorMessage\n        }\n    }\n}\n\n#$testresults | Sort-Object Describe, Context, Name, Result, Message | Format-List\nif ($totalFailed -eq 0) { Write-Build Green \"All $totalRun tests executed without a single failure!\" }\nelse { Write-Build Red \"$totalFailed tests out of $totalRun tests failed!\" }\nif ($totalFailed -gt 0)\n{\n    throw \"$totalFailed / $totalRun tests failed!\"\n}\n</code></pre>"},{"location":"development/powershell/pester/#use-test-artifacts","title":"Use Test Artifacts","text":"<p>Use the artifact generated in the Azure Pipelines yaml to publish pipeline test results.</p> <pre><code>## Using Invoke Build for running\n\n- task: PowerShell@2\n  displayName: Run Pester Tests\n  inputs:\n    filePath: build.ps1\n    arguments: '-Task PesterTest -Configuration $(Configuration)'\n    errorActionPreference: 'Continue'\n    pwsh: true\n    failOnStderr: true\n  env:\n    SYSTEM_ACCESSTOKEN: $(System.AccessToken)\n- task: PublishTestResults@2\n  displayName: Publish Pester Tests\n  inputs:\n    testResultsFormat: 'NUnit'\n    testResultsFiles: '**/TEST-*.xml' # &lt;---------  MATCHES MULTIPLE TEST FILES AND UPLOADED\n    failTaskOnFailedTests: true\n  alwaysRun: true # &lt;---------  Or it won't upload if test fails\n</code></pre>"},{"location":"development/powershell/powershell/","title":"powershell","text":"<p>Any requests?</p> <p>If you have any requests or improvements for this content, please comment below. It will open a GitHub issue for chatting further. I'd be glad to improve with any additional quick help and in general like to know if anything here in particular was helpful to someone. Cheers! \ud83d\udc4d</p>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#requirements","title":"Requirements","text":"<ul> <li>Going forward most examples will focus on Powershell (&gt;= v7), the modern successor to Windows PowerShell &amp; PowerShell Core. Install this easily through chocolatey. Just install chocolatey, and then run <code>choco upgrade powershell powershell-core -y --source chocolatey</code> and you should have both 5.1 and core ready to go if your Windows version supports it. If you are on Windows 7 as a developer, there is no  for you, just get upgraded already. Linux has it's own set of directions.</li> <li>Anything manipulating system might need admin, so run as admin in prompt.</li> <li><code>Install-Module PSFramework</code> // I use this module for better logging and overall improvements in my quality of life. It's high quality, used by big projects like DbaTools and developed by a Powershell MVP with lots of testing. Forget regular <code>Write-Verbose</code> commands and just use the <code>Write-PSFMessage -Level Verbose -Message 'TacoBear'</code> instead.</li> </ul>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#psframework","title":"PSFramework","text":"<p>I use PSFramework on all my instances, as it's fantastic expansion to some core areas with PowerShell. This great module (along with other ancillary supporting ones like <code>PSModuleDevelopment</code> are well tested and usable for solving some issues in a much more elegant and robust manner than what is natively offered.</p> <p>A few key elements it can help with are:</p> <ul> <li>Improving Configuration and variable handling without complex scope issues</li> <li>Improving overall logging and troubleshooting</li> <li>Improving object manipulation</li> <li>Runspace usability enhancements</li> <li>Scripted properties</li> </ul>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#development-optional","title":"Development (Optional)","text":"<ol> <li>Install VSCode (Users)</li> <li><code>choco upgrade vscode-powershell -y</code> or install in extension panel in VSCode. If you are using ISE primarily.... move on already.</li> </ol>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#string-formatting","title":"String Formatting","text":"Type Example Output Notes Formatting Switch 'select {0} from sys.tables' -f 'name' select name from sys.tables Same concept as .NET [string]::Format(). Token based replacement .NET String Format [string]::Format('select {0} from sys.tables','name') select name from sys.tables Why would you do this? Because you want to showoff your .NET chops?","tags":["development","powershell"]},{"location":"development/powershell/powershell/#math-number-conversions","title":"Math &amp; Number Conversions","text":"From To Example Output Notes scientific notation Decimal 2.19095E+08 / 1MB 208.945274353027 MB Native PowerShell, supports 1MB, 1KB, 1GB","tags":["development","powershell"]},{"location":"development/powershell/powershell/#date-time-conversion","title":"Date &amp; Time Conversion","text":"<p>Converting dates to Unix Epoc time can be challenging without using the correct .NET classes. There is some built in functionality for converting dates such as <code>(Get-Date).ToUniversalTime() -UFormat '%s'</code> but this can have problems with time zone offsets. A more consistent approach would be to leverage the following. This was very helpful to me in working with Grafana and InfluxDb which commonly leverage Unix Epoc time format with seconds or milliseconds precision.</p> <pre><code>$CurrentTime = [DateTimeOffset]::new([datetime]::now,[DateTimeOffset]::Now.Offset);\n\n# Unix Epoc time starts from this date\n$UnixStartTime = [DateTimeOffset]::new(1970, 1, 1, 0, 0, 0,[DateTimeOffset]::Now.Offset);\n\n# To Use This Now On Timestamp you could run the following\n$UnixTimeInMilliseconds = [Math]::Floor( ((get-date $CurrentTime) - $UnixStartTime).TotalMilliseconds)\n\n# To Use with Different Time just change the `$CurrentTime` to another value like so\n$UnixTimeInMilliseconds = [Math]::Floor( ((get-date $MyDateValue) - $UnixStartTime).TotalMilliseconds)\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#credential-management","title":"Credential Management","text":"","tags":["development","powershell"]},{"location":"development/powershell/powershell/#setup","title":"Setup","text":"<p>Look at SecretsManagement.</p> <p>SecureString should be considered deprecated. It provides a false illusion of security mostly, and it's better to approach with other methods.</p> <ul> <li>Obsolete the SecureString Type Discussion</li> <li>DE0001</li> </ul> <p>Depending on your technology stack, the best way handle this is to authenticate using a credential library.</p> <p>For example, if pulling service account credentials for SQL Server in AWS, you could put these in AWS Secrets Manager and use something like:</p> <pre><code>Import-module AWS.Tools.SecretsManager\nWrite-Warning \"Poor Example\"\nWrite-Warning \"This exposes the credential in local memory and potentionally logging. It is for demo purposes\"\n$SecretString = (Get-SECSecretValue 'service-accounts/sql-admin' -ProfileName 'myprofile').SecretString | ConvertFrom-Json -Depth 1\n$SecretString.UserName\n$SecretString.Password\n\nWrite-Host \"A better way, you could use this to directly create a secure credential object without creating a variable to store the plain text\"\n(Get-SECSecretValue 'service-accounts/sql-admin' -ProfileName 'myprofile').SecretString |\nConvertFrom-Json -Depth 1 |\nForEach-Object {\n    [pscredential]::new( $_.username, ($_.password | ConvertTo-SecureString -AsPlainText -Force))\n}\n</code></pre> <p>The SecretsManagement powershell module supports a variety of backends such as 1Password, Thycotic Secrets Server, Lastpass, and more.</p> <p>\u2197\ufe0f List of Secrets Management Modules</p>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#using-basic-authorization-with-rest","title":"Using Basic Authorization With REST","text":"<p>When leveraging some api methods you need to encode the header with basic authentication to allow authenticated requests.</p> <pre><code>#seems to work for both version 5.1 and 6.1\nparam(\n    $Uri = ''\n)\n# Load From SecretsManagement module ideally\n$AccessId = $cred.GetNetworkCredential().UserName\n$AccessKey = $cred.GetNetworkCredential().Password\n$Headers = @{\n    Authorization = 'Basic ' + [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes( ('{0}:{1}' -f $AccessId, $AccessKey) ))\n}\n$results = Invoke-RestMethod -Uri $Uri -Header $Headers\n$results\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#load-functions-from-a-folder","title":"Load Functions from a Folder","text":"<p>Prefer to use modules, but for quick adhoc work, you can organize your work in a folder and use a subfolder called functions. I find this better than trying to create one large script file with multiple functions in it.</p> <pre><code>$FunctionsFolder = Join-Path $toolsDir 'functions'\nGet-ChildItem -Path $FunctionsFolder -Filter *.ps1 | ForEach-Object {\n    Write-PSFMessage -Level Verbose -Message \"Loading: $($_.FullName)\"\n    . \"$($_.FullName)\"\n}\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#select-object-manipulation","title":"Select Object Manipulation","text":"","tags":["development","powershell"]},{"location":"development/powershell/powershell/#expanding-nested-objects","title":"Expanding Nested Objects","text":"<p>One thing that I've had challenges with is expanding nested objects with AWSPowershell, as a lot of the types aren't formatted for easy usage without expansion. For example, when expanding the basic results of <code>Get-EC2Instance</code> you can try to parse out the state, but it won't behave as expected.</p> <p>For example, if you run:</p> <pre><code>(Get-EC2Instance -Filter $ec2Filter).Instances | Select-Object InstanceId, State\n</code></pre> InstanceId State i-taco1 Amazon.EC2.Model.InstanceState i-taco2 Amazon.EC2.Model.InstanceState i-taco3 Amazon.EC2.Model.InstanceState <p>Trying to expand gets you the state, but now you don't have the original object alongside it.</p> Code Name 16 running 16 running 16 running <p>PSFramework makes this easy to work with by simply referencing the object properties for parsing in the <code>Select-PSFObject</code> statement.</p> <pre><code>(Get-EC2Instance -Filter $ec2Filter).Instances | Select-PSFObject InstanceId, 'State.Code as StateCode', 'State.Name as StateName'\n</code></pre> <p>The result is exactly you'd you need to work with</p> InstanceId StateCode StateName i-taco1 16 running i-taco2 16 running i-taco3 16 running","tags":["development","powershell"]},{"location":"development/powershell/powershell/#get-set-accessors-in-powershell","title":"Get / Set Accessors in PowerShell","text":"<p>In C# you use get/set accessors to have more control over your properties. In PowerShell, thanks to PSFramework, you can simplify object pipelines by using <code>Select-PSFObject</code> to do the equivalent and have a scripted property that handles a script block to provide a scripted property on your object.</p> <p>For example, in leveraging AWS Profiles, I wanted to get a region name mapped to a specific profile as a default region. You can do this in a couple steps using <code>ForEach-Object</code> and leverage <code>[pscustomobject]</code>, or you can simplify it greatly by running <code>Select-PSFObject</code> like this:</p> <pre><code>Get-AWSCredentials -ListProfileDetail  |  Select-PSFObject -Property ProfileName -ScriptProperty @{\n    region = @{\n        get = {\n            switch ($this.ProfileName)\n            {\n                'taco1' {'us-east-1'}\n                'taco2' {'us-east-1'}\n                'taco3' {'us-east-1'}\n                'taco4' {'eu-west-1'}\n                'taco5' {'us-west-2'}\n                'taco6' {'us-east-1'}\n                default {'us-east-1'}\n            }\n        }\n    }\n}\n</code></pre> <p>Another good example might be the desire to parse out the final key section from S3, to determine what the file name would actually be for easier filtering or searching. In this case, a simple script property could parse out the name, and then return the last item in the array using Powershell's shortcut of <code>$array[-1]</code> to get the last item.</p> <pre><code>Get-S3Object -BucketName 'tacoland' | Select-PSFObject -ScriptProperty @{\n    BaseName = @{\n        get  = {\n            (($this.Name  -split '/')[-1])\n        }\n    }\n}\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#parallel-tips-tricks","title":"Parallel Tips &amp; Tricks","text":"<p>If you are using <code>-Parallel</code> with the newer runspaces feature in PowerShell 7 or greater, then long running operations such as queries or operations that take a while might be difficult to track progress on. In my case, I wanted to be able to see the progress for build process running in parallel and found using the synchronized hashtable I was able to do this.</p> <pre><code>$hash = [hashtable]::Synchronized(@{})\n$hash.counter = 1\n@(1..100) | ForEach-Object -Throttle 8 -Parallel {\n    $hash = $using:hash\n    $hash.counter++\n    Write-Host \"Progress: $($hash.counter)\"\n    Start-Sleep -Milliseconds (Get-Random -Minimum 1 -Maximum 10)\n}\n$hash.counter\n</code></pre> <p>I put the delay in there to show that the asynchronous nature doesn't mean 1-100, it could do some faster than others and this shows on the output with content like:</p> <pre><code>Progress: 1\nProgress: 2\nProgress: 3\nProgress: 4\nProgress: 6 &lt;---- parallel, no promise of which runspace finishes first\nProgress: 5\n</code></pre> <p>A more advanced way to use this might be to help guage how long something might take to complete when running parallel SQL Server queries.</p> <pre><code>#################################################################\n# Quick Estimate of Pipeline Completion with Parallel Runspaces #\n#################################################################\n\n$TotalToProcess = 150\n$StopwatchProcess = [diagnostics.stopwatch]::StartNew()\n\n$hash = [hashtable]::Synchronized(@{ })\n$hash.counter = 1\n\n\n\n@(1..$TotalToProcess ) | ForEach-Object -Throttle 8 -Parallel {\n    $d = $_\n    $PerItemStopwatch = [diagnostics.stopwatch]::StartNew()\n    $hash = $using:hash\n\n    $hash.counter++\n    $x = $hash.counter\n\n    $ApproxSecondsEachSoFar = $hash.counter / $using:StopwatchProcess.Elapsed.TotalSeconds\n    $RemainingCount = $using:TotalToProcess - $x\n    $ApproxRemainingTime =  $RemainingCount * $ApproxSecondsEachSoFar\n    Write-Host ( \"{0:hh\\:mm\\:ss\\.fff} | {1} | $x of $using:TotalToProcess | remaining time {3} | {2}\" -f $PerItemStopwatch.Elapsed, 'Run-ThisStuff', $d, [timespan]::FromSeconds($ApproxRemainingTime).ToString())\n    Start-Sleep -Milliseconds (Get-Random -Minimum 1 -Maximum 1500)\n}\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#the-various-iteration-methods-possible","title":"The Various Iteration Methods Possible","text":"<p>PowerShell supports a wide range of iteration options. Not all are idiomatic to the language, but can be useful to know about.</p> <p>I recommend when possible to default to <code>$Items | ForEach-Object { }</code> as your default approach. This ensures a pipeline driven solution that can be enhanced later or piped to other cmdlets that are compatible with the pipeline.</p> <p>Gotcha</p> <p>This is a cmdlet, not a PowerShell language feature. This means that the behavior of break, continue, and return all operate differently in this ForEach-Object process block than when doing a <code>foreach</code> loop.</p> <p>These are ranked in the order I recommend using by default.</p> <p>Setup the results to test with.</p> <pre><code>$Items = Get-ChildItem\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#foreach-object","title":"ForEach-Object","text":"<pre><code>$Items | ForEach-Object { $_.Name.ToString().ToLower() }\n</code></pre> <ul> <li>The default go to for major loop work.</li> <li>Default first positional argument is <code>-Process {}</code> but mostly that is not provided and just the curly braces.</li> <li>It is by default the slowest on a scale of raw performance.</li> <li>Each item is loaded into memory, and it frees memory as it goes through pipeline.</li> <li>Pipelines can be chained passing input as the pipeline progresses.</li> <li> <p>Break, continue, return behave differently as you are using a function, not a language operator.</p> </li> <li> <p>Magic operator.</p> </li> <li>Seriously, I've seen it called that.</li> <li>It's only in version &gt;= 4 Magic Operators.</li> <li>Loads all results into memory before running, so can be great performance boost for certain scenarios that a <code>ForEach-Object</code> would be slower at.</li> </ul>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#foreach-magic-operator","title":"ForEach Magic Operator","text":"<pre><code>$Items.ForEach{ $_.Name.ToString().ToLower() }\n</code></pre> <ul> <li>This is the standard <code>foreach</code> loop.</li> <li>It is the easiest to use and understand for someone new to PowerShell, but highly recommend that it is used in exceptions and try to stick with <code>ForEach-Object</code> as your default for idiomatic PowerShell if you are learning.</li> <li>Standard break, continue, return behavior is a bit easier to understand.</li> </ul>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#foreach-loop","title":"foreach loop","text":"<pre><code>foreach ($item in $Items) { $_.Name.ToString().ToLower() }\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#linq","title":"Linq","text":"<p>If you find yourself exploring delegate functions in PowerShell, you should probably just use C# or find a different language as you are probably trying to screw a nail with a hammer. \ud83d\ude01</p> <pre><code>$f = [System.Func[string, string]] { param($i) $i.ToString().ToLower() }\n$f.Invoke($Items.Name)\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#cool-tricks","title":"Cool Tricks","text":"<p>Output the results of your code into a Console GUI Gridview. This recent module provides a fantastic solution to allowing filtering and selection of results passed into it.</p> <p>Install it with: <code>Install-Module Microsoft.PowerShell.ConsoleGuiTools -Scope CurrentUser -Confirm:$false</code></p> <pre><code># Gather filtered list of EC2 Instances from AWS and then provide a console gui to select and filter the results further\n$Filters = @([Amazon.EC2.Model.Filter]::new('tag:{{TAGHERE}}','{{TAG VALUE}}')\n(Get-EC2Instance -Filter $Filters)).Instances| Select-PSFObject InstanceId, PublicIpAddress,PrivateIpAddress,Tags,'State.Code as StateCode', 'State.Name as StateName'  -ScriptProperty @{\n    Name = @{\n        get  = {\n            $this.Tags.GetEnumerator().Where{$_.Key -eq 'Name'}.Value\n        }\n    }\n} | Out-ConsoleGridView\n</code></pre> <p>For quick access, save this to a Visual Studio Code snippet like below:</p> <pre><code>\"ec2-filtered-list\": {\n    \"prefix\": \"ec2-filtered-list\",\n    \"description\": \"Get EC2 Filtered results and output to interactive ConsoleGridView\",\n    \"body\": [\n        \"# Gather filtered list of EC2 Instances from AWS and then provide a console gui to select and filter the results further\",\n        \"\\\\$Filters = @([Amazon.EC2.Model.Filter]::new('tag:${1:TagKey}','${2:TagValue}')\",\n        \"(Get-EC2Instance -Filter \\\\$Filters)).Instances| Select-PSFObject InstanceId, PublicIpAddress,PrivateIpAddress,Tags,'State.Code as StateCode', 'State.Name as StateName'  -ScriptProperty @{\",\n        \"    Name = @{\",\n        \"        get  = {\",\n        \"            \\\\$this.Tags.GetEnumerator().Where{\\\\$_.Key -eq 'Name'}.Value\",\n        \"        }\",\n        \"    }\",\n        \"} | Out-ConsoleGridView\"\n    ]\n}\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#puzzles-fizz-buzz","title":"Puzzles - Fizz Buzz","text":"<p>I did this to participate in Code Golf, and felt pretty good that I landed in 112 \ud83e\udd23 with this. Really pains me to write in the code-golf style.</p>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#file-manipulation","title":"File Manipulation","text":"<p>Rename Images With a Counter in A Directory</p> <pre><code>$dir = 'images'\n$imagegroup = 'my-images'\nGet-ChildItem $dir -Filter *.png | ForEach-Object -Begin { $x = 0 } -Process {\n    $file = $_.FullName\n    $newname = Join-Path ($file | Split-Path  ) \"$imagegroup-$x.png\"\n    $x++\n    Rename-Item $file $newname\n}\n</code></pre>","tags":["development","powershell"]},{"location":"development/powershell/powershell/#compare-seperate-machine-files","title":"Compare Seperate Machine Files","text":"<p>When I was working on a project involving Windows and an installed product, I found some mismatches in the library files that were on a machine.</p> <p>Since the dlls were mismatching this caused issues with the application.</p> <p>Here's a way to simplify checking for this in the future.</p> <pre><code>$MyDirectory = ''\nFunction Format-FileSize {\nParam (\n[int]$size\n)\n  If ($size -gt 1TB)\n  {[string]::Format(\"{0:0.00} TB\", $size / 1TB)}\n  ElseIf ($size -gt 1GB)\n  {[string]::Format(\"{0:0.00} GB\", $size / 1GB)}\n  ElseIf ($size -gt 1MB)\n  {[string]::Format(\"{0:0.00} MB\", $size / 1MB)}\n  ElseIf ($size -gt 1KB)\n  {[string]::Format(\"{0:0.00} kB\", $size / 1KB)}\n  ElseIf ($size -gt 0)\n  {[string]::Format(\"{0:0.00} B\", $size)}\n  Else\n  {\"\"}\n}\n$results = Get-ChildItem -Path $MyDirectory -Recurse  | Where-Object {$_.PSIsContainer -eq $false} | ForEach-Object {\n  [System.IO.FileInfo]$FileInfo = Get-Item $_.FullName\n  try {\n    $FileVersion = $FileInfoVersionInfo.FileVersion\n  } catch {\n    $FileVersion = $null\n  }\n  try {\n    $AssemblyVersion = [Reflection.AssemblyName]::GetAssemblyName($FileInfo.FullName).Version\n  } catch {\n    $AssemblyVersion = $null\n  }\n  [PSCustomObject]@{\n    Name          = $FileInfo.Name\n    Size          = Format-FileSize $FileInfo.Length\n    FileVersion   = $FileVersionAssembly\n    Version       = $AssemblyVersion\n    Directory     = $FileInfo.Directory\n  }\n}\n$results | Export-CLIXML -path report.xml -Encoding UTF8 -Force\n</code></pre> <p>Once you run this on a system that works like you expect you\u2019ll get a serialized xml file that can be used to compare what another machine might have.</p> <p>You could then compare the results with:</p> <pre><code>$Original = Import-CLIXML -path $OriginalFile\n$Expected = Import-CLIXML -path $ExpectedFile\n\nCompare-Object -ReferenceObject $Original -DifferenceObject $Expected -Property Name,Size,FileVersion,AssemblyVersion\n</code></pre> <p>The output should look similar to this:</p> <pre><code>Name            : Newtonsoft.Json.dll\nSize            : 443.11 kB\nFileVersion     :\nAssemblyVersion : 9.0.0.0\nSideIndicator   : =&gt;\n\nName            : Newtonsoft.Json.dll\nSize            : 509.00 kB\nFileVersion     :\nAssemblyVersion : 8.0.0.0\nSideIndicator   : &lt;=\n</code></pre> <p>If you are dealing with the dreaded GAC (Global Assembly Cache) try using this model to a comparison as well.</p> <pre><code>if(@(Get-Module GAC -ListAvailable -EA 0).Count -eq 0){\nInstall-Module GAC -Scope CurrentUser\n}\n\nImport-Module Gac\nGet-GACAssembly -name '*AssemblyNames*' | Export-CLIXML -path report.xml\n</code></pre>","tags":["development","powershell"]},{"location":"development/shell/","title":"Shell","text":"<p>Focused on specific usage of zsh, bash, etc.</p>"},{"location":"development/shell/shell/","title":"shell","text":"<p>:(fas fa-info-circle fa-fw): This is a mix of shell, linux, and macOS commands. Comments are welcome with any corrections or suggestions.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#cli-usage","title":"CLI Usage","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#powershell-bash-comparison","title":"PowerShell &amp; Bash Comparison","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#brevity","title":"Brevity","text":"<p>See all aliases with <code>Get-Alias</code> and to expedite your cli usage you could use a gist like this: Aliaser.ps1</p> <p>Note that PowerShell eschews brevity for clarity, but you can alias anything you like to be nearly as succint as bash commands.</p> <p>IMO readability/brevity trumps succintness. However for interactive terminal usage aliasing can be a great tool. Use VSCode to auto-expand aliases into fully qualified functions if you decide to turn your adhoc work into a script file.</p> <p>Using <code>pushd</code> in a PowerShell session actually aliases to <code>Push-Location</code>. The difference is in addition to the path string pushd manages, you get a .NET object back with <code>[System.Management.Automation.PathInfo]</code> information including: Drive, Path, Provider, ProviderPath.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#powershell-works-with-native-tooling","title":"PowerShell Works With Native Tooling","text":"<p>I've included the similar PowerShell command to help those jumping between multiple shells.</p> <p>Please note that unlike Python, PowerShell works as a terminal with native tools + scripting language.</p> <p>You can use <code>pwsh</code> in almost every case in Linux &amp; macOS and use the same tools you prefer, while being able to execute PowerShell commands as well.</p> <p>For example something like aws cli returning json could be automatically unmarshaled into an object instead of using <code>jq</code></p> <pre><code>&amp; (aws ec2 describe-instances | ConvertFrom-Json).Instances.InstanceId\n</code></pre> <p>Another example is paths.</p> <p>Prerequiresites for the PowerShell examples:</p> <pre><code>Install-Module Microsoft.PowerShell.ConsoleGuiTools -Scope CurrentUser -Force\n</code></pre> Command shell pwsh View history <code>history</code> <code>Get-History</code> Execute Line from History <code>!Number</code> <code>Invoke-Expression (Get-History \\| Out-ConsoleGridView -OutputMode Single).CommandLine</code> Execute Last Command But With Sudo <code>sudo !!</code> Test file exists <code>test -f ./filename</code> <code>Test-Path $filename -PathType Leaf</code> or using .NET <code>[io.file]::exists($filename)</code>","tags":["development","shell"]},{"location":"development/shell/shell/#installation","title":"Installation","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#common-app-installs","title":"Common App Installs","text":"Application Notes Install Command HomeBrew Works on Linux and macOS now \ud83d\udc4f. <code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"</code>","tags":["development","shell"]},{"location":"development/shell/shell/#ansible-initialization","title":"Ansible Initialization","text":"<pre><code>#!/usr/bin/env bash\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\nbrew install python3 ansible\n</code></pre> <p>For docker/linux</p> <pre><code>sudo python3 -m pip install --upgrade pip\nCFLAGS=-Qunused-arguments CPPFLAGS=-Qunused-arguments python3 -m pip install --user ansible\necho \"======== Ansible Version Info ======== \"\nansible-galaxy --version\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#a-few-more-ansible-commands","title":"A Few More Ansible Commands","text":"Command Code Run ansible playbook against a specific tag <code>ansible-playbook main.yml --inventory inventory --ask-become-pass -t 'ui'</code> Install requirements <code>ansible-galaxy collection install community.general &amp;&amp; ansible-galaxy install --role-file requirements.yml --force --ignore-errors</code>","tags":["development","shell"]},{"location":"development/shell/shell/#installing-go-task","title":"Installing go-task","text":"<p>This tool is great for cross-platform shell scripting as it runs all the commands in the <code>Taskfile.yml</code> using a built in go shell library that supports bash syntax (and others).</p> <p>Quickly get up and running using the directions here: Install Task</p> Command Code Default Installation to local directory with debug logging enabled <code>sh -c \"$(curl -ssL https://taskfile.dev/install.sh)\" -- -d</code> Installation for user level access <code>sh -c \"$(curl -ssL https://taskfile.dev/install.sh)\" -- -d -b /usr/local/bin</code>","tags":["development","shell"]},{"location":"development/shell/shell/#installing-brew-packages","title":"Installing Brew Packages","text":"<p>This eliminates any attempt to install if the package already exists. For quick adhoc installs, this is useful.</p> <pre><code>#!/usr/bin/env bash\n\n# Minimize Homebrew updates for each run, speeding things up\nexport HOMEBREW_NO_AUTO_UPDATE=1\n\n# if linux install script, might want to include this: export PATH=\"/home/linuxbrew/.linuxbrew/bin:$PATH\"\n\n# Example of installing with a tap\nbrew tap lucagrulla/tap\npackage=cw\nbrew list $package &amp;&gt;/dev/null || brew install $package\n\n# git-delta needs an updated version, so make sure it's available\npackage=less\nbrew list $package &amp;&gt;/dev/null || brew install $package\n\npackage=git-delta\nbrew list $package &amp;&gt;/dev/null || brew install $package\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#reduce-noise-with-progress-bar","title":"Reduce Noise With Progress Bar","text":"<p>Use unzip with a progress bar to display progress, rather than the thousands of lines of output. This is an example of installing the AWS CLI v2 in a Dockerfile, while not forcing the output of each line when unzipping.</p> <p>This shows how to use the <code>pv</code> command line tool to help display progress in both a count fashion, and also by just using as a timer.</p> <pre><code>RUN apt-get -yqq update --fix-missing &amp;&amp; apt-get -yqq install pv \\\n    &amp;&amp; mkdir -p ./tmpinstall &amp;&amp; curl --silent \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"./tmpinstall/awscliv2.zip\" \\\n    &amp;&amp; COUNT=`unzip -q -l \"./tmpinstall/awscliv2.zip\" | wc -l` \\\n    &amp;&amp; mkdir -p ./tmpinstall/aws \\\n    &amp;&amp; unzip \"./tmpinstall/awscliv2.zip\" -d \"./tmpinstall/\"  | pv -l -s $COUNT &gt;/dev/null \\\n    &amp;&amp; ./tmpinstall/aws/install --update | (pv --timer --name \"\ud83e\udd16 awscli\")  \\\n    &amp;&amp; rm -rf ./tmpinstall/ \\\n    &amp;&amp; apt-get clean -y &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/library-scripts\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#check-for-and-install-tooling","title":"Check for And Install Tooling","text":"<p>This can help give you an example of how to double check that some installed tools are available as part of a setup script.</p> <pre><code>if command -v ghq &amp;&gt;/dev/null; then\n    echo '\u2714\ufe0f ghq installed'\nelse\n    warning \"\u274c failed to find ghq, attempting to setup via source\"\n    go install github.com/x-motemen/ghq@latest || echo \"\u2705 installed ghq\"\nfi\nif command -v gum &amp;&gt;/dev/null; then\n    echo '\u2714\ufe0f gum installed'\nelse\n    warning \"\u274c failed to find gum, attempting to setup via source\"\n    go install github.com/charmbracelet/gum@latest || echo \"\u2705 installed gum\"\nfi\nif ! command -v gum &amp;/dev/null; then\n  echo 'might need go binaries on path, trying now..., try adding the line to your .zshrc'\n  export PATH=\"$(go env GOPATH)/bin:${PATH}\"\nfi\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#conditional","title":"Conditional","text":"<p>Only Proceed If First Condition Returns Nothing</p> <pre><code>package=cw\nbrew list $package &amp;&gt;/dev/null || brew install $package\n</code></pre> <p>On error do this:</p> <pre><code>test -f nonexistentfile || echo \"\ud83d\ude22 boo. file does not exist\"\n</code></pre> <p>On success do the next command:</p> <pre><code>test -f ~/.bashrc &amp;&amp; echo \"\u2705 congrats, you have a bashrc file\"\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#web-requests","title":"Web Requests","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#fetch-a-github-release","title":"Fetch A GitHub Release","text":"<p>This contains a few things, including curl, jq parsing, and movement commands.</p> <p>This provides a shell script example of using those to get the latest release from GitHub, parse the json, then move this to target path. This release doesn't wrap in a tar file; it's just a binary.</p> <p>This might fail due to anonymous API hits on GitHub api are rate limited aggressively.</p> <pre><code>#!/usr/bin/env bash\n\necho \"Grabbing latest release of fetch (a github cli for release downloads)\"\nUSER=gruntwork-io\nREPO=fetch\nTAG=latest\nASSET=fetch_linux_amd64\nFILE=fetch\ncurl --silent \"https://api.github.com/repos/$USER/$REPO/releases/latest\" \\\n| jq -r \".assets[] | select(.name | test(\\\"${ASSET}\\\")) | .browser_download_url\" \\\n| wget -qi - --output-document=$FILE --progress=bar:force\n\necho \"setting as executable and moving to /usr/local/bin\"\nchmod +x $FILE\nsudo mv fetch /usr/local/bin\necho \"Downloaded $(fetch --version) successfully\"\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#fetch-a-github-release-that-requires-extraction","title":"Fetch a GitHub Release That Requires Extraction","text":"<p>This is more of a Linux focused shell script example for grabbing a release and extracting the tar file.</p> <pre><code>#!/usr/bin/env bash\nsudo apt -qqy update\nsudo apt -qqy -o Dpkg::Progress-Fancy=true install wget\n\ncurl -s https://api.github.com/repos/GitTools/GitVersion/releases/latest \\\n| grep \"browser_download_url.*gitversion\\-debian.*\\-x64.*\\.tar\\.gz\" \\\n| cut -d \":\" -f 2,3 \\\n| tr -d \\\" \\\n| wget -qi -\n\ntarball=\"$(find . -name \"gitversion-debian*.tar.gz\")\"\ntar -xzf $tarball\n\nsudo chmod +x gitversion\nsudo mv gitversion /usr/local/bin\n\nsudo rm $tarball\necho \"&gt;&gt;&gt;&gt; gitversion version: $(~/gitversion /version)\"\necho \"Trying to install dotnet tools version\"\ndotnet tool update --global GitVersion.Tool\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#concepts","title":"Concepts","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#shebang","title":"Shebang","text":"<p>A common pattern is just <code>#!/bin/bash</code>.</p> <p>To make your script more portable, by respecting the users env preferences try:</p> <ul> <li><code>#!/usr/bin/env bash</code></li> <li><code>#!/usr/bin/env zsh</code></li> <li><code>#!/usr/bin/env sh</code></li> </ul> <p>bash.cyberciti.biz reference</p> <p>Some good info on this from Shebang</p> <p>:(fas fa-code fa-fw): If you do not specify an interpreter line, the default is usually the <code>/bin/sh</code></p> <p>:(fas fa-code fa-fw): For a system boot script, use <code>/bin/sh</code></p> <p>:(fas fa-code fa-fw): The <code>/usr/bin/env</code> run a program such as a bash in a modified environment. It makes your bash script portable. The advantage of #!/usr/bin/env bash is that it will use whatever bash executable appears first in the running user's <code>$PATH</code> variable.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#ssh","title":"SSH","text":"<p>Setup your permissions for <code>~/.ssh</code></p> <pre><code>echo \"Setting full user permissions for ~/.ssh\"\nchmod -R u+rwX ~/.ssh\necho \"Remove group access for ~/.ssh\"\nchmod go-rwx ~/.ssh\necho \"now set any pem files to chmd 400 \\$key to ensure read-only\"\nchmod 0600 ~/.ssh/id_rsa\n</code></pre> <p>For why 0600 see footnote.<sup>1</sup></p> <p>Troubleshooting macOS permissions</p> <p>I've had issues with macOS adding an <code>@</code> with ACL issues on the ssh key's when downloaded.</p> <p>To resolve this, just copy the contents of the ssh key to a new file and remove the original.</p> <pre><code>cat original_key.pem &gt; key.pem\n</code></pre> <p>How To List Users In Linux</p>","tags":["development","shell"]},{"location":"development/shell/shell/#search-contents-of-a-file","title":"Search Contents of a File","text":"<p>Using <code>ripgrep</code> you can search very quickly through file contents.</p> <p>In this example, I found a text string in a PowerShell file that VSCode wasn't able to find after 1-2 mins due to the size of the directory.</p> <pre><code>rg -l -c \"Start-ThreadJob\" *.ps1\n</code></pre> <p>Benchmark</p> <p>I ran a quick test to see how ripgrep performed compared to normal grep search. Grep wasn't optimized, and by default is single threaded. Ripgrep is multithreaded, automatically honors gitignore and more.</p> <pre><code>grep -rnw $HOME -e 'Start-ThreadJob'\n</code></pre> Tool Time <code>ripgrep</code> 5m6s <code>grep</code> 1h+timeout","tags":["development","shell"]},{"location":"development/shell/shell/#using-yq-to-edit-yaml-files-for-datadog-service","title":"Using yq to edit yaml files for Datadog service","text":"<p>GitHub - mikefarah/yq: yq is a portable command-line YAML processor</p> <p>I've use yq to edit yaml files programatically, such as datadog configuration files.</p> <p>Here's a few samples on how to use this tool, using datadog agent config files as an example.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#quick-install-of-datadog-service","title":"Quick Install of Datadog Service","text":"<pre><code>DD_HOST_TAGS=\"type:custom-server,stage:dev\"\nDD_HOSTNAME=\"custom-server\"\n\nDD_AGENT_MAJOR_VERSION=7 DD_API_KEY=FancyAPIKey DD_SITE=\"datadoghq.com\" bash -c \"$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)\"\nsudo chmod -R 777 /etc/datadog-agent/\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#start-and-stop-the-datadog-services","title":"Start and stop the datadog services","text":"<pre><code>sudo systemctl stop datadog-agent\nsudo systemctl start datadog-agent\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#edit-default-datadog-config-file","title":"Edit Default Datadog Config File","text":"<p>Next, configure the main configuration with custom tags and host name, including additional ec2 tags, metadata, and a custom tag to show the specific load test this is capturing.</p> <pre><code>config=/etc/datadog-agent/datadog.yaml\nnametag=mycustom-server\ntestname=bigloadtest\n\necho \"set the basic config for app\"\nyq eval \"\n.hostname = \\\"$nametag\\\" |\n.process_config.enabled = true |\n.tags = [\\\"scope:loadtest\\\",\\\"testname:$testname\\\"] |\n.env = \\\"dev\\\" |\n.cloud_provider_metadata = [\\\"aws\\\"] |\n.collect_ec2_tags = true\" --inplace $config\nyq eval \".hostname, .process_config.enabled, .tags, .env, .cloud_provider_metadata ,.collect_ec2_tags\" $config\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#enable-datadog-network-monitoring","title":"Enable Datadog Network Monitoring","text":"<pre><code>echo \"set the process level config to search for ssh/sshd metrics\"\nsudo cp /etc/datadog-agent/system-probe.yaml.example /etc/datadog-agent/system-probe.yaml\nnetconfig=/etc/datadog-agent/system-probe.yaml\nyq eval '.network_config.enabled' $netconfig\nyq eval --inplace  '\n.network_config.enabled = true\n' $netconfig\nyq eval '.network_config.enabled' $netconfig\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#enable-datadog-process-level-tracking","title":"Enable Datadog Process Level Tracking","text":"<p>Enable process level tracking, with specific matching on <code>ssh, sshd</code>.</p> <pre><code>echo \"set the process level config to search for ssh/sshd metrics\"\nsudo cp /etc/datadog-agent/conf.d/process.d/conf.yaml.example  /etc/datadog-agent/conf.d/process.d/conf.yaml\nprocessconfig=/etc/datadog-agent/conf.d/process.d/conf.yaml\nyq eval '.instances' $processconfig\nyq eval --inplace  '\n.instances[0].name = \"ssh\" |\n.instances[0].search_string = [\"ssh\",\"sshd\"]\n' $processconfig\nyq eval --inplace  '\n.instances[1].name = \"myprocess\" |\n.instances[1].search_string = [\"myprocess\"]\n' $processconfig\nyq eval '.instances' $processconfig\n</code></pre> <p>You can do a lot with <code>yq</code>.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#parse-kubernetes-secrets-using-jq","title":"Parse Kubernetes Secrets Using JQ","text":"<p>Using jq, you can parse out secrets from base64 encoded values for some quick scripting.</p> <p>NOTE: This uses sttr but you can modify to whatever your platform provides (zsh <code>base64 -decode</code> or pwsh <code>[System.Convert]::FromBase64String($Base64String)</code>)) If you have Go installed (everyone should! \ud83d\ude00) then run <code>go install github.com/abhimanyu003/sttr@latest</code>.</p> <p>This example parses an encoded json string to help registry an Azure Container Registry from a Kubernetes stored secret.</p> <pre><code>namespace=\"mynamespace\"\nsecretname=\"mysecretname\"\n\nkubectl config set-context --current --namespace=$namespace\nconfigEncoded=$(kubectl get secret $secretname -o jsonpath='{.data.\\.dockerconfigjson}')\nconfigDecoded=$(sttr base64-decode $config)\nregistry=$(echo $configDecoded | jq '.auths | keys[]')\necho -e \"\ud83d\udc49 registry: $registry\"\ncreds=$(echo $configDecoded | jq .auths.${registry}.auth --raw-output)\necho -e \"\ud83d\udc49 username:password: $( sttr base64-decode $creds )\"\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#github-cli","title":"GitHub CLI","text":"","tags":["development","shell"]},{"location":"development/shell/shell/#view-the-logs-of-a-prior-run","title":"View The Logs Of A Prior Run","text":"<p>View the logs of the last run (or toggle to error logs with the switch).</p> <ul> <li><code>gh run view --log $(gh run list -L1 --json 'databaseId' --jq '.[].DatabaseId')</code></li> <li><code>gh run view $(gh run list --limit 1 --json databaseId --jq '.[0].databaseId' ) --log</code></li> </ul> <p>This can be chained together with other commands to quickly iterate on testing. When appropriate, you might avoid this by running act but I've had limited success with it due to various restrictions.</p> <pre><code>git commit -am 'ci: get github release working' &amp;&amp; \\\n  git push &amp;&amp; \\\n  gh workflow run release &amp;&amp; \\\n  sleep 5 &amp;&amp; \\\n  gh run watch -i1 || gh run view --log --job $(gh run list -L1 --json 'workflowDatabaseId' --jq '.[].workflowDatabaseId')\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#use-to-configure-settings-on-many-repos-at-once","title":"Use To Configure Settings on Many Repos At Once","text":"<p>This example uses [gum]<sup>2</sup> to filter. Use <code>tab</code> when selecting in the multi-entry option.</p> <pre><code>org=$(gum input --prompt 'enter GitHub org: ')\noriginallist=$( gh repo list $org --json 'name' --jq '.[].name' |  tr ' ' '\\n' )\nrepos=\"$( echo $originallist | gum filter --no-limit )\"\n\nfor repo in $( echo $repos | tr '\\n' ' ') ;\ndo\n    printf \"processing %s ... \" \"${repo}\"\n    gh api \\\n        --method PATCH \\\n        -H \"Accept: application/vnd.github+json\" \\\n        /repos/$org/$repo \\\n        -F use_squash_pr_title_as_default=true \\\n        -F squash_merge_commit_title=PR_TITLE \\\n        -F squash_merge_commit_message=PR_BODY \\\n        --silent\n        printf \"\u2714\ufe0f\\n\"\n    # return # for testing\ndone\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#clone-all-the-desired","title":"Clone All The Desired","text":"<p>Uses gum[^gum-repo] &amp; [ghq]<sup>3</sup>. See setup directions.</p>","tags":["development","shell"]},{"location":"development/shell/shell/#configure-ghq","title":"Configure ghq","text":"<p>To configure <code>ghq</code> defaults run:</p> <pre><code>git config --global ghq.vcs git\ngit config --global ghq.root $(gum input -prompt 'base git directory for repos: (recommend ~/git):  ' )\n</code></pre>","tags":["development","shell"]},{"location":"development/shell/shell/#clone-all-repos-selected","title":"Clone All Repos Selected","text":"<pre><code>org=$(gum input --prompt 'enter GitHub org: ')\noriginallist=$( gh repo list $org --json 'name' --jq '.[].name' |  tr ' ' '\\n' )\necho 'select repos (use tab to select, and type to filter)'\nrepos=\"$( echo $originallist | gum filter --no-limit )\"\n\nfor repo in $( echo $repos | tr '\\n' ' ') ;\ndo\n    printf \"processing %s ... \" \"${repo}\"\n    ghq get \"https://github.com/${org}/${repo}\" &amp;&gt; /dev/null\n    printf \"\u2714\ufe0f\\n\"\ndone\n</code></pre> <ol> <li> <p>Why are ssh keys 600 and not 400 by default? authorized_keys immutable? : linux4noobs \u21a9</p> </li> <li> <p>GitHub - charmbracelet/gum: A tool for glamorous shell scripts \ud83c\udf80 \u21a9</p> </li> <li> <p>ghq - repo cloning tool \u21a9</p> </li> </ol>","tags":["development","shell"]},{"location":"development/tooling/","title":"Tooling","text":""},{"location":"development/tooling/aqua/","title":"aqua","text":"","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#aqua-overview","title":"Aqua Overview","text":"<p>A cli version manager for the discriminating cli connoisseur, this tool is great to install binaries both at global level and project level. If you are using <code>asdf</code>, I highly recommend this as an alternative, with the caveat of it not managing python, ruby, or other runtimes. It's focused on cli development tools, and providing a global or project level version configuration that automatically installs on demand.</p> <p>Aqua runs as a proxy for the invoked cli's, which means it automatically handles installing the called tool if it's missing on demand, further cutting down initial setup time.</p> <p>You can even use in docker images or CI and have a single version pinned file helping the local and CI tools be similar.</p> <p>It's more secure than <code>asdf</code> by default.</p>","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#quick-start","title":"Quick Start","text":"<p>Quick Start includes install commands to setup.</p> <p>I use curl based install mostly: Install</p> install with brewinstall with go brew install<pre><code>brew install aquaproj/aqua/aqua\n</code></pre> pwsh go install<pre><code>pwsh -NoLogo -Command \"go install github.com/aquaproj/aqua/v2/cmd/aqua@latest\"\n</code></pre>","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#update-your-path","title":"Update Your Path","text":"macOS/Linuxwindows $HOME/.zshenv<pre><code>export PATH=\"${AQUA_ROOT_DIR:-${XDG_DATA_HOME:-$HOME/.local/share}/aquaproj-aqua}/bin:$PATH\"\n</code></pre> <p>The options below are one for max flexibility in honoring the XDG_Directory standard, the other just assumes <code>.local</code>.</p> $PROFILE<pre><code>$ENV:XDG_CONFIG_HOME = $ENV:XDG_CONFIG_HOME ? $ENV:XDG_CONFIG_HOME : (Join-Path $HOME '.config')\n$ENV:XDG_CACHE_HOME = $ENV:XDG_CACHE_HOME ? $ENV:XDG_CACHE_HOME : (Join-Path $HOME '.cache')\n$ENV:XDG_DATA_HOME = $ENV:XDG_DATA_HOME ? $ENV:XDG_DATA_HOME : ($HOME, '.local', 'share' -join [IO.Path]::DirectorySeparatorChar)\n\n$ENV:PATH = ([io.path]::Combine($HOME,'.local','share','aquaproj-aqua', 'bin')), $ENV:PATH -join [IO.Path]::PathSeparator\n# OR FOR MAX FLEXIBILITY\n$ENV:PATH = ([io.path]::Combine($ENV:XDG_DATA_HOME, 'aquaproj-aqua', 'bin')), $ENV:PATH -join [IO.Path]::PathSeparator\n</code></pre> Windows: System Environment Variables (Outside Process Scope)<pre><code>[Environment]::SetEnvironmentVariable('PATH', ((Join-Path $ENV:LOCALAPPDATA 'aquaproj-aqua' 'bin') , $ENV:PATH -join [IO.Path]::PathSeparator), 'Machine')\n</code></pre> $PROFILE.CurrentUserAllHosts<pre><code>$ENV:PATH = ((Join-Path $ENV:LOCALAPPDATA 'aquaproj-aqua' 'bin') , $ENV:PATH -join [IO.Path]::PathSeparator)\n</code></pre>","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#global-tooling-setup","title":"Global Tooling Setup","text":"<p>To create these files, navigate to the directory and run <code>aqua init &amp;&amp; aqua init-policy</code>.</p> <p>Run <code>aqua policy allow \"${XDG_CONFIG_HOME:-$HOME/.config}/aqua/aqua-policy.yaml\"</code> to allow global tooling that's customized.</p> linux/darwinwindows $HOME/.zshenv<pre><code>export PATH=\"${AQUA_ROOT_DIR:-${XDG_DATA_HOME:-$HOME/.local/share}/aquaproj-aqua}/bin:$PATH\"\nexport AQUA_GLOBAL_CONFIG=${XDG_CONFIG_HOME:-$HOME/.config}/aqua/aqua.yaml\n# export AQUA_POLICY_CONFIG=${XDG_CONFIG_HOME:-$HOME/.config}/aqua/aqua-policy.yaml\n</code></pre> $PROFILE<pre><code>  $RootLocation = $ENV:AQUA_ROOT_DIR ?? (Join-Path $ENV:XDG_DATA_HOME  'aquaproj-aqua' 'bin') ?? (Join-Path \"$HOME/.local/share\"  'aquaproj-aqua' 'bin')\n  $RootLocationWithBin = Join-Path $RootLocation 'bin'\n  $ENV:PATH = $RootLocationWithBin, $ENV:PATH -join [IO.Path]::PathSeparator\n</code></pre>","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#example-global-config","title":"Example Global Config","text":"<p>Here's an example of what I drop into the global config for managing my global default for Go and other common cli tools.</p> aqua.yamlregistry.yamlaqua-policy.yaml ${XDG_CONFIG_HOME:-$HOME/.config}/aqua/aqua.yaml<pre><code>---\n# aqua - Declarative CLI Version Manager\n# https://aquaproj.github.io/\nchecksum:\n  enabled: true\n  require_checksum: false\nregistries:\n  - type: standard\n    ref: v4.15.0 # renovate: depName=aquaproj/aqua-registry\n  - name: local\n    type: local\n    path: registry.yaml\npackages:\n  - name: golang/go@go1.20.4\n    tags: ['first']\n  - name: git-town/git-town@v7.9.0\n  - name: golangci/golangci-lint@v1.52.2\n  - name: itchyny/gojq@v0.12.12\n  - name: dandavison/delta@0.15.1\n  - name: junegunn/fzf@0.40.0\n  - name: sharkdp/bat@v0.23.0\n  - name: magefile/mage@v1.15.0\n  - name: starship/starship@v1.14.2\n  - name: BurntSushi/ripgrep@13.0.0\n  - name: sharkdp/fd@v8.7.0\n  - name: x-motemen/ghq@v1.4.2\n  - name: helm/helm@v3.12.0\n  - name: kubernetes-sigs/kind@v0.19.0\n  - name: kubernetes/kubectl\n    version: v1.25.2\n  - name: Schniz/fnm@v1.33.1\n  - name: ajeetdsouza/zoxide@v0.9.1\n  - name: miniscruff/changie@v1.12.0\n  - name: direnv/direnv@v2.32.3\n  # ... more packages here like minikube, charm tooling like gum, etc. All lazy installed `--only-link` or pre-installed with normal `aqua i`.\n</code></pre> <p>This is the custom location for packages not in the standard registry. While I recommend contributing upstream (it's really simple), sometimes less shareable tools for specific needs make sense to include here.</p> ${XDG_CONFIG_HOME:-$HOME/.config}/aqua/registry.yaml<pre><code>---\npackages:\n- type: go_install\n  name: mage-select\n  path: github.com/iwittkau/mage-select\n  description: CLI frontend for mage based on promptui.\n  search_words:\n    - mage\n    - module\n    - go\n- type: github_release\n  repo_owner: alihanyalcin\n  repo_name: gomup\n  link: https://github.com/alihanyalcin/gomup/releases/\n  asset: 'gomup_{{trimV .Version}}_{{.OS}}_{{.Arch}}.tar.gz'\n  description: gomUP is a tool to keep track of outdated dependencies and upgrade them to the latest version. Designed for monorepo Go projects and Go projects that contain multiple modules.\n  search_words:\n    - gomod\n    - module\n  replacements:\n    darwin: Darwin\n    linux: Linux\n    windows: Windows\n    386: i386\n    amd64: x86_64\n</code></pre> <p>Used to allow the custom tooling that aqua can handle outside the standard packages. For example, custom <code>cargo install</code> or <code>go install</code> packages.</p> ${XDG_CONFIG_HOME:-$HOME/.config}/aqua/aqua-policy.yaml<pre><code>---\n# aqua Policy\n# https://aquaproj.github.io/docs/tutorial-extras/policy-as-code\nregistries:\n  - type: standard\n    ref: semver(\"&gt;= 3.0.0\")\n  - name: local\n    type: local\n    path: registry.yaml\npackages:\n  - registry: standard\n  - registry: local\n</code></pre>","tags":["cli","tooling"]},{"location":"development/tooling/aqua/#using-with-ci","title":"Using With CI","text":"azure pipelines <p>This is focused on <code>ubuntu-latest</code> as the windows agent has some quirks not addressed in this format. This still uses <code>pwsh</code> on the ubuntu agent to avoid me having to rework logic for 2 platforms.</p> ...azure-pipelines.yaml<pre><code>- pwsh: |\n    &amp;curl -sSfL https://raw.githubusercontent.com/aquaproj/aqua-installer/v2.1.1/aqua-installer | bash -s -- -v v2.3.6\n    try {\n      $ENV:PATH = ([io.path]::Combine($HOME,'.local','share','aquaproj-aqua', 'bin')), $ENV:PATH -join [IO.Path]::PathSeparator\n    }\n    catch {\n      Write-Warning \"unable to load aqua: $($_.Exception.Message)\"\n    }\n    Write-Host \"aqua version: $(&amp; aqua version)\"\n    &amp;aqua update-aqua\n    Write-Host \"aqua version (after update): $(&amp; aqua version)\"\n    $env:PATH = (Join-Path $(aqua root-dir) 'bin'), $env:PATH -join [IO.Path]::PathSeparator\n    Write-Host \"run aqua install --tags first\"\n    &amp;aqua install --tags first\n    Write-Host \"install remaining aqua tools\"\n    aqua install --tags MYCUSTOM TAG # \ud83d\udc48 narrow down what you invoke\n    $ENV:PATH = (Join-Path $HOME '.config' @('aquaproj-aqua','bin')), $ENV:PATH -join [IO.Path]::PathSeparator\n    mage # ..... \ud83d\udc48 invoke commands now that things are installed\n</code></pre>","tags":["cli","tooling"]},{"location":"development/tooling/direnv/","title":"direnv","text":"","tags":["tooling"]},{"location":"development/tooling/direnv/#example-direnv","title":"Example Direnv","text":"<ul> <li>Load <code>.envrc</code> from your home directory.</li> <li>Check for <code>aqua</code> tool for the project, to allow tooling to be installed, warn if not found.</li> <li>Ensure <code>$GOPATH/bin</code> is available in path so that Go tooling can be resolved.   Put at end of PATH, so that aqua based tooling takes precedence.</li> </ul> .envrc<pre><code>source_env \"$HOME\"\nexport DIRENV_WARN_TIMEOUT='10s'\n# OPTIONAL: export PATH=\"${PATH}:${GOPATH}/bin\"\nexport MAGEFILE_ENABLE_COLOR=1\n\nINFO_COLOR=\"\\033[1;30;40m\"\nRESET_COLOR=\"\\033[0m\"\nWARNING_COLOR=\"\\033[33m\"\nEND_WARNING_COLOR=\"\\033[0m\"\nWORKING_COLOR=\"\\033[94m\"\nBACKGROUND_GREEN=\"\\033[94m\"\nRESET_BACKGROUND=\"\\033[0;49m\"\nBACKGROUND_LIGHT_GREEN=\"\\033[1;102;30m\"\nBACKGROUND_BLUE=\"\\033[44;30m\"\nBACKGROUND_LIGHT_YELLOW=\"\\033[1;103;30m\"\n\nif command -v aqua &gt;/dev/null 2&gt;&amp;1; then\n  printf \"${INFO_COLOR}\u2714\ufe0f\ufe0f aqua detected${RESET_COLOR}\\n\"\nelse\n  printf \"\u274c ${WARNING_COLOR}aqua command not recognized${RESET_COLOR}\\n\"\n  printf \"\\t${WORKING_COLOR}\ud83d\udc49 Please install aqua to automatically setup all dev tools for project:${RESET_COLOR}${BACKGROUND_GREEN}https://aquaproj.github.io/docs/tutorial${RESET_COLOR}\\n\"\n  printf \"\\tYou'll need to make sure the following statement is in your profile (.zshrc, .bashrc, .profile, etc)\\n\"\n  printf \"\\t${INFO_COLOR}${BACKGROUND_LIGHT_YELLOW}export PATH=\\\"\\${AQUA_ROOT_DIR:-\\${XDG_DATA_HOME:-\\$HOME/.local/share}/aquaproj-aqua}/bin:\\$PATH\\\"${RESET_COLOR}\\n\"\nfi\n</code></pre>","tags":["tooling"]},{"location":"development/tooling/docker/","title":"docker","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#buildx","title":"Buildx","text":"<p>Docker Buildx is a CLI plugin that extends the docker command with the full support of the features provided by Moby BuildKit builder toolkit. It provides the same user experience as docker build with many new features like creating scoped builder instances and building against multiple nodes concurrently.<sup>2</sup></p> <p>Enable.</p> <pre><code>DOCKER_BUILDKIT=1\n</code></pre> <p>Set as default builder</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#resources","title":"Resources","text":"Resource Description Codespaces Install scripts for debian, examples of complex builds and installs for dev tooling Codespaces Library Scripts Subdirectory with installation scripts so I don't need to rebuild the wheel \ud83d\ude00 Advanced Codespaces Configuration Cool tips on improving codespaces configuration performance, customization, and installs","tags":["development","docker"]},{"location":"development/tooling/docker/#assumptions","title":"Assumptions","text":"<p>The majority of the code examples apply to a base image of Ubuntu &amp; Debian.</p> <p>Since this image has GCC prebaked in, it's much easier to use with dotnet tools and other apps that require it.</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#arguments","title":"Arguments","text":"<p>Understand how ARG and FROM interac</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#building","title":"Building","text":"<p>Manually triggering build can be done via command below, replacing <code>codespace</code> with whatever tagged image name is desired.</p> <pre><code>docker build --pull --rm -f \".devcontainer/Dockerfile\" -t codespace:latest \".devcontainer\"```\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#installation-scripts","title":"Installation Scripts","text":"<p><code>gh repo clone microsoft/vscode-dev-containers</code> and then copy the <code>script-library</code> directory to <code>${ProjectDirectory}/.devcontainer/script-library</code></p>","tags":["development","docker"]},{"location":"development/tooling/docker/#syntax-tips","title":"Syntax Tips","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#installing-apps","title":"Installing Apps","text":"<p>Use an environment variable to cleanup the dockerfile install commands.</p> <pre><code># Additonal applications to install\nENV ADDITIONAL_APPS \\\n    curl \\\n    g++ \\\n    gcc \\\n    gettext \\\n    git \\\n    locales \\\n    locales-all \\\n    make \\\n    nano \\\n    openssh-client \\\n    rsync\nRUN apt -yqq install ${ADDITIONAL_APPS} --no-install-recommends  \\\n    &amp;&amp; apt-get -yqq autoremove \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#best-practice","title":"Best Practice","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#smaller-layers","title":"Smaller Layers","text":"<ul> <li> <p>Use dive and the associated VSCode extension to explore the layers for reducing size.</p> </li> <li> <p>Use cleanup commands on any layer to reduce it's cached size by running the clean and rm command at the end of the layer.</p> </li> <li> <p>Use <code>--no-install-recommends</code> to reduce installation size when running <code>apt-get</code></p> </li> </ul> <p>RUN apt-get -yyq update &amp;&amp; apt-get -yyq install tree --no-install-recommends \\    &amp;&amp; apt-get -yyq clean &amp;&amp; rm -rf /var/lib/apt/lists/*</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#user-configuration","title":"User Configuration","text":"<p>Using dotfiles with chezmoi, manually trigger using a command such as:</p> <pre><code>curl -sfL https://git.io/chezmoi | sh\necho \"enter github username for chezmoi repo\"\n./bin/chezmoi init --apply --verbose https://github.com/$(read)/chezmoi.git\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#environment-variables","title":"Environment Variables","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#homebrew","title":"Homebrew","text":"<p>You can install Homebrew for Linux in two different ways, using multi-stage build, or the vscode install script homebrew-debian.sh</p> <p>Multistage would do this:</p> <pre><code>ARG VARIANT=\"focal\"\nFROM homebrew/brew:latest AS DOCKERBREW\nFROM mcr.microsoft.com/vscode/devcontainers/base:${VARIANT}\n\nUSER root\nRUN useradd -m -s /bin/bash linuxbrew  &amp;&amp; \\\n    echo 'linuxbrew ALL=(ALL) NOPASSWD:ALL' &gt;&gt;/etc/sudoers\n\nUSER linuxbrew\nCOPY --from=DOCKERBREW /home/linuxbrew/.linuxbrew /usr/local/bin/brew\nUSER root\nRUN chmod -R a+rwx /usr/local/bin/brew\n\nUSER $USERNAME\nENV BREW_PREFIX=/home/linuxbrew/.linuxbrew\nENV PATH=${BREW_PREFIX}/sbin:${BREW_PREFIX}/bin:${PATH}\nRUN echo \"\u2705 brew version: $(brew --version)\"\n</code></pre> <p>Using the install script or the curl command to install might take signficantly longer as it recompiles due to gcc.</p> <pre><code>USER root\n\nENV BREW_PREFIX=/home/linuxbrew/.linuxbrew\nENV PATH=${BREW_PREFIX}/sbin:${BREW_PREFIX}/bin:${PATH}\nRUN yes | unminimize 2&gt;&amp;1 \\\n    &amp;&amp; echo \"\u26a1 Beginning homebrew-debian.sh\" &amp;&amp; bash /tmp/library-scripts/homebrew-debian.sh \\\n    &amp;&amp; echo \"\ud83c\udf89 HomeBrew Installed: [$(brew --version)]\" \\\n    &amp;&amp; apt-get -yyq clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#powershell-tooling","title":"PowerShell Tooling","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#python-3","title":"Python 3","text":"<p>Use the library scripts python-debian.sh in vscode repo to simplify installation then use the following install command.</p> <pre><code>ARG PYTHON_PATH=\"/usr/local/python\"\nARG PYTHON_VERSION=\"3.8.3\"\nARG PIPX_BIN_DIR=\"/usr/local/py-utils/bin\"\nARG PIPX_HOME=\"/usr/local/py-utils\"\nARG UPDATE_RC=\"true\"\nARG INSTALL_PYTHON_TOOLS=\"true\"\n\nUSER root\n\n# From .devcontainer/script-library/docs/python.md\nENV PIPX_HOME=${PIXY_HOME} \\\n    PIPX_BIN_DIR=${PIPX_BIN_DIR}}\nENV PATH=${PYTHON_PATH}/bin:${PATH}:${PIPX_BIN_DIR}\nRUN yes | unminimize 2&gt;&amp;1 \\\n    &amp;&amp; bash /tmp/library-scripts/python-debian.sh \"${PYTHON_VERSION}\" \"${PYTHON_PATH}\" \"${PIPX_HOME}\" \"${USERNAME}\" \"${UPDATE_RC}\" \"${INSTALL_PYTHON_TOOLS}\"  \\\n    &amp;&amp; apt-get -yyq clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nUSER $USERNAME\n\n# Verify the nonroot user has access now\nENV PATH=\"/usr/local/python${PYTHON_VERSION}:${PATH}\"\nRUN echo \"\ud83c\udf89 python-debian installed with version: [$(python3 --version)]\"\n\n\n\nRUN apt-get -yyq update &amp;&amp; apt-get -qyy install python3-venv python3-pip --no-install-recommends \\\n    &amp;&amp; apt-get -yyq clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#pre-commit-tooling","title":"Pre-Commit Tooling","text":"<p>This requires python and pip to be installed correctly (see previous section).</p> <pre><code>RUN python3 -m pip install pre-commit\n</code></pre> <p>Since pre-commit needs a repo to install the pre-commit hook to, after loading the project run <code>task precommit:init</code> if Task is installed, else run <code>pre-commit install</code> in the cloned repo.</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#go-task","title":"Go-Task","text":"<pre><code>version: '3'\nsilent: true\noutput: prefixed\nvars:\n  CONTAINERNAME: codespace-general\n  DOCKER_BUILDKIT: 1\ntasks:\n  build:\n    desc: build codespaces container\n    cmds:\n    - docker build --pull --rm -f \".devcontainer/Dockerfile\" -t {{ .CONTAINERNAME }}:latest \".devcontainer\"\n  rebuild:\n    desc: build codespaces container without using cache in case of cached changes preventing new updates from being picked up\n    cmds:\n    - docker build --pull --rm --no-cache -f \".devcontainer/Dockerfile\" -t {{ .CONTAINERNAME }}:latest \".devcontainer\"\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#full-dockerfile-examples","title":"Full Dockerfile Examples","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#codespaces-ubuntu-general-development-build","title":"Codespaces - Ubuntu General Development Build","text":"<p>This is a general purpose dev container for dev tooling such python3, Go, PowerShell, pre-commit, and other useful tools.</p> <p>It is designed to be used for any of these projects with some useful tooling like Brew, bit (git enhanced cli), git town and others.</p> <p>Tweaks to the <code>devcontainer.json</code> support mounting aws local credentials into the container, using volumes for high IO package/artifact directories, and improve drive performance by marking the container as the primary and the host directory mounted version to be ok to lag a bit.</p> <pre><code>  \"workspaceMount\": \"source=${localWorkspaceFolder},target=/home/codespace/workspace,type=bind,consistency=delegated\",\n  \"mounts\": [\n    \"source=vscodeextensions,target=/root/.vscode-server/extensions,type=volume\",\n    \"source=artifacts,target=${containerWorkspaceFolder}/artifacts,type=volume\",\n    \"source=packages,target=${containerWorkspaceFolder}/packages,type=volume\",\n    \"source=tools,target=${containerWorkspaceFolder}/tools,type=volume\",\n    \"source=${localEnv:HOME}${localEnv:USERPROFILE}/.aws/credentials,target=/home/codespace/.aws/credentials,type=bind,consistency=delegated\",\n  ],\n  \"postCreateCommand\": [\n    \"uname -a\",\n    \"pre-commit install\"\n  ],\n// Set *default* container specific settings.json values on container create.\n  \"settings\": {\n    \"terminal.integrated.profiles.linux\": {\n      \"bash\": {\n        \"path\": \"bash\"\n      },\n      \"zsh\": {\n        \"path\": \"zsh\"\n      },\n      \"fish\": {\n        \"path\": \"fish\"\n      },\n      \"tmux\": {\n        \"path\": \"tmux\",\n        \"icon\": \"terminal-tmux\"\n      },\n      \"pwsh\": {\n        \"path\": \"pwsh\",\n        \"icon\": \"terminal-powershell\"\n      }\n    }\n  },\n\n\n\n# See here for image contents: https://github.com/microsoft/vscode-dev-containers/tree/v0.163.1/containers/ubuntu/.devcontainer/base.Dockerfile\nARG VARIANT=\"focal\"\nFROM mcr.microsoft.com/vscode/devcontainers/base:${VARIANT}\n\n# Codespace/Docker User Config\nARG USER_UID=1000\nARG USER_GID=$USER_UID\nARG USERNAME=vscode\n\n# Common Debian Install Settings\nARG INSTALL_ZSH=\"true\"\nARG UPGRADE_PACKAGES=\"true\"\n\n# PYTHON INSTALL SETTINGS\nARG PYTHON_PATH=\"/usr/local/python\"\nARG PYTHON_VERSION=\"3.8.3\"\nARG PIPX_BIN_DIR=\"/usr/local/py-utils/bin\"\nARG PIPX_HOME=\"/usr/local/py-utils\"\nARG UPDATE_RC=\"true\"\nARG INSTALL_PYTHON_TOOLS=\"true\"\n\n# CONFIGURE USER FOR LINUXBREW\nUSER root\nRUN useradd -m -s /bin/bash linuxbrew  &amp;&amp; \\\n    echo 'linuxbrew ALL=(ALL) NOPASSWD:ALL' &gt;&gt;/etc/sudoers\n\n# Install Software Properties Common\n# From Stack Overflow: This software provides an abstraction of the used apt repositories.\n# It allows you to easily manage your distribution and independent software vendor software sources.\n# Without it, you would need to add and remove repositories (such as PPAs) manually by editing /etc/apt/sources.list and/or any subsidiary files in /etc/apt/sources.list.d\nUSER root\nRUN apt-get -yyq update &amp;&amp; apt-get -yyq install software-properties-common --no-install-recommends  &amp;&amp; add-apt-repository universe &amp;&amp; apt-get -yyq update &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python 3 &amp; Python tools\nRUN apt-get -qqy update &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\\n    &amp;&amp; apt-get -qqy install --no-install-recommends build-essential gcc python3 python3-pip python3-setuptools \\\n    &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN wget https://bootstrap.pypa.io/get-pip.py\nRUN python3 get-pip.py\nRUN pip install --upgrade pip\n\n# For spinning up an interactive desktop using container as host\n# Optional to allow simulated desktop dev experience without need to use VirtualBox VM's for this\n# Adds a lightweight [Fluxbox](http://fluxbox.org/) based desktop to the container that can be accessed using a VNC viewer or the web.\n# UI-based commands executed from the built in VS code terminal will open on the desktop automatically.\nENV DBUS_SESSION_BUS_ADDRESS=\"autolaunch:\" DISPLAY=\":1\" LANG=\"en_US.UTF-8\" LANGUAGE=\"en_US.UTF-8\"\n# Go SDK\nENV GOROOT=/usr/local/go GOPATH=/go\nENV PATH=${GOPATH}/bin:${GOROOT}/bin:${PATH}\n# Python SDK\nARG PYTHON_PATH=\"/usr/local/python\"\nENV PIPX_HOME=${PIXY_HOME} PIPX_BIN_DIR=${PIPX_BIN_DIR}\nENV PATH=${PYTHON_PATH}/bin:${PATH}:${PIPX_BIN_DIR}\n\n# RUN #yes | unminimize 2&gt;&amp;1 \\\nRUN echo \"\u26a1 Beginning common-debian.sh\" &amp;&amp; bash /tmp/library-scripts/common-debian.sh \"${INSTALL_ZSH}\" \"${USERNAME}\" \"${USER_UID}\" \"${USER_GID}\" \"${UPGRADE_PACKAGES}\" &amp;&amp; echo \"\ud83c\udf89 finished common-debian.sh\" \\\n    # &amp;&amp; echo \"\u26a1 Beginning python-debian.sh\" &amp;&amp; bash /tmp/library-scripts/python-debian.sh \"${PYTHON_VERSION}\" \"${PYTHON_PATH}\" \"${PIPX_HOME}\" \"${USERNAME}\" \"${UPDATE_RC}\" \"${INSTALL_PYTHON_TOOLS}\"  &amp;&amp; echo \"\ud83c\udf89 finished python-debian.sh\" \\\n    &amp;&amp; echo \"\u26a1 Beginning azcli-debian.sh install\" &amp;&amp; bash /tmp/library-scripts/azcli-debian.sh &amp;&amp; echo \"\ud83c\udf89 PowerShell Installed: [$(pwsh --version)]\" \\\n    &amp;&amp; echo \"\u26a1 Beginning docker-in-docker-debian.sh\" &amp;&amp; bash /tmp/library-scripts/docker-in-docker-debian.sh &amp;&amp; echo \"\ud83c\udf89 docker-in-docker-debian.sh completed\" \\\n    &amp;&amp; echo \"\u26a1 Beginning go-debian.sh\" &amp;&amp; bash /tmp/library-scripts/go-debian.sh &amp;&amp; echo \"\ud83c\udf89 go-debian completed\" \\\n    # &amp;&amp; echo \"\u26a1 Beginning sshd-debian.sh\" &amp;&amp; bash /tmp/library-scripts/sshd-debian.sh &amp;&amp; echo \"\ud83c\udf89 sshd-debian completed\" \\\n    # &amp;&amp; echo \"\u26a1 Beginning desktop-lite-debian.sh\" &amp;&amp; bash /tmp/library-scripts/desktop-lite-debian.sh &amp;&amp; echo \"\ud83c\udf89 desktop-lite-debian completed\" \\\n    &amp;&amp; apt-get -yyq clean &amp;&amp; rm -rf /var/lib/apt/lists/* &amp;&amp; rm -rf /tmp/library-scripts &amp;&amp; echo \"\ud83e\uddf9 cleaned install helper scripts\"\n\n\n#################################\n# CONFIGURE USER LEVEL SETTINGS #\n#################################\nUSER $USERNAME\n\n\n########################################\n# Configure Environment Vars For Tools #\n########################################\nENV BREW_PREFIX=/home/linuxbrew/.linuxbrew\nENV PATH=${BREW_PREFIX}/sbin:${BREW_PREFIX}/bin:${PATH}\nENV PATH=${PYTHON_PATH}/bin:${PATH}:${PIPX_BIN_DIR}\n\n# Ensure default profile directory exists for preferences to be saved and loaded as desired\nRUN pwsh -nologo -c 'New-Item -Path ($Profile | Split-Path -Parent) -ItemType Directory'\n\n\n\n#################\n# BREW PACKAGES #\n#################\n# For enhanced git cli experience\n# MACOS: Not Linux RUN HOMEBREW_NO_AUTO_UPDATE=1 brew install bit-git\n\n# For enhanced cross platform prompt with git and powershell both supported\nRUN HOMEBREW_NO_AUTO_UPDATE=1 brew install starship\n\n# Dotfiles loader so easy to initialize personal preferences in container with chezmoi init command\nRUN HOMEBREW_NO_AUTO_UPDATE=1 brew install chezmoi\n\n# RUN HOMEBREW_NO_AUTO_UPDATE=1 brew install gitversion\n\n\n#####################\n# OTHER DEV TOOLING #\n#####################\n# bit: For enhanced git cli experience\nRUN GO111MODULE=on go get -v -u github.com/chriswalz/bit\n\nUSER root\nRUN sh -c \"$(curl --location https://taskfile.dev/install.sh)\" -- -d -b /usr/local/bin\nUSER $USERNAME\n\n########################\n# GO DEVELOPMENT TOOLS #\n########################\n# Installing proactively to speed up vscode and precommit usage of various tools\n  # use binary install not go get for golangci-lint https://golangci-lint.run/usage/install/#local-installation\nRUN echo \"installing go tools\" \\\n  &amp;&amp; curl -sfL https://raw.githubusercontent.com/securego/gosec/master/install.sh | sh -s -- -b $(go env GOPATH)/bin vX.Y.Z \\\n  &amp;&amp; go get -u \"golang.org/x/tools/cmd/goimports\" \\\n  &amp;&amp; go get -u \"github.com/sqs/goreturns\" \\\n  &amp;&amp; go get -u \"golang.org/x/lint/golint\" \\\n  &amp;&amp; curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.39.0 \\\n  &amp;&amp; GO111MODULE=on go get -v -u \"github.com/go-critic/go-critic/cmd/gocritic\" \\\n  &amp;&amp; go get -v -u \"github.com/uudashr/gopkgs/v2/cmd/gopkgs\" \\\n  &amp;&amp; go get -v -u \"github.com/ramya-rao-a/go-outline\" \\\n  &amp;&amp; go get -v -u \"github.com/cweill/gotests/gotests\" \\\n  &amp;&amp; go get -v -u \"github.com/fatih/gomodifytags\" \\\n  &amp;&amp; go get -v -u \"github.com/josharian/impl\" \\\n  &amp;&amp; go get -v -u \"github.com/haya14busa/goplay/cmd/goplay\" \\\n  &amp;&amp; go get -v -u \"github.com/go-delve/delve/cmd/dlv\" \\\n  &amp;&amp; go get -v -u \"github.com/go-delve/delve/cmd/dlv@master\" \\\n  &amp;&amp; go get -v -u \"honnef.co/go/tools/cmd/staticcheck\" \\\n  &amp;&amp; go get -v -u \"golang.org/x/tools/gopls@v0.6.11\" \\\n  &amp;&amp; go get -v -u github.com/git-chglog/git-chglog/cmd/git-chglog # Generate Changelogs automatically\n\n#####################\n# Precommit Install #\n#####################\nRUN echo \"Validating python3 command is recognized: $(python3 --version)\"\nRUN echo \"Validating python3 pip module command is recognized: $(python3 -m pip --version)\"\nRUN echo \"Validating pip command is recognized: $(pip --version)\"\nRUN python3 -m pip install pre-commit\n\n\n#############################################################\n# SUPPORT CACHING VSCODE EXTENSIONS FOR FASTER PROVISIONING #\n#############################################################\n# IMPORTANT: This requires updating devcontainer.json per directions to ensure mount arguments passed in\n# See: https://code.visualstudio.com/docs/remote/containers-advanced#_avoiding-extension-reinstalls-on-container-rebuild\nRUN mkdir -p /home/$USERNAME/.vscode-server/extensions \\\n    /home/$USERNAME/.vscode-server/extensions &amp;&amp; \\\n    chown -R $USERNAME \\\n        /home/$USERNAME/.vscode-server \\\n        /home/$USERNAME/.vscode-server\n\n# Set multiple labels at once, using line-continuation characters to break long lines\nLABEL vendor=misc \\\n      type=codespaces \\\n      is-beta=true \\\n      user=vscode \\\n      is-production=false \\\n      version=\"0.0.1-beta\" \\\n      release-date=\"2021-06-01\" \\\n      description=\"development image with tooling for vscode codespaces\" \\\n      maintainer=\"me\" \\\n      name=codespaces-general\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#other-dockerfile-fragments","title":"Other Dockerfile Fragments","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#mount-ssh-keys","title":"Mount SSH Keys","text":"<ul> <li>Use cached: when the host performs changes, the container is in read only mode.<sup>1</sup></li> <li>Use delegated: when docker container performs changes, host is in read only mode.</li> <li>Use default: When both container and host actively and continuously perform changes on data.</li> </ul> <p>\"mounts\": [   \"source=${localEnv:HOME}${localEnv:USERPROFILE}/.ssh,target=/home/codespace/.ssh/,type=bind,consistency=cached\"   ]</p> <p>cannot create /home/$USERNAME/.ssh/known_hosts: Permission denied</p> <p>If this fails, try this in your Dockerfile. Not sure this is required, but did help in one test case, so I'm pinning here.</p> <pre><code>RUN mkdir -p /home/$USERNAME/.ssh/ &amp;&amp; touch /home/$USERNAME/.ssh/known_hosts\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#dotnet","title":"dotnet","text":"<pre><code># Install DOTNET tooling for benefit of tools like gitversion\nRUN echo \"downloading microsoft prod packages source\" &amp;&amp; wget https://packages.microsoft.com/config/ubuntu/20.10/packages-microsoft-prod.deb -O packages-microsoft-prod.deb &amp;&amp; dpkg -i packages-microsoft-prod.deb\n# RUN sudo dpkg --purge packages-microsoft-prod &amp;&amp; sudo dpkg -i packages-microsoft-prod.deb &amp;&amp; sudo apt-get update\nRUN echo \"Installing dotnet sdk\" &amp;&amp; apt-get update \\\n  &amp;&amp; apt-get  -yyq install apt-transport-https --no-install-recommends \\\n  &amp;&amp; apt-get  -yyq update \\\n  &amp;&amp; apt-get  -yyq install dotnet-sdk-5.0 --no-install-recommends  \\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#gitversion","title":"GitVersion","text":"<pre><code>RUN echo \"installing gitversion for automatic semver versioning\" &amp;&amp; dotnet tool install --global GitVersion.Tool\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#mage","title":"Mage","text":"<p>Go based Make alternative.</p> <pre><code>RUN go version &amp;&amp; go install github.com/magefile/mage@latest \\\n    # mage-select provides a nice little mage task selector menu\n    &amp;&amp; go install github.com/iwittkau/mage-select@latest\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#security-tools-for-lefthook","title":"Security Tools for Lefthook","text":"<pre><code>RUN go install github.com/evilmartians/lefthook@latest \\\n&amp;&amp; go install github.com/owenrumney/squealer/cmd/squealer@latest \\\n&amp;&amp; go install -v github.com/zricethezav/gitleaks@latest\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#terminal-tools","title":"Terminal Tools","text":"<pre><code>RUN curl -sf https: //gobinaries.com/chriswalz/bit | sh &amp;&amp; echo \"installed bit-git\" &amp;&amp;\n    \\ curl -fsSL https://starship.rs/install.sh | sudo bash -s -- --force &amp;&amp; echo \"completed setup of starship.rs\"\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#powershell","title":"powershell","text":"<p>Configure pwsh profile</p> <pre><code># Ensure default profile directory exists for preferences to be saved and loaded as desired\nRUN pwsh -nologo -c 'New-Item -Path ($Profile | Split-Path -Parent) -ItemType Directory'\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#installation-snippets","title":"Installation Snippets","text":"","tags":["development","docker"]},{"location":"development/tooling/docker/#markdown-lint","title":"Markdown Lint","text":"<p>Use the docker image of markdownlint to quickly fix basic formatting issues that can cause occasional issues with various markdown renderers.</p> <pre><code>docker run -i --rm -v ${PWD}:/work tmknom/markdownlint --fix --config .markdownlint.yaml /work/\n</code></pre> <p>Use a config like this to tweak the rules to your desired settings</p> <pre><code># name this file .markdownlint.yaml in your root directory or put in settings directory and pass --config in the setup.\n\ncomment: my-markdown-linting-rules\ndefault: true\n# MD003:\n#   style:\nMD007:\n  indent: 4\nno-hard-tabs: true\nwhitespace: true\nno-bare-urls: true\nfenced-code-language: true\nno-inline-html: false\nMD004:\n  style: dash\nMD025:\nMD041: false\nMD013: false\nMD046: false\n  style: consistent\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#gitversion_1","title":"Gitversion","text":"<p>Generate semver versioning from commit history automatically, removing need to manually manage semver.</p> <p>This results in a version history built by the actual commits and merges.</p> <p>Override allowed using <code>git tag -a 0.1.0 -m\"initial commit\" &amp;&amp; git push --tags</code></p> Description Code Pull gitversion <code>docker pull gittools/gitversion:latest</code> Run gitversion (calculate current semver from git history) using config from <code>build/GitVersion.yml</code> <code>docker run --rm -v ${pwd}:/repo gittools/gitversion:latest /repo /config ./build/GitVersion.yml /showvariable FullSemVer /nofetch</code> Output Build Server Variables for use in Azure DevOps <code>docker run --rm -v ${pwd}:/repo gittools/gitversion:latest /repo /config ./build/GitVersion.yml /showvariable FullSemVer /nofetch /output buildserver</code> <p>You can use those output variables in your Azure Pipeline to update the build name to include Semver versioning, or pass it into a task to use for building any app.</p> <pre><code>name: build-$(GitVersion.FullSemVer).$(Build.Reason).$(SourceBranchName).$(Date:yyyyMMdd)-$(Rev:.r)\n\nsteps:\n- bash: |\n    echo \"This is your new version: $VERSION\"\n  displayName: PublishSomething\n  env:\n    VERSION: $(GitVersion.FullSemVer)\n- pwsh: |\n  Write-Host \"This is your new $ENV:VERSION\"\n  displayName: PublishingSomethingElse\n  env:\n    VERSION: $(GitVersion.FullSemVer)\n</code></pre> <p>I normally start with <code>Mainline</code> development mode, but if you want to customize this to drive behavior off of conventional commits or other branching and delivery methods, there's a wide range of customization at you can review at: GitVersion Incrementing</p> <pre><code>---\nmode: Mainline\nbranches: {}\nignore:\n  sha: []\nmerge-message-formats: {}\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#install-aws-cli-session-manager-plugin","title":"Install AWS CLI &amp; Session Manager Plugin","text":"<p>This is for arm64 based ubuntu containers.</p> <pre><code>RUN apt-get -yqq update --fix-missing &amp;&amp; apt-get -yqq install pv \\\n    &amp;&amp; mkdir -p ./tmpinstall &amp;&amp; curl --silent \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"./tmpinstall/awscliv2.zip\" \\\n    &amp;&amp; COUNT=`unzip -q -l \"./tmpinstall/awscliv2.zip\" | wc -l` \\\n    &amp;&amp; mkdir -p ./tmpinstall/aws \\\n    &amp;&amp; unzip \"./tmpinstall/awscliv2.zip\" -d \"./tmpinstall/\"  | pv -l -s $COUNT &gt;/dev/null \\\n    &amp;&amp; ./tmpinstall/aws/install --update | (pv --timer --name \"\ud83e\udd16 awscli\")  \\\n    &amp;&amp; rm -rf ./tmpinstall/ \\\n    &amp;&amp; apt-get clean -y &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/library-scripts\n\n\n\nRUN echo \"\u2705 installing session manager plugin\" &amp;&amp; mkdir -p ./tmpinstall \\\n    &amp;&amp; curl --silent \"https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb\" -o \"./tmpinstall/session-manager-plugin.deb\" \\\n    &amp;&amp; dpkg -i ./tmpinstall/session-manager-plugin.deb \\\n    &amp;&amp; rm -rf ./tmpinstall/\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#install-go","title":"Install Go","text":"<pre><code>RUN echo \"\u2699\ufe0f installing go\" &amp;&amp; mkdir -p ./tmpinstaller/ &amp;&amp; wget \"https://raw.githubusercontent.com/udhos/update-golang/master/update-golang.sh\" -O ./tmpinstaller/update-golang.sh \\\n    &amp;&amp; wget -q https://raw.githubusercontent.com/udhos/update-golang/master/update-golang.sh.sha256 -O ./tmpinstaller/hash.txt  \\\n    &amp;&amp; chmod +r ./tmpinstaller/hash.txt &amp;&amp; pushd ./tmpinstaller &amp;&amp; sha256sum --check hash.txt &amp;&amp; popd \\\n    &amp;&amp; chmod +x ./tmpinstaller/update-golang.sh &amp;&amp; sudo bash ./tmpinstaller/update-golang.sh &amp;&amp; rm -rf ./tmpinstaller &amp;&amp; echo \"\u2705 go installed\"\nENV PATH=$PATH:/go/bin # this might be required for codespaces\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#install-dotnet","title":"Install Dotnet","text":"<p>Had issues using the instructions from site, but found a working snippet from this github doc</p> <pre><code>ARG DOT_NET_CHANNEL=2.1\nUSER root\nRUN curl -sSL &lt;https://dot.net/v1/dotnet-install.sh&gt; | bash /dev/stdin -Channel ${DOT_NET_CHANNEL} -Runtime dotnet -InstallDir /usr/share/dotnet \\\n    &amp;&amp; ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet\nUSER ${USERNAME}\n</code></pre>","tags":["development","docker"]},{"location":"development/tooling/docker/#codespaces-remote-containers","title":"Codespaces &amp; Remote Containers","text":"<p>Using remote containers is great for isolation of unique work and a clean environment. I tend to use the Universal Image provided by Codespaces, which includes Go, .NET, PowerShell, Python, NodeJS, and more.</p>","tags":["development","docker"]},{"location":"development/tooling/docker/#troubleshooting-cloning-a-repo-in-volume","title":"Troubleshooting Cloning a Repo In Volume","text":"<p>Prompted for SSH Password When Cloning Repo Via SSH In Volume</p> <p>Fixed my issues with a combination of help from:</p> <ul> <li>Git SSH public key authentication failed with git on Azure DevOps - Stack Overflow helped a lot.</li> <li>Git \u00b7 macOS Setup Guide</li> </ul> <p>I updated my <code>~/.ssh/config</code> to the following, and was able to clone without being prompted for a password.</p> <pre><code>Host *\n    AddKeysToAgent yes\n    UseKeychain yes\n    IdentityFile ~/.ssh/id_rsa\n\nHost ssh.dev.azure.com\n    PubkeyAcceptedKeyTypes=ssh-rsa\n    IdentityFile ~/.ssh/id_rsa\n    IdentitiesOnly yes\n</code></pre> <ul> <li><code>ssh-add -K ~/.ssh/id_rsa</code> to finish up.</li> </ul>","tags":["development","docker"]},{"location":"development/tooling/docker/#docker-snippets","title":"Docker Snippets","text":"<p>Run a disposable devcontainer with default image to do some experimentation in an interactive prompt.</p> <pre><code>docker run --rm --init --privileged -it mcr.microsoft.com/vscode/devcontainers/universal:linux /bin/zsh\n</code></pre> <ol> <li> <p>How do I add :cached or :delegated into a docker-compose.yml volumes list? - Stack Overflow \u21a9</p> </li> <li> <p>Docker Buildx | Docker Documentation \u21a9</p> </li> </ol>","tags":["development","docker"]},{"location":"development/tooling/git/","title":"git","text":"<p>Note</p> <p>This is a mix of git, github, azure devops repos, and other workflow tips that help me work more quickly. Comments are welcome with any corrections or suggestions.</p>","tags":["development","git","tech"]},{"location":"development/tooling/git/#install-homebrew","title":"Install Homebrew","text":"<p>Works on Linux and macOS now \ud83d\udc4f.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n</code></pre> <p>Many commands expect powershell, which runs on macOS and Linux as well. Just run <code>brew install powershell</code> to grab it and most of this will work without any further changes unless specified.</p>","tags":["development","git","tech"]},{"location":"development/tooling/git/#tools-ive-relied-on","title":"Tools I've Relied On","text":"","tags":["development","git","tech"]},{"location":"development/tooling/git/#cli","title":"CLI","text":"<ul> <li>Git-Town</li> <li>Bit</li> </ul>","tags":["development","git","tech"]},{"location":"development/tooling/git/#vscode","title":"VSCode","text":"<ul> <li>Git Graph</li> <li>Git Lens</li> <li>vivaxy/vscode-conventional-commits: \ud83d\udcacConventional Commits for VSCode.</li> </ul>","tags":["development","git","tech"]},{"location":"development/tooling/git/#github","title":"GitHub","text":"<ul> <li>GitHub CLI</li> </ul>","tags":["development","git","tech"]},{"location":"development/tooling/git/#git-aliases","title":"Git Aliases","text":"<p>Everyone has there own brand of craziness with git aliases.</p> <p>Here's a few I've found helpful.</p>","tags":["development","git","tech"]},{"location":"development/tooling/git/#azure-devops-boards","title":"Azure DevOps Boards","text":"<p>This one will create an autocompleted ready to go pull request in azure repos using the last commit title and description. If you create your commit correctly for the last one, this will ensure no extra rework required to generate the title and body of the PR, as well as the cleaned up squash message on approval.</p> <p>Install the Azure CLI and the <code>devops</code> extension will be installed automatically upon using: <code>brew install az</code></p> <pre><code>az devops configure --defaults organization=https://dev.azure.com/MyOrganization/ project=my-project-name\naz devops configure --use-git-aliases\n</code></pre> <pre><code># Azure DevOps Repos\nnew-pr = !pwsh -noprofile -nologo -c '&amp;az repos pr create --title \\\"$(git log  -1 --pretty=format:\\\"%s\\\")\\\" --description \\\"$(git log -1  --pretty=format:\\\"%b\\\")\\\" --auto-complete true --delete-source-branch true --squash --merge-commit-message \\\"$(git log  -1 --pretty=format:\\\"%s\\\")\\\" --output table --open --detect'\n</code></pre>","tags":["development","git","tech"]},{"location":"development/tooling/git/#general-commit","title":"General Commit","text":"<p>You only live once...rebase and sync from origin, commit all your changes, and generate a commit message using PowerShell NameIt module.</p> <p>Install module via: <code>Install-Module Nameit -Scope CurrentUser</code></p> <p>Install gitversion via: <code>dotnet tool install --global GitVersion.Tool</code></p> <pre><code>yolo  = !pwsh -noprofile -nologo -c 'Import-Module Nameit &amp;&amp; git add . &amp;&amp; git commit -am\\\"[wip] $(dotnet-gitversion /showvariable FullSemVer) - $((NameIt\\\\Invoke-Generate '[adjective]-[noun]' -Culture EN).ToLower())\\\" --no-verify &amp;&amp; git town sync &amp;&amp; git log --oneline -1'\n</code></pre> <p>For quickly ammending the last commit on your own private branch, you can combine these two commands to overwrite your branch with the latest changes instead of versioning.</p> <pre><code>pushf = !git push --force-with-lease\nfixup = !git commit -a --amend --no-edit\n</code></pre>","tags":["development","git","tech"]},{"location":"development/tooling/git/#cleanup","title":"Cleanup","text":"Command Code remove file from git without deleting <code>git rm --cached ./filepath.txt</code> remove directory from git without deleting <code>git rm --cached -r ./mydirectory</code>","tags":["development","git","tech"]},{"location":"development/tooling/git/#remove-files-already-committed","title":"Remove files already committed","text":"<pre><code>git rm --cached $File\n</code></pre>","tags":["development","git","tech"]},{"location":"development/tooling/git/#renaming-branch","title":"Renaming Branch","text":"<p>If you want to align with GitHub recommendeding naming of changing <code>master</code> to <code>main</code>, then this command will help you fix the local branches to correctly point <code>master</code> to the remote <code>main</code> branch.</p> <pre><code>git branch -m master main\ngit fetch origin\ngit branch -u origin/main main\n</code></pre> <p>You can configure this as a VSCode snippet for quick access by including this:</p> <pre><code>    ,\"rename-master-to-main\": {\n        \"prefix\": \"rename-master-to-main\",\n        \"body\": [\n            \"git branch -m master main\",\n            \"git fetch origin\",\n            \"git branch -u origin/main main\"\n        ],\n        \"description\": \"rename-master-to-main\"\n    }\n</code></pre>","tags":["development","git","tech"]},{"location":"development/tooling/git/#working-with-changes","title":"Working With Changes","text":"<p>All the commits the branch has that the master doesn't. <sup>1</sup></p> <pre><code>git log master..$(git branch --show-current) --oneline\n</code></pre>","tags":["development","git","tech"]},{"location":"development/tooling/git/#cleanup-tags","title":"Cleanup Tags","text":"<ol> <li>Remove tags on remote first: <code>git push --no-verify --delete MyTagName</code></li> <li>Remove every local tag in your repo: <code>git tag -d $(git tag)</code></li> <li>Pull latest tags: <code>git fetch origin --prune --prune-tags</code></li> </ol>","tags":["development","git","tech"]},{"location":"development/tooling/git/#forks","title":"Forks","text":"<ul> <li>Add remote for fork, typically covered with <code>upstream</code> name: `git remote add upstream {repolink}.</li> <li>Reset a forked branch to match the remote upstream resource: <code>git reset --hard upstream/master</code></li> </ul>","tags":["development","git","tech"]},{"location":"development/tooling/git/#resources","title":"Resources","text":"Source Description GitFixUm <sup>2</sup> FlowChart Style Help <ol> <li> <p>git-how-to-find-first-commit-of-specific-branch \u21a9</p> </li> <li> <p>GitFixUm \u21a9</p> </li> </ol>","tags":["development","git","tech"]},{"location":"development/tooling/precommit/","title":"precommit","text":"<p>A cheatsheet for various pre-commit hooks and things that help with linting, formatting, code scans and more. These all help \"shift left\" the review to eliminate more issues in the development workflow, rather than providing feedback only once the CI system is involved.</p>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#the-frameworks","title":"The Frameworks","text":"<ul> <li>GitHub - evilmartians/lefthook: Fast and powerful Git hooks manager for any type of projects. is a newer project based in Go.</li> <li>pre-commit is python based, very mature and supported.</li> </ul>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#precommit","title":"Precommit","text":"","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#install-precommit","title":"Install Precommit","text":"<p>A bit more complicated, depending on the Docker image used and the python tooling installed. Assuming you have pip installed, then run <code>pip install pre-commit --user</code></p> <p>Here's some examples to get you started.</p>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#skipping-a-precommit-hook","title":"Skipping A Precommit Hook","text":"<p>The pre-commit tasks can be overridden on a case by case basis.</p> <p>The syntax for skipping is simple, just run the task with the name of the hook excluded like this:</p> <pre><code>  Don't commit to main.....................................................Passed\n  check json5..........................................(no files to check)Skipped\n  go-fmt...................................................................Passed\n  golangci-lint...........................................................Skipped\n  go-test-all..............................................................Failed\n  - hook id: gotest \ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48  # Use the hook id, not the text of the title\n  - duration: 8.9s\n  - exit code: 2\n</code></pre> <ul> <li>To skip the example above: <code>SKIP='gotest' git commit -am\"feat(fancy): my title\" -m\"- My Message Body\" &amp;&amp; git pull --rebase &amp;&amp; git push</code>.</li> <li>To skip multiple: <code>SKIP='gotest,go-fmt' git myaction</code>.</li> </ul>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#filtering-triggering-tricks","title":"Filtering &amp; Triggering Tricks","text":"<p>Let's say you have a document directory and want to trigger a report or doc generation if anything in that changes.</p> <p>You can do this pretty elegantly with pre-commit.</p> <p>For example, let's add a mage task to generate docs when something in the package directory for go is updated.</p> <pre><code>repos:\n  # for specific updates that should result in an update to matched directories or files.\n  - repo: local\n    hooks:\n      - id: docs:generate\n        name: docs:generate\n        entry: mage docs:generate\n        language: system\n        files: ^pkg/\n        types: [file, go]\n</code></pre> <p>The types is pretty useful to not just try and match on file names.</p> <p>Use <code>identify-cli</code> which is a python cli and package included when you install pre-commit.</p> <p>Run it against a directory or file and you'll get the outputs that pre-commit will accept.</p> <p>For example, against a markdown file: <code>identify-cli README.md</code> and you should get: <code>[\"file\", \"markdown\", \"non-executable\", \"text\"]</code>. Any of these (or all) can be put to filter when the hook runs.</p> <p>Against a Go file: <code>[\"file\", \"go\", \"non-executable\", \"text\"]</code>.</p> <p>LeftHook</p> <p>Using pre-commit framework heavily, and no longer relying on Lefthook.</p>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#lefthook","title":"Lefthook","text":"<p>A great tool, but requires more work and not as fully featured as pre-commit. In most cases, I'd recommend pre-commit tooling over Lefthook.</p> <p>If you are just starting out this requires more hands on, but can result in faster checks and commits.</p> <p>My advice would be to maybe start with pre-commit if you want plug and play, and lefthook if you want to control the pre-commits explicitly and optimize for performance.</p> <p>As long as you have the Go SDK installed, just run <code>go install github.com/evilmartians/lefthook@master</code>.</p> <p>This framework is a little \"rougher\" and less supported than pre-commit framework, but for simple self-maintained hooks I've preferred this as it much faster, and so I end up using it more.</p> <p>Other installation methods are located at the installation guide <sup>1</sup></p>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#lefhook-tips","title":"Lefhook Tips","text":"<ul> <li>Originally I broke out lefthook into multiple files, so I could drop them into a directory, but now I stick with one.   Since it still requires editing the main file to extend and point to another file, I've found a single file simplier to maintain.</li> <li>Disable parallel for anything formatting files or possible not thread safe.   While parallel seems great, most of the pre-commit tasks should run quickly, and formatting and linting files at the same time could lead into conflicts or problems.   Use parallel for seperate language test runs perhaps, like running Python tests and Go tests since those shouldn't conflict.</li> <li><code>piped: true</code> is useful but hides the underlying tasks in the summary, so I suggest avoid unless you have tasks that really should step by step feed into each other.   In this case, maybe you should have this just be part of your task run, such as <code>mage lint fmt</code> rather than 2 seperate pre-commit hooks.</li> </ul>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#using-lefthook","title":"Using Lefthook","text":"<p>Here's some updated configurations I've started using.</p>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#output","title":"Output","text":"<p>Reduce the noise:</p> <pre><code>skip_output:\n  - meta\n  - success\n# - summary\nskip:\n  - merge\n  - rebase\n</code></pre>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#pre-commit-checks","title":"Pre-commit Checks","text":"<p>These are basic quick checks for markdown (docs as code). This illustrates one of the challenges in pre-commit framework tooling.</p> <p>Ideally, you want the pre-commit checks to only touch the files that changed to make things quick, but this requires some work-arounds, since not all tools support a comma delimited list of files passed in.</p> <p>One big improvement to lefthook, would be supporting <code>for_each</code> operator, so that cross-platform looping on matched files could be run, instead of having to parse inside the script here. I'm pretty sure that this would be more compatible with various platforms as well, since this I believe uses your native shell, so you'd have to be in WSL2 in Windows, for example, for the bash-like syntax to work.</p> <p>See ci-configuration-files for markdown lint config examples.</p> <p>Install <code>gojq</code> or replace with <code>jq</code> if you have it.</p> <pre><code>pre-commit:\n  tags: markdown fmt\n  parallel: false\n  commands:\n    markdownlintfix:\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*{.md}'\n      run: |\n        echo \"\u26a1 markdownlint on: {files}\"\n        for file in {files}\n        do\n          echo \"\ud83d\udd28 markdownlint: $file\"\n          docker run --rm -v ${PWD}:/workdir --entrypoint=\"markdownlint-cli2-fix\" davidanson/markdownlint-cli2:latest \"$file\"\n        done\n    markdownlintcheck:\n      files: git diff-index --name-only HEAD\n      exclude: '_licenses/*'\n      # exclude: '_licenses/*'\n      # files: git diff-index --name-only HEAD #git ls-files **/*.md -m #git diff-index --name-only HEAD #git ls-files **/*/*.md  -m\n      glob: '*{.md}'\n      run: |\n        echo \"\u26a1 markdownlint on: {files}\"\n        for file in {files}\n        do\n          echo \"\ud83d\udd28 markdownlint: $file\"\n          docker run --rm -v ${PWD}:/workdir --entrypoint=\"markdownlint-cli2\" davidanson/markdownlint-cli2:latest \"$file\"\n        done\n    # REQUIREMENTS: go install github.com/itchyny/gojq/cmd/gojq@latest # cross platform alternative to jq\n    shellcheck:\n      tags: gotool gojq\n      name: shellcheck\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*.sh'\n      run: docker run --rm -v ${PWD}:/mnt koalaman/shellcheck:stable --format=json {files}  | gojq\n    # REQUIREMENTS: npm install --global prettier\n    yamlfmt:\n      files: git diff-index --name-only HEAD\n      glob: '*.yaml|*.yml'\n      exclude: '.licenses/*'\n      skip_empty: false\n      run: prettier --loglevel warn --no-error-on-unmatched-pattern --write \"{.yaml,.yml}\"\n    # REQUIREMENTS: go install go.atrox.dev/sync-dotenv@latest\n    # used to sync default dotenv files to an example file to avoid commits on main .env\n    envfile:\n      name: update env.example file\n      files: '*.env'\n      exclude: '.licenses/*'\n      run: |\n        cd env\n        touch .env\n        sync-dotenv\n    # REQUIREMENTS: Mage Tasks Built (See github.com/sheldonhull/magetools)\n    # CI=1 helps reduce formatting output to minimal\n    # MAGEFILE_HASHFAST improves speed of calling mage by assuming your tasks haven't changed\n    go:\n      piped: true\n      tags: go lint fmt\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*.{go,mod,sum}' #'*.go|*.mod|*.sum'\n      commands:\n        fmt:\n          run: CI=1 MAGEFILE_HASHFAST=1 mage fmt\n        lint:\n          run: CI=1 MAGEFILE_HASHFAST=1 mage lint\n</code></pre>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/precommit/#pre-push-checks","title":"Pre-Push Checks","text":"<p>Most of these Mage oriented tasks from my magetools repo.</p> <p>Note that while they filter based on the files being Go related, they run against the entire repo.</p> <pre><code>pre-push:\n  parallel: false\n  commands:\n    fmt:\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*.{go,mod,sum}'\n      # run: CI=1 MAGEFILE_HASHFAST=1 mage fmt      # alias for task that can contain formatting for all fmt tasks if you wish\n      # run: CI=1 MAGEFILE_HASHFAST=1 mage go:fmt   # gofumpt based formatting\n      run: CI=1 MAGEFILE_HASHFAST=1 mage go:wrap  # golines based formatting\n    lint:\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*.{go,mod,sum}'\n      run: CI=1 MAGEFILE_HASHFAST=1 mage lint\n    test:\n      files: git diff-index --name-only HEAD\n      exclude: '.licenses/*'\n      glob: '*.{go,mod,sum}'\n      run: CI=1 MAGEFILE_HASHFAST=1 GOTEST_FLAGS='-tags integration' mage go:test\n    gitleaks:\n      tags: security gotool linux macos nowindows\n      run: CI=1 MAGEFILE_HASHFAST=1 mage secrets:check\n</code></pre> <ol> <li> <p>lefthook/full_guide.md at master \u00b7 evilmartians/lefthook \u00b7 GitHub \u21a9</p> </li> </ol>","tags":["development","devops","linting","automation"]},{"location":"development/tooling/task/","title":"task","text":"","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#prerequisites","title":"Prerequisites","text":"<p>Install go-task</p> <p>Install brew (if not already installed)</p> <p>Install Pre-commit</p> <p>Gotchas</p> <p>OS Specific Limitations exist for Windows. For instance, without wrapping with a <code>pwsh -c</code> command you might get a failure with <code>mkdir</code>.</p> <p>See prior issue Command can not be run in Windows \u00b7 Issue #319 \u00b7 go-task/task \u00b7 GitHub for more information.</p>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#common-config-setup","title":"Common Config Setup","text":"<pre><code>version: '3'\nsilent: true\noutput: prefixed\ndotenv: [.env]\n\nincludes:\n  compose: ./Taskfile.compose.yml\n  precommit: ./Taskfile.precommit.yml\nenv:\n  AWS_SDK_LOAD_CONFIG: 1\n  AWS_REGION: us-east-1\n  DOCKER_BUILDKIT: 1\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#common-variable-setup","title":"Common Variable Setup","text":"<p>The following are console escape sequences to provide some nice formatting for the output</p> <pre><code>vars:\n  black: \\033[:0;30m\n  red: \\033[:0;31m\n  green: \\033[:0;32m\n  orange: \\033[:0;33m\n  blue: \\033[:0;34m\n  purple: \\033[:0;35m\n  cyan: \\033[:0;36m\n  light_gray: \\033[:0;37m\n  dark_gray: \\033[:1;30m\n  light_red: \\033[:1;31m\n  light_green: \\033[:1;32m\n  yellow: \\033[:1;33m\n  light_blue: \\033[:1;34m\n  light_purple: \\033[:1;35m\n  light_cyan: \\033[:1;36m\n  white: \\033[:1;37m\n  nocolor: \\u001b[0m\n  reversed: \\u001b[7m\n  ARTIFACT_DIRECTORY: ./artifacts/\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#common-base-config","title":"Common Base Config","text":"<p>This is stuff I'd normally paste to get me started on a task file.</p> <pre><code>tasks:\n  default:\n    cmds:\n      - task: list\n  list:\n    desc: list tasks\n    cmds:\n      - task --list\n  vars:\n    desc: variable output\n    cmds:\n      - |\n        echo -e \"{{.light_gray}}=== \u2139 {{ .reversed }} Variable Info from Task {{ .nocolor }} === \"\n        echo -e \"{{.light_gray}}ARTIFACT_DIRECTORY          {{.nocolor}}: {{ .orange}}{{ .ARTIFACT_DIRECTORY }} {{ .nocolor }}\"\n        echo -e \"{{.light_gray}}DOCKER_BUILDKIT             {{.nocolor}}: {{ .orange}}{{ .DOCKER_BUILDKIT }} {{ .nocolor }}\"\n        echo -e \"{{.light_gray}}AWS_SDK_LOAD_CONFIG         {{.nocolor}}: {{ .orange}}{{ .AWS_SDK_LOAD_CONFIG }} {{ .nocolor }}\"\n        echo -e \"{{.light_gray}}AWS_REGION                  {{.nocolor}}: {{ .orange}}{{ .AWS_REGION }} {{ .nocolor }}\"\n  test:\n    desc: run basic tests against compose projects\n    prefix: \ud83e\uddea\n    cmds:\n      - |\n        echo \"todo\"\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#console-logging","title":"console logging","text":"<pre><code>test -f nonexistentfile ||         echo -e \"{{.red}}file does not exist: [{{ .NONEXISTENTFILE }}]  {{.nocolor}}\"\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#pre-commit","title":"Pre-Commit","text":"<p>I use this framework to simplify my project linting and checks.</p> <p>You can load this as a seperate base file by creating it in the same root directory of your project with the name: <code>Taskfile.precommit.yml</code> and include it as the base config shows.</p> <pre><code>---\nversion: '3'\nsilent: true\noutput: prefixed\n\ntasks:\n  run:\n    desc: run pre-commit against all files manually\n    cmds:\n      - pre-commit run --all-files\n  autoupdate:\n    desc: update the precommit file with latest\n    cmds:\n      - pre-commit autoupdate\n  init:\n    desc: ensure precommit tooling is available\n    prefix: \u2699\ufe0f\n    cmds:\n      - |\n        {{if eq OS \"windows\"}}\n        pip install pre-commit || echo -e \"{{.light_cyan}} \ud83d\udd25 you need python installed to run this  {{.nocolor}}\"\n        {{else}}\n        echo \"setting up precommit. This requires brew (works on Linux &amp; macOS)\"\n        echo \"if fails install linux brew with following command\"\n        if brew --version &amp;&gt;/dev/null ; then\n          echo -e  \"{{.green}} \u2705 Command succeeded, validated homebrew installed {{.nocolor}}\"\n        else\n          echo -e \"{{.red}} \u2757 Command failed. Homebrew not detected {{.nocolor}}\"\n          echo -e \"{{.red}}\u2757 install homebrew on Linux or macOS (not root) using the following command and try again: {{.nocolor}}\"\n          echo -e \"{{.orange}} /bin/bash -c \"\"\\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\" {{.nocolor}}\"\n        fi\n        export HOMEBREW_NO_AUTO_UPDATE=1\n        package=pre-commit\n        brew list $package &amp;&gt;/dev/null || brew install $package\n        {{end}}\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#install-tool-from-github-release","title":"Install Tool From GitHub Release","text":"<p>Without using any external dependency tooling, here's a way to add a task that might need to grab a binary) using <code>jq</code> and <code>curl</code>.</p> <pre><code>init:ci:\n  desc: setup tooling for project and download dependencies\n  cmds:\n    - |\n      go mod tidy &amp;&amp; echo -e \"{{.green}} \u2705 go mod tidy completed{{.nocolor}}\"\n      go install github.com/goreleaser/goreleaser@latest\n      go get github.com/caarlos0/svu  # Semver versioning tool\n      mkdir {{ .TOOLS_DIRECTORY }}\n      {{if eq OS \"windows\"}}\n      DOWNLOAD_URL=`curl -sL https://api.github.com/repos/restechnica/semverbot/releases/latest | jq -r '.assets[].browser_download_url' | grep \"windows\"`\n      curl -qo tools/sbot -sL $DOWNLOAD_URL\n      {{end}}\n      {{if eq OS \"darwin\"}}\n      DOWNLOAD_URL=`curl -sL https://api.github.com/repos/restechnica/semverbot/releases/latest | jq -r '.assets[].browser_download_url' | grep \"darwin\"`\n      curl -qo tools/sbot -sL $DOWNLOAD_URL\n      chmod +rwx ./tools/sbot\n      {{end}}\n      {{if eq OS \"linux\"}}\n      DOWNLOAD_URL=`curl -sL https://api.github.com/repos/restechnica/semverbot/releases/latest | jq -r '.assets[].browser_download_url' | grep \"linux\"`\n      curl -qo tools/sbot -sL $DOWNLOAD_URL\n      chmod +rwx ./tools/sbot\n      {{end}}\n      echo -e \"{{.green}} \u2705 go semverbot downloaded to tools{{.nocolor}}\"\n</code></pre>","tags":["development","shell","task","devops"]},{"location":"development/tooling/task/#initialize-project-tooling","title":"Initialize Project Tooling","text":"<p>I think any project requiring non-standardized tooling should have this setup in a standard <code>init</code> style command that makes it easy to get up and running, assuming that the basic SDK tooling is installed of course. To solve SDK's and other lower level tooling, you'll want to use Docker with Codespaces or other methods to ensure tooling setup is standardized and easy (Ansible, Docker, etc.)</p> <pre><code>init:\n  desc: initialize all tooling for ci and developer work locally\n  cmds:\n    - task: init:dev\n    - task: init:ci\ninit:dev:\n  desc: initialize tools for a developer, but not required for CI\n  cmds:\n    - |\n      go install github.com/evilmartians/lefthook@master\n      lefthook install\ninit:ci:\n  desc: setup tooling for project and download dependencies\n  cmds:\n    - |\n      go mod tidy &amp;&amp; echo -e \"{{.green}} \u2705 go mod tidy completed{{.nocolor}}\"\n      go install github.com/goreleaser/goreleaser@latest\n</code></pre> <p>This would be how I'd setup a project.</p> <p>Notice the seperation of <code>ci</code> and <code>dev</code> tooling.</p> <p>This is important if you don't want to needlessly add duration to your CI checks.</p> <p>This will give flexibility to ensure tooling like Lefthook or others aren't installed by a CI build.</p>","tags":["development","shell","task","devops"]},{"location":"how-to-default-ssrs-date-parameters-to-the-first-and-last-day-of-the-the-previous-month/","title":"How to default SSRS date parameters to the first and last day of the the previous month","text":"<p>Populating default dates in SSRS can be helpful to save the user from having to constantly input the date range they normally would use. When a report is pulled for last month's information, defaulting the date fields for the user can help streamline their usage of the report, instead of them manually selecting with the date-picker control in SSRS. The formula's I used were:</p> <pre><code>Beginning of Current Month (EOM) DateSerial(Year(Date.Now), Month(Date.Now), 1)\nBeginning of Last Month (BOM) DateAdd(DateInterval.Month, -1, DateSerial(Year(Date.Now), Month(Date.Now), 1))\nEnd of Last Month (EOM) DateAdd(DateInterval.Minute, -1, DateSerial(Year(Date.Now), Month(Date.Now), 1))\n</code></pre> <p>To set the default date of the parameters:</p> <ol> <li>First open up the Report Data Window, and choose your date parameters.</li> </ol> <ol> <li>Navigate to Default values, and click the Fx button to edit the expression for the field.</li> <li>Paste the formula into the expression field and save.</li> </ol> <p>Result: Your default dates should now show last month's date range. You can apply your own rounding or date types if you wish, this provides the time as well, since I was working with smalldatetime, datetime, and datetime2 datatypes.</p>","tags":["sql-server"]},{"location":"dynamic-sql-and-a-char-crash/","title":"dynamic sql and a char crash","text":"<p>Dynamic SQL can be helpful, but a pain to debug. I spent hours today working on figuring out why my simple date comparison in dynamic SQL wasn't working. Found out that the remote database I was connecting to had a char date instead of a datetime. I found the comparison of CHARDATE &gt; VARCHARDATE failed to error out, but also failed to give a proper result set. Changing the look-up to ensure both dates were converted to date fixed the issue. During this debugging I was reviewing my dynamically created SQL statement.</p> <p>I learned that SSMS limits the amount of text it will return. In trying to view the single large UNION ALL statement, I was experiencing truncated results. I wanted to ensure the code being executed looked proper, but couldn't get past the truncation.</p> <p>Enter SSMSBoost to the rescue!</p> <p>SSMSboost is created by developers and very responsive to requests. I'll do a proper review soon. They offer a visualize data option that goes beyond the usage I employed. For my purpose, I clicked on the cell and selected visual cell as text, and opened the data in notepad++. This showed the full text without truncation. I was able to move on in my debugging then as I knew the dynamic sql statement was not actually truncated except to my SSMS output. Dynamic SQL is a great tool, but if I had been working with direct queries, the issue would have been much faster to resolve!</p>","tags":["sql-server"]},{"location":"native-ssms-a-second-class-citizen-no-longer/","title":"Native SSMS a second class citizen no longer...","text":"<p>Intellisense can be a boon to adding quick development. Quick hints on scope specific variables, syntax suggestions, function descriptions and more provide a valuable tool to productive coding.Coding in SQL Server Management Studio (SSMS) has greatly improved over the version releases, but it still lags behind the power of Visual Studio's intellisense, template insertions with \"fill in the blank\" functionality .</p> <p>Additionally, the lack of automatic indentation means that lining up sub-queries and levels of logic can be annoyingly time consuming... especially when not everyone on a team following the exact same coding standards. Legibility can easily suffer. Intellisense fails to properly update at times and reads pending statements as errors in syntax disabling the prompts from providing help. Automatic SQL formatting has been a long missed feature in SSMS. Trying to line up levels of nested queries can be very time consuming, and variances in the way people layout the query can effect readability.</p> <p>As a developer, I'm constantly looking for ways to improve my coding experience and streamline repetitive bits of coding. One such tool is SSMSBoost which provides many valuable shortcuts and features that SSMS omitted. The SSMSBoost team provides a free community edition to their fellow developers and responds to feedback promptly. I'd highly recommend checking them out.</p> <p>However, SSMS Intellisense improvement is by far the most powerful and productive coding tool that developers can look to improve. This tool enables quick hints to job the foggy mind missing that all important cup of espresso in the morning!</p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#cut-above-the-competition","title":"Cut above the competition","text":"<p>In my search for coding tools I've reviewed three main contenders in the arena: ApexSQL Complete, Red Gate SQL Prompt, and DbForge SQL Complete. For me, the best tool ended up being DbForge SQL Complete. The product fills in the gap on intellisense improvements. I've installed in SSMS 2012, which is an improved environment and developers can use while still working with SQL Server 2008R2.</p> <p>ApexSQL Complete was offered free to the community with some powerful formatting capabilities. However, I was never able to get the formatting to truly match the clean results of Red Gate SQL Prompt and DbForge SQL Complete. Additionally, ApexSQL Complete had some lag issues originally which seem to have been improved in recent releases, but still seemed to get in the way more than the other tools. Their object info prompts were rudimentary and not formatted well.</p> <p>Red Gate SQL Prompt is a fine product, and rates highly. However, the customization offered in the formatting profiles was very limited and seemed focused on simplicity rather than offering complete control. It also seemed to have some unpolished edges, such as typing INSERT INTO TABLE, would provide the template of columns and value upon accepting the intellisense prompt, but the insertion didn't follow the same formatting standards you had setup, and thus was harder to work with. It also was an expensive option compared to the others, with a Professional license running over $300+ for just a year of upgrades and support. DbForge SQL Complete offered the best solution for my needs. The amount of customization offered was incredible and the end result was a powerful formatting tool with amazing results, and a intellisense tool that truly improved my coding experience (enough that I wanted to share with others this tool!) This tool is also compatible with Visual Studio, so now those doing SQL work in Visual studio will have a great tool to help fill the deficiencies of VS for SQL development.</p> <p>Before reviewing the tool, I'd also mention that DBForge offers a user vote/forum with which I've had feedback on almost every single issue right away. Their communication seems top notch, in additional to providing special programs and discounts for their products. One bug I found in the formatting was addressed with a new update within 2 weeks. That's a responsive company!</p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#db-forge-sql-complete","title":"DB Forge SQL Complete","text":"<p>This review is performed on the full version. An express version is also offered to the community as a service. This tool has a portion of the full version's functionality to whet the eager developer's appetite. A free trial is offered on the full version... but beware you won't be able to be satisfied with native SSMS features after this!</p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#general-options-the-normally-boring-stuff-thats-not-quite-boring","title":"General Options - The Normally Boring Stuff That's Not Quite Boring","text":"<p>Improvements to the SSMS Coding environment are shown. A shortcut to refresh the cache of intellisense, formatting options, a command to execute the currently selected statement, outline the structure of the document, snippet management, and feedback. I'll review only the portions that seem to have the greatest impact, as most of it is self explanatory.</p> <p></p> <p>Of particular note is the \"automatically trigger after\" option. For users that want intellisense to help out only after a long pause and not constantly change while typing, this option is helpful. Setting a longer time on the ms will keep the box from showing up unless a command is pressed to initiate the intellisense showing up (Default: Control-Space).</p> <p>The highlight occurrences option is a great feature added by Devart. When a particular object is selected in the query editor window, other occurrences are highlighted for easy viewing.</p> <p></p> <p></p> <p>Automatic alias generation is another boon to the developer. SQL Complete is will automatically generate an alias for you upon object selection from intellisense. This also means that column selections later on will be correctly assign the alias, reducing typing. select * from company c -- this alias was auto generated when selected</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#seamless-shortcuts-provide-ssms-satisfaction","title":"Seamless Shortcuts provide SSMS Satisfaction","text":"<p>Alias generation is powerful, but overshadowed by one of the simplest yet much needed features that SSMS Native Intellisense lacks: column selection and wild card expansion.</p> <p>Dragging the column names from object explorer results in a list of columns delimited by commas, but not stacked vertically, it also doesn't handle multiple table columns being dragged at the same time, or a \"limited selection of columns.\"</p> <p>SQL Complete fills this gap. Pressing tab allows expansion of columns in a stacked list, with alias's already assigned correctly. The column select appends to the same line, but with the automatic formatting tool, the finished list is easily stacked vertically with the press of the format shortcut. These are simple examples, but I've found to extremely helpful when working with complex queries involving numerous tables. Expanding all the columns available or selecting, with correct alias generation is a major improvement to the SSMS development environment</p> <p></p> <p></p> <p></p> <p>Insert statements are \"auto completed\" helping provide quick statement generation, this is a lifesaver!</p> <p></p> <p></p> <p>Prompts also help developers know the table structure of objects/views, stored procedure parameters, and function usage. Might save some folks MSDN googling just to remind themselves about the correct function usage!</p> <p></p> <p></p> <p>Compare this to the native function prompt:</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#snippet-templates-are-made-a-reality","title":"Snippet Templates are made a reality","text":"<p><code>&lt;enter mind blown event&gt;</code> Snippet tools are helpful for saving some typing, but until now they've been typically limited to just pasting in text. The full functioned snippet functionality of Visual Studio with field selection by navigating with tab hasn't existed in SSMS.... till now.</p> <p>Snippets are brought to us with SQL Complete with an amazing thought to detail. Not only are the snippets brought into our editor window, but are also displayed in intellisense, with\"fields\" provided to navigate to to \"fill in the blanks.\" Variables for customization are offered and the sky is the limit here!</p> <p></p> <p></p> <p>Below I demonstrate a cursor snippet. The only keystrokes I used to complete this action were:</p> <p>cu + Tab @MyCustom + Tab + custom_name_i_entered_in_one_place Total Keystrokes = 48 (10.9% Cost ) Total keystrokes without addin (and no mistakes) = 440 (89.1% cost) Would you keep an execution plan version that was 89% higher cost... 392 keystrokes later?</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#formatting-made-painless","title":"Formatting made painless","text":"<p>Formatting SQL code manually can be a pain. There are online tools offered with some limited functionality, but who really wants to use a web based formatting tool with production code? Addins for SSMS formatting are limited in number. Parsing the logical construct of a SQL statement isn't quite a simple as indenting a foreach loop in C#. SQL Complete offers one of the best SSMS SQL formatters I've come across.</p> <p>The level of customization is intense. A few screenshots are provided, but I will not go into all the levels of customization, instead I'll provide you with some MSDN code samples that SQL Complete beautified. Running the format can be done on a selection or an entire document. Settings can be saved so a team could easily share a formatting standard. This would prevent each person from having to configure all their settings manually, a major plus in setup for any team!</p> <p></p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#ugly-duckling-sql-turned-into-the-beautified-sql-youd-be-proud-of","title":"Ugly Duckling SQL turned into the beautified SQL you'd be proud of","text":"<p>Here are some examples of the formatting tool at work in order from a simple query to a complex recursive CTE. I believe this tool handles it beautifully!</p> <p>BEFORE</p> <p></p> <p>AFTER</p> <p></p> <p>BEFORE</p> <p></p> <p>After: *line breaks handled with union's</p> <p></p> <p>BEFORE: Complex CTE</p> <p></p> <p>AFTER</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#final-thoughts","title":"Final Thoughts","text":"<p>Additional features are available, such as automatically finding object in object explorer when selected in query window, outlining of document, contextually based join statements, and more. I covered only the features I considered the highest impact on my workflow. Finding a tool to enhance the development process in SSMS should be something SQL developer's consider.</p> <p>Once you utilize a tool like this, you'll find that the improvements help you focus on the more important things. Spending time thinking about \"lining\" up code is a waste of time when a developer could be focused on the content. Formatting all code to be lined up the same with a press of your shortcut keys also helps you not focus on trying to discern the formatting standards of others, and instead get down to the important work! The continual improvements and responsiveness of Devart to feedback and resolving issues I had makes me give their product a two thumbs up award!</p>","tags":["sql-server","cool-tools"]},{"location":"native-ssms-a-second-class-citizen-no-longer/#disclaimer","title":"Disclaimer","text":"<p>I do not work for Devart or any competing product. I reviewed this program since I've been sold on the value of it, and wanted to participate in their High Five program which helps developers that like their products share with others, and be eligible for discount/reward if a review of their product is completed. I believe the tool to be useful, and has improved my experience with SSMS. I hope other SQL developers can benefit, especially those who were unaware of the availability of such a great addin.</p>","tags":["sql-server","cool-tools"]},{"location":"snippet-designate-a-certain-time-of-the-day-in-getdate%28%29/","title":"snippet designate a certain time of the day in getdate()","text":"<p>Snippet to designate a certain time of the day to evaluate in the current day. If you need to limit a result to the current date after a particular time, strip the time out of the date, and concatenate the current time together with it, and then convert back to datetime2.</p> <pre><code>select convert(datetime2(0),cast(cast(getdate() as date) as varchar(10)) + ' 09:00 ')\n</code></pre>","tags":["sql-server"]},{"location":"installing-ssms-2012-all-by-it%27s-lonesome/","title":"Installing SSMS 2012 all by it's lonesome","text":"<p>SQL Server Management Studio (SSMS) is not offered as a standalone download on MSDN. Installation requires the user to download the sql server installation package and choose to install only this single feature. For developers, SQL Developer edition is a great choice.</p> <p>Here's some screenshots to give you a guide on installing SSMS by itself when working with the full installer.</p> <p>{{&lt; admonition type=\"info\" title=\"Updated: 2017-02\" &gt;}} This doesn't apply for future SSMS versions as they began (I believe with 2014) to package SSMS outside of the database engine installer, allowing continual iterations and improvements for SSMS outside of being included as part of SQL server patching (finally!). If you are still reading this.... get an updated SSMS! {{&lt; /admonition &gt;}}</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>","tags":["sql-server"]},{"location":"calculating-the-next-beginning-of-month-and-the-current-end-of-month/","title":"Calculating the next beginning of month and the current end of month","text":"<p>Handling dates is always a fun challenge in T-SQL! Finding the current end of month and next months beginning of month is straight forward, but I like to find new ways to do things that take less coding, and hate date conversions that require a lot of manipulation of characters and concatenation. This was what I came up with for avoiding character conversions and concatenation for finding the current BOM (beginning of month) and EOM (end of month) values. Adjust according to your needs. Cheers!</p> <pre><code>    --use datediff from 0, ie default 1900 date, to calculate current months as int\n    declare @ThisMonth int = datediff(month,0,cast(getdate() as date))\n\n    --add 1 to the current month to get the next month\n    declare @NextBom date = dateadd(month,@ThisMonth+1,0)\n\n    -- subtract a day from the beginning of next month to get the current end of month, without worrying about 28, 30, or 31 days.\n    declare @ThisEom date = dateadd(day,-1,@NextBom)\n    select @ThisMonth select @NextBom select @ThisEom\n</code></pre>","tags":["sql-server"]},{"location":"get-synonym-definitions-for-all-databases-in-server/","title":"Get synonym definitions for all databases in server","text":"<p>If you want to audit your enviroment to look at all your synonyms and see where they are pointing, you can use <code>exec sys.sp_MSforeachdb</code> to loop through databases, and even filter. It will save some coding. However, my research indicates it is probably a bad practice to rely on this undocumented function as it may have issues not forseen and fully tested.</p> <p>Additionally, support may drop for it in the future. I recreated what I needed with a cursor to obtain all the synonym definitions into a temp table and display  results.:</p> <pre><code>/*\n    create temp table for holding synonym definitions &amp; list of DB\n    */\n\n    if object_id('tempdb..#dblist') is not null\n        drop table #dblist;\n    select\n        *\n    into #dblist\n    from\n        sys.databases\n    where\n        name not in ('master', 'tempdb', 'model', 'msdb')\n        and State_desc = 'ONLINE'\n        and Is_In_Standby = 0\n    if object_id('tempdb..#temp') is not null\n        drop table #temp;\n\n    create table #temp\n        (\n            db_name               sysname\n            ,object_id             int\n            ,name                  sysname\n            ,base_object_name      sysname\n            ,server_name_hardcoded as case\n                when base_object_name like '%ThisDatabaseIsOkToHardCode%'\n                then 0\n                when len(base_object_name)\n                        - len(replace(base_object_name, '.', '')) &gt; 2\n                then 1\n                else 0\n            end\n        )\n\n    go\n\n    declare @DbName sysname\n    declare @XSQL varchar(max)\n    declare @CompleteSQL varchar(max)\n    declare db_cursor cursor fast_forward read_only local for select\n                name\n            from\n                #dblist\n    open db_cursor\n    fetch next from db_cursor into @DbName;\n\n    while @@fetch_status = 0\n    begin\n        set @XSQL = '\n                    insert into #temp\n                    ( db_name ,object_id ,name,base_object_name )\n                    select\n                        db_name()\n                        ,s.object_id\n                        ,s.name\n                        ,s.base_object_name\n                    from\n                        sys.synonyms s\n                    '\n        set @CompleteSQL = 'USE ' + @DbName\n                            + '; EXEC sp_executesql N'''\n                            + @XSQL + '''';\n        exec (@CompleteSQL)\n        fetch next from db_cursor into @DbName;\n    end\n\n    close db_cursor\n    deallocate db_cursor\n    go\n    select\n        *\n    from\n        #temp t\n</code></pre>","tags":["sql-server"]},{"location":"on-how-to-googlify-your-sql-statements-for-future-searching/","title":"On how to Googlify your SQL statements for future searching","text":"<p>For sake of future generations, let's begin to reduce typing and reuse code we've built. I think we can all agree that TSQL statements are often repeated. Ideally, snippets should be created to reduce repeated typing and let us focus on logic and content. However, some statements may not really be \"snippet worthy\", and just be quick adhoc queries. In the past, the solution for saving queries for reuse or reference in the future would be to just save in the projects folder manually. However, it is difficult to always make sure the file is saved, review previous version that may be want had overrode, or even review what statements you actually executed. SSMSToolsPack has historically offered a great logging option. However, as an individual it was hard to justify the cost out of my own pocket. SSMSBoost has provided a great solution! Note that this was recently added (April), and is a \"rough draft\" , with minimal interface options, yet provides an amazing solution that I've found to offer a great solution. In addition to the other great features that SSMSBoost offers (which I'll write about in the future), SSMSBoost now offers 3 unique solutions to saving work.</p> <ol> <li>Executed Statement Logging This feature saves all your executed statements (ie, when you hit execute) as a .sql file for future reference.  As of today, there is no GUI for managing this. Never fear, I have a great solution for you.</li> <li>Editor History Logging This feature saves the current contents of your open query windows at predefined intervals, by default set to 60 seconds. According to their documentation, if no changes have been made to file, it will not save a new version. It will only add a new version once changes are detected to a file.  3.Tab History Logging If you crash SSMS, close SSMS without saving tabs, or have some unsavory Windows behavior that requires SSMS to be restarted, don't fear... your tabs are saved. When restarting you can select restore tabs and begin work again. I've found this feature to be a lifesaver! </li> </ol>","tags":["sql-server","cool-tools"]},{"location":"on-how-to-googlify-your-sql-statements-for-future-searching/#searching-your-executed-and-editor-history","title":"Searching Your Executed and Editor History","text":"<p>Instructions I recommend for setup and searching your entire sql history nearly instantly.</p> <ol> <li>Install SSMSBoost (free community edition if you can't support with professional version)</li> <li>Install DocFetcher(open source full text search tool. Best I found for searching and previewing sql files without the complexity of using GREP or other similar tools)</li> <li>Download and run Preview Handler from WinHelp</li> <li>Run Preview Handler &gt; Find .SQL &gt; Preview as plain text</li> <li>Run SSMS &gt; Open Settings in SSMSBoost</li> <li>Configure settings as you see fit. I personally move my Editor History and Executed statement's location to my SSMS Folder, so that I can use something like Create Synchronicity to backup all my work daily. </li> <li>Restart SSMS for settings to take effect.</li> <li>Start DocFetcher, go to settings in the top right hand corner. </li> </ol> <p>Basic Settings I choose (If you aren't using Bitstreamfont... you are missing out) </p>","tags":["sql-server","cool-tools"]},{"location":"on-how-to-googlify-your-sql-statements-for-future-searching/#docfetcher-advance-settings-tweaks-change","title":"Docfetcher Advance settings tweaks Change","text":"<pre><code>CurvyTabs = true HtmlExtensions = html;htm;xhtml;shtml;shtm;php;asp;jsp;sql InitialSorting = -8\n</code></pre> <ul> <li>Why? Curvy tabs... because curves are nice</li> <li>HTML Extensions, obvious</li> <li> <p>Initial Sorting = -8 means that instead of sorting by \"match %\" which I didn't find helpful for me, to sort by modified date in desc order. This means I'll find the most most recent match for the text I'm searching for at the top of my list.</p> </li> <li> <p>Setup your custom indexes. I setup separate indexes for executed statements and editor history so I could filter down what I cared about and eliminate near duplicate matches for the most part. Right click in blank space to create index.</p> </li> </ul> <p>I setup as follows:  </p> <ol> <li>Now the DocFetcher daemon will run in the background, if you copied my settings, and update your indexes.  Searching requires no complex regex, and can be done easily with statements. I'd caution on putting exact phrases in quotes, as it does detect wildcards. </li> </ol>","tags":["sql-server","cool-tools"]},{"location":"view-computed-columns-in-database/","title":"View computed columns in database","text":"<p>Snippet to quickly view computed column information. You can also view this by doing a \"create table\" script. This however, was a little cleaner to read and view for me.</p> <pre><code>select\n    database_name = db_name()\n    ,object_schema_name = object_schema_name( object_id )\n    ,object_name = object_name( object_id )\n    ,full_object_name = object_schema_name( object_id ) + '.' + object_name( object_id )\n    ,column_name = name\n    ,cc.is_persisted\n    ,cc.Definition\nfrom\n    sys.computed_columns cc\norder\n    by full_object_name asc\n</code></pre>","tags":["sql-server"]},{"location":"a-moment-of-void-in-the-cranium-reveals-a-recursive-computed-column-with-an-esoteric-message/","title":"A moment of void in the cranium reveals a recursive computed column with an","text":"<p><code>Msg 402, Level 16, State 1, Line 67 The data types varchar and void type are incompatible in the add operator.</code></p> <p>I came across this error today when I accidentally used a computed column in a temp table, that referenced itself. This very unhelpful message was caused by referring to the computed column itself in the computed column definition, ie typo. Beware!</p>","tags":["sql-server"]},{"location":"tsql-snippet-for-viewing-basic-info-on-database-principals-and-their-permissions/","title":"TSQL Snippet for viewing basic info on database principals and their permissions","text":"<p>Quick snippet I put together for reviewing basic info on database users/principals, permissions, and members if the principal is a role.:</p> <pre><code>/*******************************************************\n    Some Basic Info on Database principals, permissions, explicit permissions, and if role, who is in this role currently\n    *******************************************************/\n\n    ;with roleMembers as (\n                            select\n                                drm.role_principal_id\n                            ,dp.principal_id\n                            ,dp.name\n                            from\n                                sys.database_role_members drm\n                                inner join sys.database_principals dp\n                                    on drm.member_principal_id = dp.principal_id\n                            )\n    select\n        db_name()\n        ,dp.name\n        ,stuff((\n                select distinct\n                    ', ' + p.permission_name\n                from\n                    sys.database_permissions p\n                where\n                    dp.principal_id = p.grantee_principal_id\n                    and p.major_id  = 0\n                    and p.state     = 'G'\n                for xml path(''), type\n                ).value('.', 'varchar(max)'), 1, 1, ''\n                ) as general_permissions\n        ,stuff((\n                select distinct\n                    ', ' + p.permission_name\n                from\n                    sys.database_permissions p\n                where\n                    dp.principal_id = p.grantee_principal_id\n                    and p.major_id  = 0\n                    and p.state     = 'D'\n                for xml path(''), type\n                ).value('.', 'varchar(max)'), 1, 1, ''\n                ) as deny_permissions\n        ,stuff((\n                select distinct\n                    ', ' + p.permission_name + ' on ' + object_schema_name(p.major_id) + '.' + object_name(p.major_id)\n                from\n                    sys.database_permissions p\n                where\n                    dp.principal_id = p.grantee_principal_id\n                    and p.major_id  &lt;&gt; 0\n                for xml path(''), type\n                ).value('.', 'varchar(max)'), 1, 1, ''\n                ) as specific_permissions\n        ,stuff((\n                select distinct\n                    ', ' + r.name\n                from\n                    roleMembers r\n                where\n                    r.role_principal_id = dp.principal_id\n                for xml path(''), type\n                ).value('.', 'varchar(max)'), 1, 1, ''\n                ) as current_active_members\n    from\n        sys.database_principals dp\n    order by\n        dp.name asc;\n</code></pre>","tags":["sql-server"]},{"location":"ssms-2012-extender-for-the-times-you-want-some-organization-to-the-random-pile/","title":"SSMS 2012 Extender for the times you want some organization to the random pile","text":"<p>When dealing with large amounts of objects in a database, navigation can be tedious with SSMS object explorer. This extender organizes the objects into groups based on schema, helping a developer easily navigate to the appropriate object. The current version didn't work for views, but the other objects were grouped effectively. Highly recommend!</p> <p>SSMS 2012 Extender </p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"check-constraints-can-help-enforce-the-all-or-nothing-approach-when-it-comes-to-column-updates/","title":"Check Constraints can help enforce the all or nothing approach when it comes","text":"<p>If you have a set of columns inside your table that you want to allow nulls in, however if one of the columns is updated force all columns in the set to be updated, use a check constraint. In my case, I had 3 columns for delete info, which were nullable. However, if one value was updated in there, I want all three of the delete columns to require updating. I created the script below to generate the creation and removal of these constraints on a list of tables:</p> <pre><code>/* CHECK CONSTRAINT TO ENSURE SET OF COLUMNS IS NULL OR IF UPDATED,\nTHAT ALL COLUMNS IN SET ARE UPDATED Columns: delete_date null delete_by_id null delete_comment null PASS CONDITION\n1: IF ALL COLUMNS NULL = PASS PASS CONDITION\n2: IF ALL COLUMNS ARE UPDATED/NOT NULL = PASS\n\nFAIL: IF 1,2 OF THE COLUMNS ARE UPDATED, BUT NOT ALL 3 THEN FAIL\n*/\n\n/* GENERATE CHECK CONSTRAINT ON ALL SELECTED TABLES TO REQUIRE ALL DELETE\nDATE COLUMNS TO BE UPDATED CORRECTLY */\n\nselect\n    t.TABLE_SCHEMA\n    ,t.TABLE_NAME\n    ,script_to_remove_if_exists = ' IF exists (select * from sys.objects where name =''check_' + t.TABLE_SCHEMA + '_' + t.TABLE_NAME + '_softdelete_requires_all_delete_columns_populated_20130718'') begin alter table ' + t.TABLE_SCHEMA + '.' + t.TABLE_NAME + ' drop constraint check_' + t.TABLE_SCHEMA + '_' + t.TABLE_NAME + '_softdelete_requires_all_delete_columns_populated_20130718 end '\n    ,script_to_run =              ' alter table ' + t.TABLE_SCHEMA + '.' + t.TABLE_NAME + ' add constraint check_' + t.TABLE_SCHEMA + '_' + t.TABLE_NAME + '_softdelete_requires_all_delete_columns_populated_20130718 check ( ( case when delete_date is not null then 1 else 0 end + case when delete_by_id is not null then 1 else 0 end + case when delete_comment is not null then 1 else 0 end ) in (0, 3) ) '\nfrom\n    INFORMATION_SCHEMA.TABLES t\nwhere\n    t.TABLE_NAME like 'mytablename%'\n    and exists (select\n            *\n        from\n            INFORMATION_SCHEMA.COLUMNS C\n        where\n            t.TABLE_CATALOG = C.TABLE_CATALOG\n            and t.TABLE_SCHEMA = C.TABLE_SCHEMA\n            and t.TABLE_NAME = C.TABLE_NAME\n            and C.COLUMN_NAME = 'delete_by_id')\n    and exists (select\n            *\n        from\n            INFORMATION_SCHEMA.COLUMNS C\n        where\n            t.TABLE_CATALOG = C.TABLE_CATALOG\n            and t.TABLE_SCHEMA = C.TABLE_SCHEMA\n            and t.TABLE_NAME = C.TABLE_NAME\n            and C.COLUMN_NAME = 'delete_comment')\n    and exists (select\n            *\n        from\n            INFORMATION_SCHEMA.COLUMNS C\n        where\n            t.TABLE_CATALOG = C.TABLE_CATALOG\n            and t.TABLE_SCHEMA = C.TABLE_SCHEMA\n            and t.TABLE_NAME = C.TABLE_NAME\n            and C.COLUMN_NAME = 'delete_date')\norder by\n    t.TABLE_SCHEMA asc\ngo\n</code></pre>","tags":["sql-server"]},{"location":"shortcut-to-reference-examples%2C-syntax%2C-and-definitions-straight-from-ssms/","title":"Shortcut to reference examples, syntax, and definitions straight from SSMS","text":"<p>I've never really used the F1 key for help files with most applications. I was surprised at the usefulness in SSMS I discovered today that uses scripting to actually get you MSDN articles relevant to your current selection in the query editor.</p> <p>If you have a keyword selected and want to view details, definition, and examples on it, you can highlight the phrase or select the word, press F1, and SSMS will pull up the appropriate MSDN article. The only issue I ran into was that it pulls up the most recent article, so if you aren't running SQL Server 2012, make sure you select the &lt; 2012 documentation to be sure it is accurate.</p> <p></p> <p></p>","tags":["sql-server"]},{"location":"renaming-all-references-inside-stored-procedures-and-functions-can-be-migraine-worthy-without-a-little-help/","title":"Renaming all references inside stored procedures and functions can be migraine worthy without a little help...","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2016-03-18\" &gt;}} Cleaned up formatting. This is older code limited to procs and functions. I'm sure there is a better way to do this now, but leaving here as it might help someone else in the meantime. {{&lt; /admonition &gt;}}</p> <p>If you run across migrating or copying a database structure for some purpose, yet need to change the database references or some other string value inside all the procedures and functions to point to the newly named object, you are in for a lot of work! I built this procedure to search all procedures and functions, and script the replacement across multiple databases, to streamline this type of conversion.</p> <p>I'll post up one for views and synonyms later, as my time was limited to post this. In my case, this script was built to replace DB1 with DB2, and I had to accomplish this across several databases at once.</p> <p>This script might help save you some time!</p>","tags":["sql-server"]},{"location":"scalar-functions-can-be-the-hidden-boogie-man/","title":"Scalar functions can be the hidden boogie man","text":"<p>Ran across a comment the other day that scalar functions prohibit parallelism for a query when included. I thought it would be worth taking a look, but didn't take it 100% seriously. Came across the same indication today when reviewing MVP deep dives, so I put it to the test.Turns out even a simple select with a dateadd in a scalar format was affected enough with that one action to drop 5% on the execution plan. When dealing with merge or other processes that would benefit from parallelism, this would become even more pronounced. Suggest reading \"Death by UDF\" section by Kevin Boles. This comment is buried at the very end of the chapter. He indicates</p> <p>\"One final parting gift: scalar UDFs also void the use of parallel query plans, which is why the FormatDate UDFpegged only ONE CPU core on my laptop! \" (Page 194-summary)</p>","tags":["sql-server"]},{"location":"eliminate-overlapping-dates/","title":"Eliminate Overlapping Dates","text":"<p>I was looking for an efficient way to eliminate overlapping days when provided with a historical table that provided events that could overlap. In my case, I had dates show the range of a process. However, the multiple start and end dates could overlap, and even run concurrently. To eliminate double counting the days the process truly was in play I needed a way to find eliminate the overlap, and eliminate duplicate days when running in parallel. I researched ways to complete this and found the solution through this post. Solutions to Packing Date and Time Intervals Puzzle </p> <p>Itzik provided an excellent solution, though I had to take time to digest. The only problem I ran into, was his solution was focused on a single user and dates. For my purposes, I need to evaluate an account and further break it down by overlap on a particular process. Grateful for SQL MVP's contributions to the community as this was a brain bender!</p> <p></p>","tags":["sql-server"]},{"location":"finding-groups-consecutive-months/","title":"Finding Groups - Consecutive Months","text":"<p>A step by step explanation on one way to get a consecutive period of months, which could easily be adapted to days, years, or other values. I'll continue on this track and post a tutorial on eliminating overlapping dates soon.</p>","tags":["sql-server"]},{"location":"snippet-alert-useful-dates-%28eom%2C-bom%2C-etc%29/","title":"Snippet Alert: Useful dates (eom, bom, etc)","text":"<p>Common date values you may need to reference that you may not want to write from scratch each time.</p> <p>Hope this helps someone else!</p>","tags":["sql-server"]},{"location":"generate-random-date-with-starting-point/","title":"Generate Random Date With Starting Point","text":"<p>If you want to create sample random samples when dealing with date calculations to test your results, you can easily create a start and end point of randomly created dates. This is a snippet I've saved for reuse:</p> <p><code>DATEADD(day, (ABS(CHECKSUM(NEWID())) % $Days Seed Value$), '$MinDate$')</code></p> <p>This should let you set the starting point (Min Date) and choose how far you want to go up from there as a the seed value. Ie, starting point 1/1/2014 with seed of 60 will create random dates up to 60 days outside the min date specified. Stackoverflow Original Discussion: How to update rows with a random date asked Apr 27 '09</p>","tags":["sql-server"]},{"location":"get-information-on-current-traces-running/","title":"Get Information on Current Traces Running","text":"<p>This is just a quick informational query to save as a snippet to get some quick information on running traces, and provide the stop and close snippet quickly if you want to stop a server side trace you created.</p>","tags":["sql-server"]},{"location":"or-pattern-causing-indexing-scans-in-parameter-based-queries/","title":"Tl;dr","text":"<p>(time constraints prevented me from reworking significantly)</p> <p>\"or-condition-performance\" article on SQL Server Pro was forwarded over to me to research by a friend who indicated that using a variable with an or pattern had historically caused table scans. This was a suprise to me as all previous queries with optional parameters I'd used in the past seemed to use index seeks. I had to dig into this a little deeper to see if had been missing this in my past work and needed to find an alternative method for optional parameters. Original test procedure copied from original.</p> <p>The result from running the test1 procedure was to find a clustered index scan. SQL Server optimizer should be able to utilize the or conditions as long as an index covers the predicates, so I dug in deeper. When I ran a random query against a few tables I found this was creating table scans. Looking a little deeper I decided to evaluate the indexing on the tables to see if it was an issues with indexing, and not the pattern of using the OR against a variable.</p> <p>Test 1: SCANS - except with option recompile Found index scans on all my versions of running this except when including option(recompile) inside the stored procedure statement text. This of course fixed the issue by allowing sql to build the plan based on the exact value passed in, however, this would be at the cost of increasing CPU and negating the benefits of having a cached plan ready. Test 2: Ran <code>exec sys.sp_updatestats</code></p> <pre><code>Updating [dbo].[or_test]     [PK__or_test__3213E83F7953A2B2], update is not necessary...     [idx_or_test_col1], update is not necessary...     [idx_or_test_col2], update is not necessary...     [ix_nc_CoveringIndex], update is not necessary...     0 index(es)/statistic(s) have been updated, 4 did not require update.\n</code></pre> <p>After researching for hours more, and reading many posts, I discovered I've been missing this in previous work, probably due to query plan caching. When utilizing the variable from the stored procedure, the parameters are \"sniffed\". This means the plan is not rebuilt for each execution, but instead first execution is cached with the values it utilized. Thereafter, the optimizer can reuse this plan. The difference is that if you provide a value you manually plug into your test such as \"declare @Value = 'FOO' \" then the optimizer has an actual value to use for each of your manually run executions. This means that if you have properly indexed the column, it would be sargable.</p> <p>However, the stored procedure is not passing in the actual value after the first run, it is trying to save the CPU demand the optimizer will need, and instead use the cached plan. This is likely the cause of my missing this in the past, as all my execution testing was based on commenting out the stored proc header and running manual tests. In this case, I'd correctly be seeing table seeks if indexed properly, because the optimizer was obtaining the actual values from my session. When executing the stored procedure externally, it looks to utilize parameter sniffing, which when working correctly is a good thing not a bug.</p> <p>However, when result sets can greatly vary, the problem of parameter sniffing can become a problem. In addition, if the OR statement is utilized as my original problem mentioned, the optimizer can decide that since parameter value is unknown at this time, that with an OR clause it would be better to run a table scan since all table results might be returned, rather than a seek.</p> <p>To bypass this, there are various approaches, but all are a compromise of some sort. The common approach to resolving if truly various selections may be made (in the case of SSRS reports for example) is to utilize option(recompile). This provide the actual value back to the compiler at run time, causing higher CPU usage, but also ensuring the usage of indexes and reducing scans when the columns are properly indexed. Again, this is one solution among several, which can include building dynamic query strings, logic blocks, and other methods.</p>","tags":["sql-server"]},{"location":"dev-tools-x-yplorer-%28review-1%29-catalog/","title":"Dev Tools: XYplorer (review 1) - Catalog","text":"<p>currently on version 14.60 I'm a big fan of finding tools that help automate and streamline things that should are routine actions.Surprisingly, I've found it incredibly challenging to move away from the default Windows Explorer for file management, as the familiarity it offers makes it somewhat tough to be patient with learning an alternative, especially if the alternative offers more complication. That's where I stood with XYPlorer for sometime. It is a developer's tool first and foremost. It is complicated, but with this complication comes an extremely robust set of features. I honestly think that I'll never really know all of them, as this is more than just a swiss knife for file management.</p> <p>This is almost like stepping from a text editor for editing code to a full blown visual studio IDE. There is just that that much to learn! Over time, I'm finding myself less frustrated by using it, and more amazed at the tweaks here and there that can be found that can greatly enhance one's file management and workflow, personal and professional. I won't cover all features, but I think instead of doing a full blown review on the product, I'm going to add some incremental reviews on features as I discover, otherwise the vast feature-set will end up causing nothing but writer's block and I'll never share anything (cause I'll be busy learning)</p>","tags":["cool-tools","ramblings"]},{"location":"dev-tools-x-yplorer-%28review-1%29-catalog/#catalog","title":"Catalog","text":"<ul> <li>Replaces Favorites with additional functionality The favorites section is one of my most used features in explorer. I setup the default locations I'm commonly navigating to, such as my SQL Query files location, cloud drives, temporary projects I'm working on, appdata folders I need access to occasionally, and more. XYPlorer Expands on this greatly by the concept of Catalogs. Instead of just having a shortcut, Catalogs allows one to expand the concept of shortcuts far beyond Windows Explorer (hereafter referred to as WE) and combines the favorites functionality with much more features. <p>The Tree is an image of your computer's file system. It shows you all what's there. But, most of the time all is just too much... The Catalog is the answer: here you can grow your own personal tree. Your favorite locations are deep down in some heavily nested structures? Lift them to the surface! Side by side with locations from the other end of your hard disk. You can navigate by the Catalog (finally a one-click favorite solution!) and you can drop onto the Catalog's items. XYPlorer Help The catalog houses many categories. Each of these categories can provide various functionality beyond just linking to favorites.</p> </li> </ul> <p></p> <p>Here you can see applications listed directly. They provide functionality to open the app, open a file you drag onto it with the app (bypassing need to use \"open with\" dialogue)</p> <p></p> <p>Opening the properties of a file allow one to futher edit the actions the application performs.</p> <p>  .... to be continued.</p> <p>Lots of functionality in the catalog to benefit from, but time is limited, I'm going to visit further in next post.</p> <p>note: was given a license by developer to help me evaluate long term. This did not affect my review, as it wasn't solicited at all by XYPlorer developer.</p>","tags":["cool-tools","ramblings"]},{"location":"case-of-the-mondays-causing-me-to-randomly-redefine-the-scope-of-global-temp-tables/","title":"Case of the Mondays... causing me to randomly redefine the Scope of Global","text":"<p>Today, I was reminded that global temp tables scope lasts for the session, and doesn't last beyond that. The difference is the scope of the global temp allows access by other users and sessions while it exists, and is not limited in scope to just the calling session. For some reason I can't remember, I had thought the global temp table lasted a bit longer. Remembering this solved the frustration of wondering why my adhoc comparison report was empty..... #mondayfail SQLMag article I referenced</p>","tags":["sql-server"]},{"location":"dev-tools-the-file-searcher-launcher-to-rule-them-all/","title":"Dev Tools: The File Searcher/Launcher to rule them all","text":"<p>Why does this not have more recognition? In the experimentation of various file management and launching apps, I've tried several (Launchy, Listary, etc), but none have offered the speed and customization of Find and Run Robot. This app is a life saver for the power user! Here is an example of how you can have a hotkey to immediately launch a customized google search. The group alias gives you extensibility to filter the text you are typing to identify this alias of \"Google Me\" as the result to use since we typed ? as the first part of the string (that is the anchor ^).</p> <p></p> <p>Note the encoding is handled by $$u1 for the websearch, automatically correctly encoding spaces, semicolons, and other characters.</p> <p></p> <p>Here's the final result of what you'd start typing.</p> <p></p> <p>The cool part about this is the ability to not only match the initial regex, but also to filter inside this pattern to provide lists of options inside our match. In this example, I wanted to list favorite website by typing \"G\" at the beginning of the string followed by the keyword to filter my websites. This can be accomplished by anchoring the beginning of the regex filter to ^g, then filtering with the $$1.</p> <p></p> <p>Here is the initial filtered match based only on \"G\"</p> <p></p> <p>And finally the magic happens when the letters after \"g\" are parsed to get the website I want. This allows one to launch favorite websites easily, and you could even customize the url or more based on what regex magic you work!</p> <p></p> <p>All of these concepts apply to launching favorite apps and more. FARR2 has more customization than apps like launchy, symenu, and more, as it allows one to easily tweak the search \"score\" and add bonus points to items matching common folders or file types such as exe, xlsx, and more. Score model is pretty amazing.</p> <p></p> <p>Example of customized options to boost certain valuable matches in search results.</p> <p></p> <p>Finally, the killer feature for those fans of Everything search tool (Void) is the integration of the Everything search engine as an option to quickly search your entire computer in millseconds. You can easily setup a search filter with a space at the beginning so that all you have to do is type space and your search phrase and it will switch over to using the plugin search engine.</p> <p></p> <p>Why does this tool not get more recognization! What a life saver as you search through sql files, projects, and docs! Hope this helped point you in the direction of an amazing tool... post a comment if you try it out and tell me what you think! Find and Run Robot Help Find and Run Robot Download</p>","tags":["sql-server"]},{"location":"dev-tools-farr2-launching-groups-of-files-or-apps-at-once/","title":"Dev Tools: FARR2 Launching groups of files or apps at once","text":"<p>There are probably a common number of apps you pull up when you pull up your system. For example, I pull up my Trello board, outlook, XYplorer, Sublime text 3, Sql server management studio, and ketarin (app updater). Found that you can easily setup a simply alias and launch a group of apps or files at anytime by simply typing the keyword.</p> <p></p> <p>This could easily launch a favorite group of files by adding to your startup or project list as you go. The right click on search results gives you this option on the fly.</p> <p></p>","tags":["cool-tools"]},{"location":"statistics-parsing/","title":"Statistics Parsing","text":"<p>Never really enjoyed reading through the statistics IO results, as it makes it hard to easily guage total impact when you have a long list of tables. A friend referred me to: http://www.statisticsparser.com/ This site is great! However, I really don't like manually copying and pasting the results each time. I threw together a quick autohotkey script that will detect your clipboard change event, look for \"scan count\" keyword, and then open a \"chrome app\", paste the results and submit. Note that I have the option \"window name enabled\" at the bottom of the textbox on the webpage. If you don't the tabcount navigation might be a little off, so tweak this if you want.</p>","tags":["sql-server"]},{"location":"restoring-a-database-that-doesn%27t-exist/","title":"Restoring a database that doesn't exist","text":"<p>When restoring a database that doesn't exist, say for instance when a client sends a database to you, you can't use the option to restore database, because there is no database matching to restore. To get around this you need to use the Restore Files and Filegroups option and then restore the database.   Another option I found interesting was the support for loading database hosted on a fileshare. Brentozar has an article on hosting databases on a NAS that I found interesting. I haven't tried it yet, but think it has a great usage case for dealing with various databases loaded from clients. If you haven't read any material by him... then my question is why are you reading mine? His whole team is da bombiggity.... stop reading my stuff and head on over there!</p>","tags":["sql-server"]},{"location":"sql-sentry-pro-explorer-is-worth-it/","title":"SQL Sentry Pro Explorer is worth it...","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2017-04-21\" &gt;}} Another great bit of news from reviewing this older post I wrote... SQL Sentry Pro is now a free tool thanks to the generosity of the Sentry One team! It's a must have. Go download it for sure. {{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"info\" title=\"Updated: 2015-04-28\" &gt;}} I created a few autohotkey scripts and solved the problem of collapsing panes and a few other annoyances. This has improved my experience a little. - Also noted one major improvement that would help with tuning is aggregating the total IO, and stats, rather than only each individual statement. I've found the need to compare two very different plans to see the total writes/reads variation and the impact on IO, but I've having to utilize another tool for statistics IO parsing to run totals, and then come back to the SQL Sentry Plan explorer for other details. The SQL Sentry plan explorer tool could be improved by enhancing with totals/sums to better compare various runs of plans. I can make do without it, but it makes me have to do a lot of workarounds for now. {{&lt; /admonition &gt;}}</p> <p>I'll post more later, but after a full day of query tuning on a difficult view, I'd definitely say the cost for PRO is worth it. I'm a fan of sql sentry (free), and decided recently to push for a license at work on this tool. Turns out it was well worth it. The ability to measure variance in plans with small changes without cluttering up SSMS without 20 versions was incredibly helpful and time saving. There are a few quirks that really bother me, but not enough to negate the benefits of this tool. Perks - Save a continual session on troubleshooting a query - Evaluate Logical IO easily in the same view - Save comments on each plan version run to identify the changes you made and what impact it had Negatives - Not integrated with SSMS or preferred text editor so the text editor extremely sparse on features. - No ability to easily sum logical IO and COMPARE to another plan, really you have to open two tabs and eyeball them. That is the biggest frustration, no easy comparison side by side without opening the same session and eyeballing. - NO KEYBOARD SHORTCUTS. GEEZ is that frustrating as you are trying to power through some changes, copy cells/io, and more. Overall: Love the product. Hope they enhance the producivity and efficient aspect more as that's the only area I'm seeing it's short in. Here are some screenshots from my work with it today. I additionally compared the final IO with http://statisticsparser.com/index.html </p>","tags":["sql-server","performance-tuning"]},{"location":"utilizing-the-power-of-table-parameters-to-reduce-io%2C-improve-performance%2C-decrease-pollution%2C-and-achieve-world-peace/","title":"Utilizing the power of table parameters to reduce IO, improve performance, decrease pollution, and achieve world peace...","text":"<p>I was dealing with a challenging dynamic sql procedure that allowed a .NET app to pass in a list of columns and a view name, and it would generate a select statement from this view. Due to requirements at the time, I needed the flexibility of the \"MAIN\" proc which generated a dynamic select statement, while overriding certain requested views by executing a stored proc instead of the dynamic sql.</p> <p>During this, I started looking into the string parsing being completed for a comma delimited list of numbers to lookup (the primary key). I figured I'd explore the benefits of the user defined table and pass through the list of ids from the .NET application with a table parameter instead of using comma delimited list. Some great material</p> <p>I came across indicated the overhead might be a little more client side, but that the benefits to cardinality estimation and providing SQL Server a table to work with can far outweigh the initial startup cost when dealing with lots of results to join against. The main area I wanted to address first, that I couldn't find any clear documentation on was the memory footprint. I saw mention on various sources that a TVP can have a lower memory footprint in SQL Server's execution due to the fact as intermediate storage it can be pointed at by reference, rather than creating a new copy each time, like when working with parsing into another variable using comma delimited lists.</p> <p>I get that passing the stored proc a table variable means it's working with provided object, but what about the portability of this object? In my case, there are at least 2 levels being worked. The MAIN proc and the CHILD proc. The child proc needs access to the same list of ids. The dynamic statement in the MAIN proc also needs the list of ids. Currently it was creating the list of ids by inserting into a table parameter the delimited list of values.</p> <p>Could I instead consider passing the actual table parameter around since it's by a readonly object and hopefully keep referring to it, instead of having separate copies being created each time. This could reduce the IO requirements and tempdb activity by having a single TVP being used by the MAIN and CHILD procs.   Summarized IO:  The footprint is reduced when dealing with IO from the child statement, because it keeps pointing to the same in memory object. I also validated this further by examining a more complex version of the same query that compares the comma delimited list against executing a nested stored procedure, which in turn has dynamic sql that needs the table parameter passed to it. The results of the review show successfully that it keeps pointing to the same temp object! </p> <p>In summary, the table valued parameter can end up being pretty powerful when dealing with passing a list of values that may need to be referenced by several actions or passed to nested procs (not that this is the best practice anyway). Disclaimer: this is working with the constraints of what I have to release soon, so not saying that nested procs with dynamic sql in both MAIN and CHILD are a great practice, but sometimes you gotta do what you gotta do!</p>","tags":["sql-server","deep-dive"]},{"location":"upgrade-from-sql-2014-evaluation-to-developer-edition/","title":"Upgrade from SQL 2014 Evaluation to Developer Edition","text":"<p>Couldn't find documentation showing that upgrade from SQL 2014 evaluation version was possible to developer edition. I just successfully converted an evaluation version to developer edition.</p> <ol> <li>Obtain the key for the developer edition (in my case I had to download the .ISO from MSDN downloads, and go through the installation wizard to get it)</li> <li>Run the installation center app for SQL Server 2014</li> <li>Select edition upgrade</li> <li>Plug in your new serial from the developer edition. Pretty simply, but thought I'd post a confirmation for anyone wanting confirmation the upgrade path was an option for developer. Probably somewhere in the MSDN documentation, but I couldn't find it with a quick search.</li> </ol>","tags":["sql-server"]},{"location":"enabling-instant-file-initialization/","title":"Enabling Instant File Initialization","text":"<p>Found a couple good walkthroughs on enabling instant file initialization. However, I'm becoming more familar with the nuances of various setups and found it confusing in trying to map the correct user/group to enable this option. In my case, I had the SQL Service running under NT SERVICE/MSSSQLSERVER and as such this logic wasn't showing up when trying to find groups/users to add to the necessary permissions. Lo and behold...</p> <p>I typed it in manually and it worked. If time permits I'll update the article later with a more technical explanation, but as of now, this is just a quick functional post to show what resolved the issue. Add the service account or group (whatever you have sql server in) to the perform volume maintenance privileges in the local security policy.</p> <p> </p>","tags":["sql-server"]},{"location":"some-simple-examples-of-querying-xml-with-sql/","title":"Some simple examples of querying xml with sql","text":"<p>XML is a beast if you've never tackled it. Here are some simple examples of what I discovered as I experimented and walked through obtaining values out of a XML column.</p>","tags":["sql-server"]},{"location":"what-was-i-thinking-deleting-myself-from-localdb/","title":"What was I thinking? Deleting myself from localdb?","text":"<p>Was testing a batch file to add a user to a localdb instance. Assumed that my user as admin on the machine wouldn't have an issue inserting myself back.... didn't think that one through too carefully. Executing any type of SQLCMD against it denied me. SSMS denied me. No SA had been setup on it, so I couldn't login as SA either. Looked for various solutions, and ended up uninstalling and reinstalling (localdb)v11.0 so that I'd stop having myself denied permissions.</p> <p>This however, didn't fix my issue. The solution that ended up working from me came from dba.stackstackexchange.</p> <p>I ended up deleting everything in the v11.0 Instances folder and then issuing the following command <code>sqllocaldb.exe c v11.0</code></p> <p>Resulting in message: <code>LocalDB instance \"v11.0\" created with version 11.0.3000.0.</code></p> <p>Success! This resulted in the instance being created successfully, and then I was able to login with SSMS. Apparently today was my day for learning some localdb permissions issues. What a blast..... Could have avoided this if I had simply used a test login, or had setup the SA with a proper password for logging in. <code>#sqlfail</code></p>","tags":["sql-server"]},{"location":"database-stuck-in-single-user-mode-due-to-botched-restore/","title":"Database Stuck in Single-User Mode Due to Botched Restore","text":"<p>Working in a development environment, I botched up a restore. After this restore attempt to overwrite my database with the previous version, I had it stuck in single-user mode.</p> <p>SSMS provided me with helpful messages such as this: <code>Changes to the state or options of database 'PoorDb' cannot be made at this time. The database is in single-user mode, and a user is currently connected to it.</code> Additionally, I was told I was the deadlock victim when attempting to set the user mode back to multi-user.</p> <p>Going forward I looked at several articles from Stack Overflow and various other blogs, and followed the recommended steps such as</p> <p>I even added a step to kill the connections to it by using this statement, helpfully posted by Matthew Haugen</p> <p>Finally went through and removed all my connections from master based on an additional post. No luck. Stopped my monitoring tools, no luck. At this point, it felt like a Monday for sure.</p> <p>Since I was working in a development environment, I went all gung ho and killed every session with my login name, as there seemed to be quite a few , except for the spid executing. Apparently, the blocking process was executing from master, probably the incomplete restore that didn't successfully rollback. I'll have to improve my transaction handling on this, as I just ran it straight with no error checks.</p>","tags":["sql-server"]},{"location":"database-stuck-in-single-user-mode-due-to-botched-restore/#victory","title":"VICTORY!","text":"<p>What a waste of time, but at least I know to watch out next time, ensure my actions are checked for error and rolled back. I'm going to just blame it on the darn SSMS GUI. Seems like a convenient scapegoat this time.</p> <p>Successful pushed out my changes with the following script:</p>","tags":["sql-server"]},{"location":"running-very-large-scripts-is-not-a-strong-area-for-ssms/","title":"Running very large scripts is not a strong area for SSMS","text":"","tags":["sql-server"]},{"location":"running-very-large-scripts-is-not-a-strong-area-for-ssms/#out-of-memory-argggh","title":"out of memory, argggh!","text":"<p>Am I the only one that has experienced the various out of memory issues with SSMS? Not according to google!</p>","tags":["sql-server"]},{"location":"running-very-large-scripts-is-not-a-strong-area-for-ssms/#lovingly-crafted-in-the-forges-of-well-dbforge","title":"lovingly crafted in the forges of.. well ... dbforge","text":"<p>I've a huge fan of Devarts products. I've done a review in the past on their SQL Complete addin, which is the single most used tool in my SQL arsenal. It vanquishes nasty unformatted code into a standard lined up format I can easily read. The 100's of options to customize the formatting make it the most customizable formatter I've found for SQL code. This SQL Complete however, is a plugin for SSMS. It is native in their alternative to Sql Server Management Studio, dbForge Studio. Highly recommend checking this out. It's affordable, especially if you compare against other products that offer less.... and they have a dark theme muaaah!</p>","tags":["sql-server"]},{"location":"running-very-large-scripts-is-not-a-strong-area-for-ssms/#execute-script-that-is-far-too-large","title":"execute script that is far too large","text":"<p>I'll post up more detail when time permits on some of the other features, but one noticeably cool feature is the \"execute large script\" option.</p> <p></p> <p> You can see the progress and the update in the output log, but the entire script isn't slowing down your GUI. In fact, you can just putter along and keep coding.</p> <p> Other options to accomplish the same thing include executing via SQLCMD, powershell, or breaking things up into smaller files. This just happened to be a pretty convenient option!</p>","tags":["sql-server"]},{"location":"running-very-large-scripts-is-not-a-strong-area-for-ssms/#have-i-switched","title":"Have I switched?","text":"<p>I haven't switched to using it as my primary development environment because of 2 reasons. Extensions... I do have quite a few that work in SSMS like SSMS Tools, SSMS Toolpack, and some Red Gate functionality. I lose that by switching over to dbForge Studio. Also, some of the keyboard shortcuts like delete line and others I'm so used to aren't in there. Regretably, they don't support importing a color scheme from visual studio, so you lose out on sites like https://studiostyl.es/ Other than a few minor quibbles like that I'm pretty happy with it. They've done a great job and the skinning of the app is great, giving you the option of dark or light themes.</p> <p>Devart apps provided to me for my evaluation, but are not biasing my recommendation.</p>","tags":["sql-server"]},{"location":"set-noexec-is-my-new-friend/","title":"SET NOEXEC is my new friend","text":"<p>Have never really played around with the option: <code>SET NOEXEC ON</code> Turns out this can be a helpful way to validate larger batch scripts before actually making changes, to ensure compilation happens. If you choose, you can verify syntax by \"parsing\" in SSMS. However, this doesn't compile. Compilation checks more than your syntax. It actually validates the objects referenced exist.</p> <p>The execution of statements in SQL Server has two phases: compilation and execution. This setting is useful for having SQL Server validate the syntax and object names in Transact-SQL code when executing. It is also useful for debugging statements that would generally be part of a larger batch of statements. MSDN #188394 I previously had done this basic verification by running an estimated execution plan. This had the benefit of finding compilation errors as well. However, with large batch jobs it could be problematic and slow, as it had to process and return all execution plans for the statements, which I didn't need. Having this function can be a nice resource for validation of scripts before running, without the overhead of estimated execution plans. Additionally, you can handle transactional rollbacks and prevent cascading problems from happening when running a batch that changes your database by setting <code>NOEXEC ON</code> when encountering an error. Red Gate SQL Compare does this elegantly: </p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/","title":"Qure Analyzer Benchmark Testing","text":"<p>{{&lt; admonition type=\"warning\" title=\"Updated 2018-03-18\" &gt;}} Wouldn't recommend tool at this time. Development seemed to cease, resulting in me not being able to use with later versions of SQL Server. When I came back recently to check on it, the app was sold and it was with a new company. Didn't see to have progressed much based on a quick look and not really interested in it at this point. Instead, other solutions like the new DEA (Database Experimentation Assistant from Microsoft) would be where I'd spend more effort. {{&lt; /admonition &gt;}}</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#the-problem-of-monster-views","title":"the problem of monster views","text":"<p>I've been working with tuning an application that performs a lot of Entity framework calls, as well as stored procedure activity. The stored procedures could be processing a large amount of data, or a small amount, depending on the configuration of the client. The major issue was the source of the data for the client application was a view with 20+ joins involved. In breaking down the logic, most of the joins really functioned as just adding additional columns of data. The logical grouping mean that mostly 2-3 tables at a time joined to provide a set of columns based almost exclusively on the source table. I needed to get away from this pattern, as I was finding tremendous issues with cardinality estimation for SQL Server, resulting in sometimes 20GB nested loop join worktables by the optimizer.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#simplify","title":"Simplify","text":"","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#the-incredible-masterful-well-actually-quiet-simple-solution-simplify","title":"the incredible masterful... well... actually quiet simple solution... simplify!","text":"<p>My proposed solution was to break up the large view into small pieces, which I identified by logically grouping the joins/related columns. Instead of 20+ joins to return the columns needed, I'd instead provide 10 separate selects of 2 tables or so in each query. These would be processed as a dataset on the client, instead of returning a single large datatable. The next issue was to identify the improvements based on evaluating the possibility of larger amounts of base accounts pulled at once with the new format, vs the smaller batch size the large view required to function at all.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#first-approach-was-statistics-io-sql-stress","title":"first approach was statistics io &amp; Sql stress","text":"<p>Using the SQL Stress tool along with evaluating the statistics io, time was the my first course of action. However, the problem I ran across was really that I needed to run this dozens of times in a row, and evaluate the impact on the client performance as well. SQL stress provided a great way to run the query manually, but with the input variables from the app, I really wanted to run the app as the end user experience, and request the batch job on the largest amount of rows I could get. This way, I truly was matching all the input from the app, and getting timings from this. In addition, I was looking for a good way to evaluate the workloads against each other.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#the-right-tool-for-the-analysis-qure-profiler","title":"the right tool for the analysis = Qure Profiler","text":"<p>I'd used ClearTrace before, and found it helpful, but hard to consume some comparison type reports easily. I needed something to help me identify the improvement or degradation of performance and Qure Analyzer solved this for me, turning something very messy into a much simpler proposition (which also helped with my end research presentation to the big kahonas). Dbsophic Qure Profiler has had some great reviews for assisting in database profiling, but I haven't had a chance until recently to fully dive into using it. Since I was doing batch testing, I figured now would be a great time to re-review it. The numerous variables at play made this pretty complex to really quantify any improvement based on a single run alone.  So, I brought back up Qure Profiler to help me out. Necessity is the mother of experimentation.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#initial-performance-evaluation-run-batch-level-analysis","title":"Initial Performance Evaluation Run (Batch level analysis)","text":"<p>Evaluated runs on the largest current grouping. This was 1232 base accounts. I would later evaluate with larger batch sizes, but started simple.</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#actual-execution-details-with-qure-profiler","title":"Actual Execution Details with Qure Profiler","text":"<p>Tested Again With QURE Profiler set to minimum batch info. I also ran DBCC free proc  cache to attempt to better ensure the plan was correctly rebuilt for the new counts, and that it was a fresh start in the comparison as far as impact on the disk IO.</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#comparing-100-to-larger-batch-sizes-in-the-base-account-request","title":"Comparing 100 to larger batch sizes in the base account request","text":"<p>This final comparison shows 42% improvement by using 500 rows at a time. This seemed to be a good compromise at this point to increase batch sizes, while still maintaining lower logical reads. Next step was to test against a larger database to evaluate scalability.</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#actual-execution-results-on-larger-database","title":"Actual Execution Results on Larger database","text":"<p>Evaluating against a database with about 500GB of data, I found the best execution time seemed to be the base account count (tongue twister) seems to be the 1000 batch size at this time. It is returning the results in the shortest duration and the lowest impact on reads. FREE PROC CACHE COMPLETED ON EACH STEP</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#qure-profiler-workload-comparison","title":"Qure Profiler workload comparison","text":"<p>Set the baseline as the 100 batch size (which is the current way of processing the request). Qure provided an extremely powerful side by side comparison of both of the workloads. The best value is still the 1000 batch size, showing that the logical reads at point.</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#comparing-to-the-original-methodology-of-a-huge-view-with-20-joins","title":"Comparing to the original methodology of a huge view with 20+ joins","text":"<p>Against two databases, both performed signficantly better with the simplified approach. One database was much faster, while another that was completely timing out with 20GB+ nested loop join worktables finally ran without incident. Major win! The changes look to have positively improved the performance overall for both databases representing two much different usage patterns (and data stored in it) When comparing new (1000) simplified procs vs 100 on original views it showed:  - 27% shorter per execution - 77% less cpu impact - 81% less logical read impact</p> <p></p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#additional-functionality-i-havent-even-touched-on","title":"Additional functionality I haven't even touched on","text":"<p>Qure Profiler offers additional functionality that I haven't even gotten a chance to touch on. It can normalize the count of events to compare an even 100 against another 100 even when the second trace might have run longer and caught the event more than the 100 times. Check out their page for more details.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#tldr-summary","title":"TL;DR Summary","text":"<p>Long post. If you read this far, I commend you for either being a great page scroller, or for being tenacious and finding all my graph's super interesting. If that's you, you get #SqlCred I'm finding that with workload testing, Sql server workload comparison is one of the harder things to do right. There are a lot of variables to take into consideration, and even when doing a simple test on batch size changes like I did, aggregating the comparison results into usable statistics and in a presentable format can be daunting. Highly recommend the dbsophic product for a great utility to help save a lot of time in doing this comparison.  This tool goes into my top SQL server performance tuning tools for sure. I'm going to be evaluating their more advanced Qure Optimizer soon as well, as it might help identify other issues I've missed on tuning by evaluating adjustments against a test copy of a database.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#are-there-any-cons","title":"Are there any cons?","text":"<p>I didn't run into any significant issues that impacted my ability to use it. I do think there were a few stability issues that I had to work around by restarting the app a few times, but for the time it saved, I wasn't complaining too much. 1. Need to offer option to use extended events. This is on their roadmap apparently. Xevents should help lower the impact from doing the profiling on the workload being tested. 2. Some app errors related to closing workload comparison tabs. I reported this bug and hopefully they'll continue to improve the application. 3. Maybe some GUI improvements to make it more in-line with modern UI standards? Just a personal preference. I didn't find the tab based comparison approach the most intuitive. Not terrible, but would be a nice improvement in the future.</p>","tags":["sql-server"]},{"location":"using-qure-profiler-to-benchmark-tuning-progress/#what-could-they-improve","title":"What could they improve?","text":"<ol> <li>Offer comparison report option that wouldn't require as many screenshots, and instead summarize the selected items in a pane that you could copy to image. This would be slick!</li> </ol>","tags":["sql-server"]},{"location":"red-gate-sql-source-control-v4-offers-schema-locks/","title":"Red Gate SQL Source Control v4 offers schema locks","text":"<p>Looks like the rapid release channel now has a great feature for locking database objects that you are working on. Having worked in a shared environment before, this could have been a major help. It's like the poor man's version of checking an object out in visual studio except on database objects! With multiple developers working in a shared environment, this might help reduce conflicting multiple changes on the same object.</p> <p>Note that this doesn't look to evaluate dependency chains, so there is always the risk of a dependent object being impacted. I think though that this has some promise, and is a great improvement for shared environment SQL development that uses source control.</p>","tags":["sql-server","redgate","cool-tools"]},{"location":"stranger-danger-the-need-for-trust-with-constraints/","title":"Stranger Danger... The need for trust with constraints","text":"<p>I ran into an issue with errors with an database upgrade running into a violation of a foreign key constraint. Don't know how it happened. Figured that while I'm at it, I'd go ahead and evaluate every single check constraint in the database to see if I could identify any other violations, because they shouldn't be happening.</p>","tags":["sql-server"]},{"location":"stranger-danger-the-need-for-trust-with-constraints/#improve-the-execution-plan-by-checking-the-data","title":"improve the execution plan by checking the data","text":"<p>In my reading, I found out that checking the constraints can enable the constraint to be marked as trusted. The trusted constraints are then able to be used to build a better query plan execution. I knew that constraints could help the execution, but didn't know that they could have a trusted or untrusted trait.</p>","tags":["sql-server"]},{"location":"stranger-danger-the-need-for-trust-with-constraints/#brentozar-to-the-rescue","title":"Brentozar to the rescue","text":"<p>I'm serious, this guy and his team are awesome. This one single team and their web resources have single handled helped me gain more understanding on SQL server than any other resource. I love how they give back to the community, and their communication always is full of humor and good examples. Kudos! Anyway, commendation aside, the explanation from sp_blitz was fantastic at summarizing the issue.</p> <p>After this change, you may see improved query performance for tables with trusted keys and constraints. - Blitz Result: Foreign Keys or Check Constraints Not Trusted   As the site further mentions, this can cause locks and performance issues, so this validation might be better done off hours. The benefit might be worth it though!</p>","tags":["sql-server"]},{"location":"stranger-danger-the-need-for-trust-with-constraints/#my-adaption-of-the-check-constraint-script","title":"my adaption of the check constraint script","text":"<p>I appreciate the script as a starting point (see link above). I adapted to run this individually on each check constraint and log the errors that occurred. This runs though all FK and CHECK constraints in the database you are in, and then checks the data behind the constraint to ensure it is noted as trusted.</p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/","title":"Monitoring SQL Server on a budget","text":"","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#cheap-aint-easy","title":"Cheap ain't easy","text":"<p>There's a lot of tools out there, and very few that are polished, have a good UI, and some reasonable functionality to help monitoring, that don't cost an arm and a leg. One such tool I've recently begun to appreciate is MiniDBA . I was generously provided with a license to evaluate this and continue testing, and have recently had an actual chance to start using it more in my environment. The cost for MiniDBA is one of the most affordable I've found for a live monitoring tool with a good UI design (eye candy is critical for monitoring a server as we all know  ) At the time of this post's original date, there is a simple free version for monitoring a single instance on the machine running. This free version is awesome if you have a VM running full-time, as you could have it stay running and monitor the instance you care about. Paying $50 for developer and $100 for Enterprise gives you more flexible management with alerts, multiple servers, and a service to collect the data instead of having to run the GUI app the whole time.</p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#wait-stats","title":"wait stats","text":"<p>Wait stats are the first place to typically go to when analyzing the delays a server may face. MiniDBA offers a few cool ways of looking at the data, including getting the diff on waits since the point in time you started looking at it, helping isolate the waits that really matter to you right now.</p> <p></p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#get-alerts-on-critical-server-issues","title":"get alerts on critical server issues","text":"<p>I'd love to see this more extensible/customizable, but it's a good start. The time \"resolved\" would also be great when reviewing the history to be able to see how long before an issue was resolved.</p> <p></p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#general-healthcheck-on-best-practices","title":"general healthcheck on \"best practices\"","text":"<p>Again, some really cool stuff in here. I'd love more customization opportunity to actually expand or customize these as I have a boatload of custom DMV's for evaluating best practice setup conditions on a SQL server. It would be great to extend this more.</p> <p></p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#active-connections","title":"active connections","text":"<p>Pretty straightforward, but one plus is it offers ability to view the execution plan for each SPID, potentionally helping save a few steps. Note the execution plans are not shown at the server level \"SQL tab\", but at the database level. This reminds me of a less thorough \"sp_whoIsActive\".</p> <p></p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#other-features","title":"other features","text":"<p>There are features to look at like: - table sizes - index sizes - files in the database - memory - default trace - last 3000 transaction log entries - locks on objects.</p>","tags":["sql-server"]},{"location":"monitoring-sql-server-on-a-budget/#visual-monitoring","title":"visual monitoring","text":"<p>The key of course for a great monitoring tool is not just a bunch of text data thrown at you, but a great visual representation of various facts so you can easily identify something wrong. I think the developer did a great job in providing a useful \"dashboard\". I think more customization or ability to look at a point in time more specifically would be great (like SqlSentry offers) but at the same time, the scope of the MiniDBA project seems to focus on simplicity, and not offering so much that it becomes complicated. I'd say for the price, the value is pretty good for a team looking for a simple tool with a few visual ways of looking at the performance, while still giving some active connection monitoring. Again, there's a lot of other options out there for monitoring, even the built in functionality. But for value, this is a pretty good option, as it seems to focus on simplicity, usability, and not being a $1000+ per server license.</p> <p></p>","tags":["sql-server"]},{"location":"brentozar%27s-training-chocolate-%26-cowboy-hats-included/","title":"Brentozar''s Training: Chocolate & Cowboy Hats Included","text":"<p>It was entertaining to listen to a technical wizard fighting the obsession with waiting to eat chocolate with 3k viewers watching. Kendra wore about 4-5 cowboy hats in an effort to help those of us who wear many hats feel welcome.... Now that's the kinda of training I enjoy! No pretense, just honest real, and insightful training with enjoyable humor included for free</p> <p>Highly recommend attending the webex presentations occurring today and tomorrow with Brent Ozar and his amazing team. They are giving back to the community with some excellent training and Q&amp;A. Attended the Shared Storage discussion yesterday and wasn't disappointed (I don't think they've ever disappointed with their training!) Register for the training now, as it is the second of the 3 days, with the first training occurring here @ 10am CST. Brentozar Event Registration</p>","tags":["sql-server"]},{"location":"multi-cursor-editing-sql-feels-like-the-movie-inception-just-became-real/","title":"Multi-Cursor Editing SQL feels like the movie Inception just became real","text":"<ul> <li>Yes... multicursor editing is epic</li> <li>No... SSMS doesn't support multi-cursor editing the way it should.</li> <li>Yes... you can do some basic editing with multiple lines using alt-shift</li> <li>No... it doesn't come close to what you can do with Sublime. Cool thing is you can open text in Sublime synced w/SSMS cursor position and switch between the two with a shortcut. That will be a post for another day, I'm just telling you now to get your appetite going. If you can't tell, I love shortcuts. Sublime + AHK pretty much covers most text editing needs you'll ever have. Feel free to send me a check for all the time you'll save from my revelation. I apologize in advance for the video quality. I plan on recording a better one in the future, after I wrap my head around the awesomeness of Camtasia Studio 8</li> </ul>","tags":["sql-server"]},{"location":"split-personality-text-eiting-in-ssms-with-sublime-text-3/","title":"Split personality text editing in SSMS with Sublime Text 3","text":"<p>My preview post showed a demonstration of the multi-cursor editing power of Sublime Text 3 when speeding up your coding with SQL server.There is a pretty straight forward way to setup sublime (or one of your preferred text editors) to open the same file you are editing in SQL Management Studio without much hassle. I find this helpful when the type of editing might benefit from some of the fantastic functionality in Sublime.</p>","tags":["sql-server","cool-tools"]},{"location":"split-personality-text-eiting-in-ssms-with-sublime-text-3/#external-tool-menu","title":"External Tool Menu","text":"<p>Go to <code>Tools &gt; External Tools</code></p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"split-personality-text-eiting-in-ssms-with-sublime-text-3/#setup-sublime-commands-to-open","title":"Setup Sublime Commands to Open","text":"Setting Value Title Edit in Sublime Command C:\\Program Files\\Sublime Text 3\\sublime_text.exe Arguments $(ItemPath):$(CurLine):$(CurCol) Initial Directory $(ItemDir) <p>Limitation: Unsaved temporary files from SSMS are empty when you navigate to them. If you save the SQL file you will be able to correctly switch to the file in Sublime and edit in Sublime and SSMS together.</p> <p>Important: One thing I personally experienced that wasn't consistent was the handling of unsaved files. If the file is SqlQuery as a temp file that hasn't been saved, then this opening didn't work for me. Once I had the file named/saved, it worked perfectly, even bringing the cursor position in Sublime to match what was currently in SSMS.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"split-personality-text-eiting-in-ssms-with-sublime-text-3/#refresh-file3","title":"Refresh File3","text":"<p><code>Tools &gt; Options &gt; Environment &gt; Documents</code> You can setup the auto-refresh to be in the background if you wish, or manually select the refresh from SSMS when it detects the change. If the auto-refresh happens while you are editing sometimes it caused me to have redo some work (or control-z) in Sublime, but for the most part it's pretty seamless.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"xml-attribute-vs-element-assignment-when-working-with-sql/","title":"XML Attribute VS Element Assignment when working with SQL","text":"","tags":["sql-server"]},{"location":"xml-attribute-vs-element-assignment-when-working-with-sql/#xml-handling-with-aliases","title":"XML handling with aliases","text":"<p>I find it interesting the difference in behavior with querying XML between column assignment, and quoted alias naming. It's a generally understood best practice to not use the deprecated syntax of column aliasing using a quoted name. For example:</p> <p><pre><code>select\n    [escapedWithBracketsIsGood] = case when raining then cats else dogs end\n    ,NoEscapeNeededPerGoodNamingPractices = case when rains then pours else friday end\n    ,case when writtenThisWay then ICringe else UseAssignmentWay end as NormalWayMostFolksDoIt\n    ,'BadNaming' = case when food then eat else hungry end\n    ,case when work then eat else JobLess end as 'VeryBadNaming'\n</code></pre> You can see the difference in naming. The bottom too are deprecated syntax from older naming standards, and should be avoided typically. Aliasing is pretty straight forward and the variance in doing your alias pattern doesn't normally have an actual impact on the results or way of handling... Except when doing with XML apparently</p> <p></p>","tags":["sql-server"]},{"location":"xml-attribute-vs-element-assignment-when-working-with-sql/#further-reading","title":"Further reading","text":"<p>MSDN covers in more detail and precision in the this page The handling of XML is a newer area to me, as previously I've avoided like the plague. However, in working w/apps sometimes creating XML configuration files might be useful so exploring this can helpful to understand how to manipulate and even obtain values from it.</p>","tags":["sql-server"]},{"location":"dynamic-sql-%26-quotename/","title":"Dynamic SQL & Quotename","text":"","tags":["sql-server"]},{"location":"dynamic-sql-%26-quotename/#not-quite-fineprint-but-sure-feels-like-it","title":"Not quite fineprint, but sure feels like it!","text":"<p>Quotename can be a pretty cool function to simplify your dynamic sql, as it can ease some of the escaping of strings. However, I normally use it for table/column names, and so hadn't ran into a \"gotcha\" of this function until today. It's limited to 128 characters, and if you pass in greater than 128 characters will yield a null. Yep... you could be trying to track down that error for a null string somewhere in your concatenation for a while... only to find out this silent error is occurring. I'd like to thank NoSqlSolution for mentioning this and helping me go back to the other window I had open and rereading it.... I guess sometimes it pays to read the darn BOL.</p> <p></p>","tags":["sql-server"]},{"location":"best-tools-for-taking-notes-in-development/","title":"Best Tools for Taking Notes In Development","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2016-04-08\" &gt;}} Been working through some issues with Clarify app on my personal computer, something I messed up. @trevordevore with @clarifyapp has worked step by step through it with me providing some great personal assistance. In addition, he's given me some tips about my research in merging in a php script to automatically upload my images to cloudinary for hosting. I just finished producing some documentation with Clarify 2, and I have to say it's one of the best tools I can think of for producing a stellar documentation without requiring the user to spend time doing much formatting. I think the points in this post still remain, I wish they'd continue to expand it a little more. Maybe an extension that replicates a library of screenshots like Snagit, or something else for quick ad hoc annotation and pasting would be welcome. Overall, these are minor concerns with the great results the app already produces. {{&lt; /admonition &gt;}}</p> <p>Developers have a lot to juggle.</p> <p>Do a few of these things sound familiar:</p> <ul> <li>Juggle current sprint tasks</li> <li>Resolve outstanding issues needing immediate attention</li> <li>Research/Professional Development</li> <li>Long term development objectives that you can only get fragments of time to work on</li> <li>Performance testing - and being able to remember numbers/figures with all the moving pieces</li> <li>With all the time left blog, write an insightful blog post that will be hailed by millions</li> </ul> <p>I've been on a long quest to evaluate the best tool to help me personally document and stay up to date with some of these tasks. Documenting and task management can be easily overlapped. For the purpose of my notes, I'm going to focus primarily on the documenting aspect. Whether you use Onenote, Trello, word, or any other system, the most important thing is an easy way to recap and evaluate all the work you've done</p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#a-few-goals-for-a-best-fit","title":"A few goals for a best fit","text":"<ol> <li>Screenshots are a key for me (see next section if you wonder why)</li> <li>Annotation of screenshot images are a must</li> <li>Easily able to copy and paste to reuse the material or export in a way that others could consume</li> <li>Basic authoring, nothing fancy needed, just basic bold/italics, etc. Something to help visually organize, as raw text can be hard to skim through if a lot of content.</li> </ol>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#why-do-care-so-much-about-screenshots","title":"Why do care so much about screenshots?","text":"<p>The problem with solely using text for content is the variety of content we run across.</p> <p>For instance, when I'm working on performance tuning a database, I will have text based details from SSMS, a grid of results that may be relevant. I may utilize a performance tuning tool like Qure Profiler, or find a diagnostic graph from perfmon that I want to preserve. The variety of formats is daunting to try and capture in a usable format into a documentation tool without considering screenshots. Since the other tools present information in a usable format, but not always exportable format I can use a screenshot to capture the data I reviewed without spending time trying to get things just right.</p> <p>I also find visually walking through a problem easier to remember when re-reviewing in the future. Your mileage may vary.</p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#tools-i-evaluated","title":"Tools I Evaluated","text":"<p>Disclaimer: Please note that most of these tools I received a free license for indicating I'd review. I DO NOT base recommendations on these. A piece of crud is a piece of crud regardless of it it is free. I normally like to review products for an extended period before writing about any of them</p> <p>My Final Review Rating after intense self examination, contemplation of my navel for hours on end, and the hope of lucrative contracts becoming a professional software reviewer....</p> <p>Epic: Highly Recommended</p> <ol> <li>Clarify 2</li> <li>Snagit</li> <li>Camtasia Studio</li> </ol> <p>Close: Good product</p> <ol> <li>Greenshot1. Great free screenshot utility. Best general screenshot tool besides Snagit I think I've found.</li> <li>OneNote</li> <li>Great for OCR scans of text, but basically useless for anything relating to annotations/commenting on an image.</li> </ol> <p>Benched: Not a good fit for me *not reflective of all users</p> <ol> <li> <p>Ashampoo Snap 8</p> <pre><code>1.  UI was ok, but found it lacking with keyboard shortcuts and power user usage. The annotations were very dated and seemed very cartoonish for the most part, so I wasn't very happy with the end result. I think the app is a good one, just needs some redesign on a few elements for making it cleaner and modernized. The canvas expansion was very unintuitive as well, making it unfriendly for merging several screenshots together.\n</code></pre> </li> <li> <p>Screenpresso</p> <pre><code>1.  organizes by folder/project, which was a cool concept. Overall very limited and nothing that really stood out as a paid product.\n</code></pre> </li> <li> <p>ShareX</p> <pre><code>1.  Very extensible. One of my favorite features was the _on-screen annotations,_ which let you capture an area on the screen, and do basic annotations on it without any apparent GUI. Nice! Overall I can't recommend due to the learning curve and complexity. It's extremely powerful, but that comes at the cost of being very complex to configure and get working. Documentation was ok, but still for some more advanced setting up of custom share destinations I found it difficult to find help.\n</code></pre> </li> <li> <p>ActivePresenter (free version)</p> <pre><code>1.  Great capturing of screenshots and mouse movements into training video based on detecting movement/actions. This was a great find, however, I can't recommend it for any but those with no work budget, as it's extremely clunky for usage relating to development/notes/personal workflow. It's very busy and hard to find settings to configure, but then again... free!\n</code></pre> </li> </ol>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#epic-highly-recommended","title":"Epic: Highly Recommended","text":"<p>All 3 of these products are commercial. All three have solid functionality, and I really find them to be beneficial to my development process. They each have a different scope in what they accomplish so I could see myself using each of them in various ways from this point forward.</p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#clarify-2","title":"Clarify 2","text":"<p>This was a gem I discovered and initially passed over. The site documentation focused a lot of documenting steps for tutorials/help, and I went ahead and tried to see what it would be like as it looked polished. So far, my favorite discovery in documenting/notes for development and blogging. This tool has the polish of a well design tool, while still having some powerful extensibility. I think it focuses very well on one thing which is documenting step by step work. This ties directly in with blogging and tech documentation. In a blog, I'm covering specific areas, and organizing my thoughts in blocks.</p> <p>On the tech side, when I'm evaluating a SQL performance problem or tuning effort, I systematically am going through comparisons of before and after, impact assessment, and identifying the improvements against baseline. This tool allows this workflow perfectly.</p> <p>The export options are pretty polished. I did a quick walk-through for a issue at work and got several complements about the polished documentation I produced. Best part is this took minimal effort.</p> <p>Pros</p> <ul> <li>Absolutely intuitive and clean</li> <li>Lack of over complication with unneeded features. Instead, it focuses on usability.</li> <li>The annotation style is my favorite, very modern and minimal, not like some that use more cartoonish annotation styles.</li> <li>Rounding of images on canvas as a default, looks polished.</li> <li>Easy manipulation of canvas, no complication in trying to add multiple images</li> <li>Fantastic export options. Polished PDF, copy to clipboard as rich text, _EXPORT TO WORDPRESS _this article was written with this tool entirely* Cons</li> <li>Keyboard shortcuts are lacking</li> <li>Could really use with an extension of functionality to just capture single screenshots and annotate like snagit. As it stands, the tool is focused more on the documentation aspect, but with some enhancements it would work as a screenshot annotation tool. Sort of like a single step in their documentation tool, without all the rest.</li> </ul> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#clarify-2-image-annotation-editing","title":"Clarify 2 Image Annotation Editing","text":"<p>This image shows some of the useful annotation powers that clarify offers. The canvas that images are placed on can easily be re-sized with a mouse drag and images pasted.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#snagit","title":"Snagit","text":"<p>I avoided this product for a while as I honestly thought it was so mainstream that it wouldn't really benefit a power user/developer. So far, I'm glad to say I'm wrong. It's a well designed product that I've come to start using as my primary screen capture utility, along with Clarify</p> <p>Pros</p> <ul> <li>Wealth of export functionality</li> <li>Good annotation capability with shapes, text, and so on.</li> <li>Can capture screen video for a quick tutorial or walkthrough, no extensive editing options on this however. This is the focus of Camtasia.</li> <li>Ability to tag screenshots (so bug screenshots could be easily re-reviewed)</li> </ul> <p>Cons</p> <ul> <li>Keyboard shortcuts are a little lacking with annotations</li> <li>Some of the effects/annotations seems a little cheesy to me, but there are plenty of ways to find a good fit that formats in the style I like.</li> <li>No rounded corner presets for the image canvas if you want that. There are workarounds, but nothing built in.</li> </ul> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#snagit-extensibility-with-output","title":"Snagit Extensibility with output","text":"<p>A wealth of export functionality is in Snagit, along with the additional plugins you can pull in.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#snagit-profile-extensibility-with-scripted-profiles","title":"Snagit Profile Extensibility with scripted profiles","text":"<p>Along with keyboard shortcuts and powerful export options, I found you could create capture profiles, letting you with setup a shortcut (keyboard or just menu driven) to capture and save as a special format, basically scripting several steps you'd manually repeat together.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#snagit-annotations","title":"Snagit Annotations","text":"<p>Annotations are pretty comprehensive with some functionality to splice out the middle of images (say a toolbar), merge multiple images, and more. Pretty well designed and along with Clarify 2 the best implementation from any utilities I've experimented with so far.</p> <p>I did like the ability to \"curve\" an arrow on the annotations, as it let you smoothly draw an annotation that could curve around content without obfuscating it.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#camtasia","title":"Camtasia","text":"<p>Camtasia crosses over the screenshot territory into more of a screen capture area. This functionality can be very powerful however, as I've found the ability to walk through tech issues with a recording is powerful. I'll probably blog with Camtasia eventually as I get more time to do audio recordings. The primary benefit I've found is the easy gif creation with full annotations, blurring, and other effects that help when providing examples while protecting some sensitive connection/context information.</p> <p>With Camtasia, I've found it tremendously intuitive, as I've already worked with NLE (Non-linear editors), and the process of putting together a video with it is very simple. Some products I've tried that compare somewhat would be ActivePresenter, which is great for creating tutorials with a lot of \"whitespace removed\" by detecting the activity via keyboard and clicks.</p> <p>However, I'd say it still has a hurdle of usability, and is not intuitive compared to the Techsmith offering, making it a great option if you have _zero _budget, but not a good option if you really want to create quickly a quality video.</p>","tags":["sql-server","cool-tools"]},{"location":"best-tools-for-taking-notes-in-development/#conclusion","title":"conclusion","text":"<p>Quality tools can help the development workflow. I personally think a tremendous amount of value comes from utilizing images for varied capturing of work in addition to typed notes, as you can gain a lot of information this way. Hopefully, my suggestions might help spark a few new ideas for great tools to help with your workflow.</p>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/","title":"Documenting Your Database with Diagrams","text":"<p>Don't get too excited. I know you love documentation, and just can't wait to spend some time digging in to document your database thoroughly. I imagine you probably want to build visio charts manually, or draw whiteboard diagrams by hand and take pictures.</p> <p>For the rest of us that are lazy, a tool to help document your database is a great idea. I'm a big fan of SQL Doc by Red Gate, and ApexSQL Doc . I ended up using ApexSQL doc to document the database at my work, though Red Gates is also a great option. Both provide a great schema documentation tool that can generate a helpful CHM file to browse through (which I much prefer to trying to sort through PDF/WORD docs)</p> <p>However, there is one thing that I was recently tasked with that made me appreciate a little more hands on documentation. In the case that you are deploying an application or any set of tables/structure that might end up being used by someone else, describing your data model can be a really helpful resource to someone trying to query or implement some custom implementation against your data. This might be helping document some data structures for someone building a report, or even developers trying to leverage some of the data in a separate implementation.</p> <p>Understanding the data model as a dba/dev is important to being able to architect improvements as well, so I've found that going through and diagramming some of the logical structures/groupings can be a tremendous benefit to better understanding a database that perhaps you didn't architect from scratch, or has legacy results.</p> <p>Some positives I see:</p> <ol> <li>Forces a better understanding of how your data model is constructed</li> <li>Can help highlight weaknesses and flaws in your data model</li> <li>Can help explain the data model to folks who may not be living and breathing your business, so if terminology and fields sometimes are confusing on how things relate, the data model can help clarify some of these things.</li> </ol> <p>Some negatives:</p> <ol> <li>Requires a little elbow grease</li> <li>You'll find things to fix</li> </ol>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#visio","title":"Visio","text":"<p>Visio 2010 is the last Visio version that provides the capability to reverse engineer a database into a diagram. This is unfortunate. I worked for a while with this, but ended up shelving Visio as a long term option because of it's unintuitive nature, and behavior at times. For example, refreshing your database schema to identify changes wouldn't help flag any new changes, you'd have tables that were in several diagrams suddenly being given new table names like TableTest, TableTest1, TableTest2, instead of reusing the same table in the model. Also, the layout and arrangement can be pretty messy at times. Working with data sources requires you to use GENERIC OLEDB from what I recall, otherwise I got errors using SQL 2014. Lots of little things that added up to a poor experience. Gave it the old college try.... not worth it in my personal opinion</p>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#visual-paradigm","title":"Visual Paradigm","text":"<p>Visual Paradigm Gallery  This tool has some amazing functionality, but unfortunately was very tedious and complex for the purposes of documenting an existing database. I think this aspect of Visual Paradigm's reverse database toolset needs some major productivity and UI tweaks to be usable. It may be great for someone working with diagrams for creating databases and models, but for a SQL dba working to document their database better, it wasn't a great option. Even though very unintuitive, I did like the customization options for layout and the arrangement. The problem was none of the changes performed in realtime, thereby making it an annoying guesswork game. Not a huge fan. Also, very difficult to easily identify tables that had related tables to add with a click, so not easy to navigate all the tables and related tables in an intuitive way. Not a big fan of having to tweak 10 settings via 10 trips to a right click context menu. Their presentation workflow needs some major improvements.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#dbvisualizer","title":"DBVisualizer","text":"<p>DbVisualizer By far the best auto arranging layout tool I found. Amazing. It uses yEd as the backbone of the diagramming, so I also downloaded yEd, but for me to utilize, I'll definitely have to play around as the engine is powerful, but would take programming to get it to do anything similar to what DbVisualizer accomplished. It's more of an exploratory tool than a documenting one, but you can save the graph to .gml format and open in yEd to change. A little extra work, but possible if you like the results. It doesn't display foreign key columns, so it's more of looking at the referring/relationship side of things, without a full column list. I'm pretty much sold that this is bar none the best auto-laid out representation of tables I've ever seen with any tool. I'll be keeping the free DbVisualizer as a tool for reviewing adhoc data models for sure! The negative for documentation purposes is that none of these graphs are saved, so it's good for adhoc or one time, but not to regenerate.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#aquasoft-data-studio","title":"Aquasoft Data Studio","text":"<p>They have a great output, but unfortunately they don't allow you to only show key columns, therefore your diagrams get pretty bloated. It's pricey, has some great other features, and I'd be a big fan, except the diagramming options aren't as robust as the others for customization. If you are documenting a data model, I like to only show key columns (PK/FK) to ensure there is not excessive noise. I did reach out to company on this option, and they said: \"You can't show the PK and FK without the entity. So, you can't show them be themselves.\" The lack of any extra options to submit feedback, or anything as a potential buyer put me off to this product for now.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#other-tools-that-were-way-too-complex-and-pricey-to-fit-documentation-needs","title":"Other tools that were way too complex and pricey to fit documentation needs","text":"<ol> <li>Oracle SQL Developer - very difficult to get the SQL connection up and running. Not unintuitive, not visually pleasing output</li> <li>Toad Data Modeler - same as above. Not visualizer pleasing, too many steps. Targeted at design rather than documentation.</li> <li>ERStudio... didn't even consider, much too pricey for documentation purposes</li> <li>SQL Architect - much too unintuitive and complex</li> </ol>","tags":["sql-server","cool-tools"]},{"location":"documenting-your-database-with-diagrams/#final-winner-dbschema","title":"Final Winner: DbSchema","text":"<p>DBSchema This is the winner. I switched all our diagrams over to this tool. A few major pros:</p> <ol> <li>All diagrams saved as \"layouts\" in your project. This means I can come back at anytime and fix/tweak.</li> <li>I can refresh the database schema imported and it will flag all the changes, and then the diagrams will automatically get updated.</li> <li>It generates really nice HTML interactive documentation. Major win! The major pro I found for this is with a little upfront work in arranging better, I could refresh all diagrams (maybe a new fk gets added), and then bulk export all these as HTML interactive database diagrams with full detail. You can hover over the FK lines and it will highlight the relationship etc. This is great for usability! Also, removes a lot of manual work in refreshing database column diagrams if something gets added/changed. One of the most significant points to this tool is the functionality + price! The price for a commercial licenses is extremely reasonable (as of 2015-12-09 the pricing was under $200) Some things that could use improvement:</li> <li>Improved autoarrangement - Allow preset option of only showing FK/PK ( i have an autohotkey script I built to do this, contact me if you want it, helped simplify)</li> <li>Allow optional layouts like Tree, Hierarchical</li> </ol> <p></p>","tags":["sql-server","cool-tools"]},{"location":"model-needs-exclusive-lock/","title":"Model needs exclusive lock","text":"<p>Ran into an issue where a developer was trying to create a database and was denied due to no ability to obtain exclusive lock on model. After verifying with other blogs, I found that creating a database required exclusive lock to use model as a template for the new database creation.</p> <p>In my case I had connected with SSMS directly to model for some queries instead of master. In this case, SQL Complete (Devarts's excellent alternative to SQL Prompt) was querying the schema had this open session was blocking usage of model to create a new database. After killing this low priority query session, no issues were experienced.</p> <p>Good to remember! Don't connect directly to model unless you have a specific reason to do so. Otherwise, you might be the culprit on some blocking errors.</p>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/","title":"Transaction Logging & Recovery (101)","text":"<p>Logging &amp; Recovery Notes from SQL Server Logging, Recovery, and the Transaction Log (pluralsight) Paul Randal Going to share some key points I've found helpful from taking the course by Paul Randal at pluralsight. I've found his knowledge and in detail presentation extremely helpful, and a great way to expand my expertise. Highly recommend pluralsight.com as well as anything by Paul Randal (just be prepared to have your brain turned to mush... he's definitely not writing \"how to write a select statement\" articles</p>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/#logging-recovery","title":"Logging &amp; Recovery","text":"<p>Why is it important to know about this?</p> <ul> <li>One of the most critical components to how SQL server works and provides the Durability in ACID (Atomicity, Consistency, Isolation, Durability) is the logging and recovery mechanisms.</li> <li>If log files didn't exist we couldn't guarantee after a crash the integrity of the database. This recovery process ensures after a crash even if the data file hasn't be changed yet (after a checkpoint) that SQL server can reply all the transactions that had been performed and thereby recover to the consistent point, ensuring integrity. This is critical and why we know even with a crash that no transactions will be left \"halfway\", as we'd require the transactions to be harden to the log file before SQL server would allow the data file to be changed. If this was done in the reverse order of writing to the data file, then if a crash happened, the log file might be out of sync, and you couldn't reply actions that might not have been fully made to the log file as the data file and log file wouldn't be in sync with the transactions noted.</li> <li>The logging &amp; recovery mechanism is actually the backend architecture driving most of the disaster recovery options like Mirroring, Replication, Availability Groups and more. They each have different ways of handling/processing, but underneath, they all rely on utilizing the transaction logs. - Logging = the backbone of sql server.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/#data-files-arent-changed-immediately","title":"Data Files Aren't Changed Immediately","text":"<ul> <li>Unlike what you might think initially, data files are actually note being written to realtime with transactions. This would be inefficient. Instead, the log is written to, hardened, and then periodic checkpoints (default I believe is 1 minute) take these changes that have been hardened and ensure the changed pages in the buffer (dirty pages at this point) are updated as well.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/#cant-get-away-from-transactions","title":"Can't get away from transactions","text":"<ul> <li>Note that all actions occur in transactions. If not explicitly stated, then an implicit transaction is gathered by SQL server in how it handles the action (when dealing with DML). For example, if we alter 2 tables, we could manually set a transaction for each, so if one fails, both were rolled back to the original state, allowing us to commit one table changes even if the other experienced a failure. If we combined both of these statements without defining a transaction then SQL server would imply a transaction, and this might result in a different behavior.</li> </ul> <p>For example:</p> <pre><code>    alter table foo\n    alter table bar\n    GO\n    -- if foo failed, then both tables would not be changed, as the transaction itself failed\n</code></pre> <p>on the other hand the statement below would be promised in SSMS as a batch separator and SQL server would have two separate transaction for each. If one had an error, then the other would still be able to proceed.</p> <pre><code>    alter table foo\n    GO\n    alter table bar\n</code></pre>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/#everything-gets-logged","title":"Everything gets logged!","text":"<ul> <li>Everything has some logging to describe changes in log file (even in simple recovery)</li> <li>Version store &amp; workfile changes in tempdb</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28101%29/#commitment-issues","title":"Commitment Issues","text":"<p>Transaction Has Committed</p> <ul> <li>Before the transaction can commit, the transaction log file has to be written through to disk. - Once the transaction log is written out to disk, the transaction is considered durable</li> <li>If you are using mirroring, the system will stop and wait for the replica/mirror to harden the transaction to the mirror db log file on disk, and then can harden the transaction log to the disk on the primary.</li> <li>The Log file basically represents an exact playback of what changes have been made, so even if the buffer was cleared (removing the pages that were changed in buffer), SQL crashed, or your server went down, SQL server can recover the changes that were made from the log file. This is the \"description\" of the changes that were made.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-2%29/","title":"Transaction Logging & Recovery (part 2)","text":"<p>Continuation of some notes regarding the excellent content by Paul Randal in Pluralsight: SQL Server: Logging, Recovery, and the Transaction Log. Please consider supporting his excellent material by using Pluralsight and subscribing to his blog. He's contributed a vast amount to the SQL server community through SQLSkills</p>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-2%29/#transaction-log-file","title":"Transaction Log File","text":"<ul> <li>The initial size of the log file is the larger of 0.58 MB or 25% of the total data files specified in the create database statement. For example, if you create a database with 4 separate files, it would increase the initial size the log file is set to.</li> <li>This would be different if you've changed MODEL database to set the default log and database size.</li> <li>The log file physically created must be zero initialized. Note that the benefits of instant file initialization apply to the data file, but the log file still has to be fully zero initialized, so a large log file creation doesn't benefit from instant file initialization. Previous Post on Enabling File Initialization --- Examine the errorlog (after you've enabled trace flag 3605,3004) EXEC xp_readerrorlog; GO</li> <li>When examining the results, you can see the zeroing of the log file, but not the datafile if you have instant file initialization enabled.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-2%29/#virtual-log-files","title":"Virtual Log Files","text":"<ul> <li>The transaction log is divided into virtual log files. This helps the system manage the log file more efficiently.</li> <li>New VLF's are inactive &amp; not used.</li> <li>Active VLF's contain the log record activity and can't be reused until they have been noted as available by SQL server. My seque based on the fun experience of giant log files.</li> <li>Note: In Brentozar Office Hours Brent talked about the common misconception of SIMPLE VS FULL logging. Most folks (guilty) think that SIMPLE reduces the amount of logging SQL server performs, thereby improving the overall performance. However, in a general sense this is a misconception. Logging, as previously discussed from my previous post on (101), is the core of SQL server, and required for transaction durability. The difference between SIMPLE and FULL is mostly to do with how the transaction log space is marked as available for reuse.</li> <li>SIMPLE: after data files are updated and the data file is now consistent with the changes the log has recorded, the transaction logs are now marked as free and available.</li> <li>FULL: all the transaction log records, even after hardened to the data file, are still used. This is what can cause the common issue of exponential log file growth with folks not aware of how it works. 300 GB log file on a small database due to now one watching? Been there? This is because the log file will keep appending the log entries overtime, without freeing up space in the existing transaction log, unless some action is taken to let SQL server know the transaction log file space is available for reuse.</li> <li>Marking the space as available is done by ensuring you have a solid backup solution in place that is continually backing up the transaction log in the backup set) thereby letting SQL server know that the transaction log has been backed up and space can be reused in the existing log file.</li> <li> <p>The normal process would be to ensure you have a full backup, incremental backups, and transaction log backups running on a schedule.&gt; Under the full recovery model or bulk-logged recovery model, if the transaction log has not been backed up recently, backup might be what is preventing log truncation. If the log has never been backed up, you must create two log backups to permit the Database Engine to truncate the log to the point of the last backup. Truncating the log frees space for new log records. To keep the log from filling up again, take log backups frequently.</p> <p>MSDN Troubleshooting a Full Transaction Log</p> </li> <li> <p>My past experience was running into this challenge when performing a huge amount of bulk transactions. I ran the space out on a drive because the log files continued to grow with no backups on the log file running. The solution in my particular situation was to take a full backup, change the database recovery to Bulk-logged or SIMPLE, perform the massive changes, then get right back to full-recovery with backup. This helped ensure the log file growth didn't keep escalating (in my case it was the appropriate action, but normally you want to design the size of the transactions to be smaller, and the backup strategy to be continual so you don't run into this issue)</p> </li> </ul>","tags":["sql-server"]},{"location":"verifying-instant-file-initialization/","title":"Verifying Instant File Initialization","text":"<p>Ran into a few issues verifying instant file initialization. I was trying to ensure that file initialization was enabled, but found out that running the xp_cmd to execute whoami /priv could be inaccurate when I'm not running it from the account that has the privileges. This means that if my sql service account has different permissions than I do, I could get the incorrect reading on if it is enabled.</p> <p>Paul Randal covers a second approach using the sysinternals tool Accesschk, which seems promising. However, in my case, I didn't have permissions to run in the environment was I was trying to check. I found a way to do this by rereading original article in which Paul Randal demonstrates the usage of trace flags 3004,3605. This provided a very simple way to quickly ensure I was getting the correct results back. For even more detail on this, I highly recommend his Logging, Recovery, and Transaction Log course. I adapted pieces of his script for my quick error check on this issue.</p>","tags":["sql-server"]},{"location":"verifying-instant-file-initialization/#successfully-verifying","title":"Successfully Verifying","text":"<p>Successfully added instant file initialization should mean when you review the log you will not have any MDF showing up in the error log for zeroing. I adapted the sql script for reading the error log in a more filtered manner from this post: SQL Internals Useful Parameters for XP Reader (2014)</p> <p></p>","tags":["sql-server"]},{"location":"seeker-%26-servant-fantastic-music-with-incredible-dynamics/","title":"Seeker & Servant: Fantastic Music with incredible dynamics","text":"<p>Just recently discovered this artist after being exposed to an article from Worship Leader magazine. Fantastic dynamics. The dynamics and beautiful harmonies are pretty darn close to what I'd love experimenting with if I had a group of folks playing those instruments. Interestingly, the vocal harmonies are very simple, but I found very beautiful. It's compromised of a tenor and a baritone range, and is a fresh change stuff like Shane and Shane which both have incredibly high ranges. I found it very approachable to enjoy singing with. The power of the musical dynamics and beautiful lyrics was a major win. I'll be following them closely.</p> <p>Get their latest album for free here: Seeker &amp; Servant Similar Artists/Albums:</p> <ul> <li>Fleet Foxes</li> </ul> <ul> <li>Dustin Kensrue: Carry the Fire</li> </ul> <p>Would Like If:</p> <ol> <li>You like post-rock</li> <li>Long Vamping Dynamics</li> <li>Minimalist Arrangements</li> <li>Tight 2 part harmonies</li> </ol> <p>Here's the album on spotify for those who want to check it out!</p>","tags":["ramblings","music"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/","title":"Transaction Logging & Recovery (part 3)","text":"<p>Continuation of some notes regarding the excellent content by Paul Randal in Pluralsight: SQL Server: Logging, Recovery, and the Transaction Log. Please consider supporting his excellent material by using Pluralsight and subscribing to his blog. He's contributed a vast amount to the SQL server community through SQLSkills This is my absorbing of key elements that I never had worked through</p>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#jackalopes-are-realso-are-virtual-log-files","title":"Jackalopes Are Real....so are Virtual Log Files","text":"<p>Ever seen a picture of a jackalope? Image by Mark Freeman (Jackalope, Grand Canyon North Rim, Oct 07) Creative Commons License</p> <p>This is how I used to feel about Virtual Log Files. Folks were saying things like</p> <ul> <li>\"Your server may be slowing down because of those darn VLF's\".....</li> <li>\"Have you checked your VLF count\"...</li> <li>\"My VLF count was x\" and more</li> </ul> <p>Finding clarification on VLF (Virtual Log Files) can be difficult, as what is considered a high count for some may be contradicted by another with another \"target VLF count\" claim. Paul Randal unpacks this excellently in his class, providing some great transparency.</p> <p></p>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#why-should-i-care-about-vlfs","title":"Why Should I Care About VLFs?","text":"<p>In an excellent article regarding the performance impact analysis of VLF's, Linchi Shea provides some valuable insight into the impact. For more detailed analysis &amp; graphs please look at this great article: Performance impact: a large number of virtual log files - Part I (2009)</p> <ol> <li>Inserts were about 4 times as slow</li> <li>Updates were about 8 times slower</li> <li>Deletes were about 5 times slower</li> <li>Recovery time can be impacted Slow recovery times and slow performance due to large numbers of Virtual Log Files (2008)</li> <li>Triggers &amp; Log Backups can be slowed down Tony Rogerson article (2007)</li> </ol>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#virtual-log-files","title":"Virtual Log Files","text":"<ul> <li>At the beginning of each log file is a header. This is 8kb header that contains settings like autogrowth &amp; size metadata.</li> <li>Active VLF's are not free for usage until they are marked as available when clearing the log (see previous post about backups)</li> <li>When you create a db you have one active VLF file, but as you progress more VLF's will be used.</li> <li>Too few or too many VLF's can cause problems.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#vlf-count","title":"VLF Count","text":"<ul> <li>You cannot change the number and size of VLF's in a new portion of the transaction log. This is SQL server driven.</li> <li>The VLF size is determined by a formula.</li> <li>For detailed breakdown of the changes that SQL 2014 brought for the VLF algorithm, see this excellent post by Paul Randal: Important change to VLF creation algorithm in SQL Server 2014  Since I'm working with SQL 2014, I found it interesting as the increased VLF count issue that can be impacting to server performance has been greatly improved. Paul's example cited that the number of VLF's in his example would result in 3192 VLF prior to 2014, but with SQL 2014 it decreased down to 455, which is a substantial improvement. Paul indicated that the prior algorithm was designed primarily for around 1997-1980's, when log files wouldn't be sized as large. Also note a critical question that he answers: COMPATIBILITY LEVEL IS IGNORED BY THE STORAGE ENGINE PROCESSOR This is great information he's shared, as I've found it confusing at times to separate out the Query Engine impact from compatibility level, and understanding this scope of impact can help with assessing possible impact.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#more-detail-than-you-ever-wanted-to-know-on-vlfs","title":"More Detail than You Ever Wanted to Know on VLF's","text":"<ul> <li>VLF's internally contain log block sizes. 512-60KB.</li> <li>When the log block is filled it must be flushed to disk.</li> <li>Within the log block are the log records.</li> <li>VLF's contain a header. This indicates whether or not the VLF is active or not, LSN, and parity bits.</li> <li>VLF log records support multiple concurrent threads, so the associated transaction records don't have to be grouped.</li> <li>LSN. I've heard the term used, but until you understand the pieces above, the term won't make sense. - Log Sequence Number = VLF Sequence Number : Log Block Number : Log Record</li> <li>They are important as the LSN is stamped on the data file to show the most recent log record it reflects, letting sql server know during crash recovery that recovery needs to occur or not.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#number-of-log-files","title":"Number of Log Files","text":"<p>This is determine by a formula that has been updated for 2014.</p> <ul> <li>Different size growths have different number of VLFs.</li> <li>VLF's don't care about the total size, but instead about the growth.</li> <li>For instance, Above 1 GB growth events on log file will split into 16 new VLF's, 1/16.</li> </ul>","tags":["sql-server"]},{"location":"transaction-logging-%26-recovery-%28part-3%29/#faq-ive-asked-and-looked-for-some-answers","title":"FAQ (I've asked and looked for some answers!)","text":"<p>**Create small log and then expand or create larger log initially? **</p> <p>Paul Randal answered:  No. If I was creating, say a 64 GB log, I'd create it as 8GB then expand in 8GB chunks to 64GB to keep the number of VLFs small. That means each VLF will be 0.5 GB, which is a good size. What is the ideal l number of VLFs? Some key articles I've found for detailed answers on understanding proper VLF count:</p> <ol> <li>Transaction Log VLFs - too many or too few (2008)</li> <li>8 Steps to better Transaction Log throughput (2005)</li> <li>A Busy/Accidental DBA's Guide to Managing VLFs (2009) Resources</li> <li>Brentozar SP_BLITZ will check VLF counts How do I ensure my log file gets marked as available for reuse when in full recovery? Full recovery is required for point-in-time recovery after a failure. This is because every change to data or to database objects are written to the transaction log prior to being committed. These transactions are then written to the data file as SQL Server sees fit after this initial write to disk. The transaction log is a rolling history of all changes in the database and will allow for redo of each transaction in case of failure to rebuild the state of the data at failure. In the case of Full Recovery, the transaction log continues to expand until a checkpoint is issued via a successful transaction log backup. Top 13 SQL Server Mistakes and Misteps (2012) This great article by Tim Ford should be reviewed, as it's one of the best simple breakdowns of growth issues and prevention that I've read.</li> </ol>","tags":["sql-server"]},{"location":"diff-all-files-reviewing-changesets-quickly/","title":"Diff all files","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2017-07-14\" &gt;}} Still find this incredibly awesome! Developer just updated for Visual Studio 2017 after a lot of hard work. Github repo here for any issues. Big thanks to DeadlyDog for this great tool and putting in the effort to update for VS2017. deadlydog/VS.DiffAllFiles: Visual Studio Extension to make comparing files before and after committing them to Git and TFS faster and easier {{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"info\" title=\"Updated: 2016-01-25\" &gt;}} I started using the option to compare a single file at a time, since it picks up the syntax highlighting then. The other way (merge files into one) can't handle syntax highlighting with the various files types all mixed into the same window. Diff All Files extension handles this beautifully with proceeding one by one and automatically opening the next file to compare or allowing you to hit next to close and reopen with the next file to compare. I still enjoy using this due to the \"context\" lines that reduce the noise to just the actual section being changed. {{&lt; /admonition &gt;}}</p> <p>If you need to do code comparison on a lot of files in Visual Studio, I ran across an extension Diff All Files that is really helpful for merging down the individual file changes into more consumable format.</p> <p>In the changeset view you can select diff all files and whatever settings you've setup in the Tools &gt; Options &gt; Diff all Files settings will then feed through automatically.</p> <p></p> <p>You then click on diff all files in the changeset viewer</p> <p></p> <p>All the non-excluded items will then be merged into a single file, if this option was selected, or separate windows. Personally, I've found the merged view really helpful to have one single pane to scroll through.</p> <p></p>","tags":["coding","cool-tools","sql-server"]},{"location":"diff-all-files-reviewing-changesets-quickly/#third-party-comparison-perks","title":"Third Party Comparison Perks","text":"<p>To go a step beyond this you could use a third party comparison tool. My preferred option is Araxis Merge (disclaimer: they provided me with a license for evaluation, which doesn't impact my assessment). I really like the fact it merges down similarities into a concise comparison. This tool gives you the option to provide only the different lines, with a X number of lines before and ahead for context. This could reduce thousands of lines of code to just a few hundred that have variances. Win! Highly recommend you check them out, as if you are doing open source they have a free license for that. If you do a lot of code comparisons, some of the additional functionality in their app might be worth it, otherwise the built in viewer in TFS is pretty good.</p> <p>In my test comparison, I reduced 3245 lines of code in the comparison window to 25 lines, which was the actual changes + 10 lines above/below for each change to have context. This resulted in only a few hundred lines to scroll through. Made my life easier! The only con for me with Araxis is no dark theme.... but I'll live.</p> <p></p>","tags":["coding","cool-tools","sql-server"]},{"location":"diff-all-files-reviewing-changesets-quickly/#configuring-third-party-tools","title":"configuring third party tools","text":"<p>I saved this snippet from working through Araxis documentation for setting up their tool with Visual Studio.</p> <p>I also just ran across another site that had a nice resource list of configuration settings for different comparison tools.  Diff All Tools - Visual Studio Extension</p>","tags":["coding","cool-tools","sql-server"]},{"location":"diff-all-files-reviewing-changesets-quickly/#other-comparison-tools","title":"Other comparison tools","text":"<p>Third party tools can be great for some extra perks like File versioning, comparing folders, and more. You can get by without them, of course, but if you do a lot of change review consider some of the options. A few others I've reviewed (if pro/paid they provided license for me to evaluate)</p> <ul> <li>Delta Walker (pro): Great UI, does image similarities comparison tool. Didn't seem to have a \"show only changed lines with context\" like Araxis at the time I reviewed. Mac app!</li> <li>Devart Code Compare (pro/free): I love most programs Devart puts out. This is a good comparison app. Wasn't my preferred option, and at times struggled with large files, but overall was good in comparison accuracy.</li> <li>DiffMerge: Brief overview, solid tool, just didn't have specific improvements I needed (focused on changeset/sql comparison)</li> <li>Beyond Compare: didn't dive into this extensively. They have a devoted following with a lot of loyalty, so check them out if you want to evaluate their software.</li> <li>Built in TFS diff - great for a built in tool, I just wanted to have some enhancements in what to ignore and summarize.</li> <li>SQL Compare: my favorite for reviewing changsets that are directly in TFS. I wish they'd add a module for just pulling up the diff view without having to open SQL Compare to setup a comparison though. Sounds like a good user voice feedback item :-)</li> </ul>","tags":["coding","cool-tools","sql-server"]},{"location":"easy-way-to-test-log-shipping-or-availability-groups-setup/","title":"Easy way to test log shipping or availability groups setup","text":"<p>Have been working through the fantastic training resources from Brent Ozar's Everything Bundle and on the recommended resources they mention after all the training on log shipping and availability groups that you can use a lab environment from TechNet to actually get going on familiarizing yourself with the technology more.</p> <p>This is great! Of course, it's not the full deal, but this gives a tangible way to get moving on familiarizing yourself with this complex technology. TechNet Virtual Labs Availability Groups Side note: Fantastic training resource with great roi with Brent Ozar Unlimited \"Everything Bundle\" if you apply the current podcast review discount of 78%. Great value with immediate return. I also like pluralsight, but find it takes more time investment to get the immediate value. Their courses are short, compact, and full of great material.</p> <p>Recommend you add to your training budget asap. Podcast Review Discount Combine this with a subscription to Office Hours with the app Pocket Cast, and you'll have an easy way to keep up to date with some great tips.</p>","tags":["sql-server"]},{"location":"calculating-some-max-mirror-stats/","title":"Calculating Some Max Mirror Stats","text":"<p>This turned out to be quite a challenge. I couldn't find anything that made this very clean and straight forward to calculate, and in my case I was trying to gauge how many mirroring databases I could run on a server.In my scenario, I wasn't running Expensive Edition (@BrentO coined this wonderful phrase), so was looking for the best way to assess numbers by doing mirroring on a large number of databases, in my case &gt; 300 eventually. The documentation was... well.... a bit confusing. I felt like my notes were from the movie \"A Beautiful Mind\" as I tried to calculate just how many mirrors were too many! This is my code snippet for calculating some basic numbers as I walked through the process. Seems much easier after I finished breaking down the steps. And yes, Expensive Edition had additional thread impact due to multi-threading after I asked about this. Feedback is welcome if you notice a logical error. Note that this is \"theoretical\". As I've discovered, thread count gets reduced with increase activity so the number mirrored database that can be mirrored with serious performance issues gets decreased with more activity on the server.</p>","tags":["sql-server"]},{"location":"failover-all-databases-to-other-server/","title":"Failover all databases to other server","text":"<p>Quick snippet I threw together to help with failing over synchronized databases to the other server in bulk. No way I want to click that darn Fail-over button repeatedly. This scripts the statements to print (i commented out the exec portion) so that you can preview the results and run manually.Note that it's also useful to have a way to do this as leaving databases running on the mirror server for an indefinite period can violate licensing terms on the secondary server when it's a fail-over server and not meant to be the primary.</p>","tags":["sql-server"]},{"location":"previewing-the-new-ssrs-2016-portal/","title":"Previewing the new SSRS 2016 portal","text":"<p>Ran into an issue with the \"Preview New Reporting Portal\" link on a fresh install of 2016 giving me a not found error.</p> <p></p> <p>Changing the virtual directory in the Report URL tab for SSRS configuration fixed this invalid link. In my case, I changed /Report to /Reporting. Thanks to Adam on Stack Overflow for providing the solution and saving me a lot of time!</p>","tags":["sql-server","ssrs"]},{"location":"sql-2012-sp3-and-entity-framework-conflict/","title":"SQL 2012 SP3 and entity framework conflict","text":"","tags":["entity-framework","sql-server"]},{"location":"sql-2012-sp3-and-entity-framework-conflict/#the-problem","title":"the problem","text":"<p>An issue with SQL Server 2012 SP3 was identified that impacted EF4/5 due to additional datatypes in the dll.</p> <pre><code>System.EntryPointNotFoundException: Unable to find an entry point named 'SetClrFeatureSwitchMap' in DLL 'SqlServerSpatial110.dll'\n</code></pre>","tags":["entity-framework","sql-server"]},{"location":"sql-2012-sp3-and-entity-framework-conflict/#diagnosing","title":"diagnosing","text":"<p>To easily identify the available dll versions of sql server, I ran a quick adhoc bat file.</p> <p>The output returns a simple text file like this:</p> <p> A post in technet mentioned that the DLL shipped with SP3 could cause these conflicts and if the uninstall didn't clean up the GAC correctly, problems could occur with Entity Framework calls.</p> <p>Can confirm in my case it was due to dll shipped in SQL Server SP3.  I had to uninstall the patch but the newer dll was still in the gac so I had to overwrite with the older version using gacutil. ( Edited by snowcow Thursday, January 14, 2016 12:41 PM )</p>","tags":["entity-framework","sql-server"]},{"location":"sql-2012-sp3-and-entity-framework-conflict/#the-fix","title":"The Fix","text":"<p>In my case, I still needed the current SP3 version, but we wanted to make sure that the app was pointing to the older version to avoid this error. I apparently needed to point backwards to:  <code>C:\\Windows\\assembly\\GAC_MSIL\\Microsoft.SqlServer.Types\\10.0.0.0__89845dcd8080cc91</code> Stack Overflow, the golden mecca of programming knowledge, saved the day with a solid answer</p> <p>EF Cannot Update Database This forces the EntityFramework to use the version 10 of the SqlServer.Types.dll, which doesn't have the Geometry type apparently. - KdBoer When the fix was applied to map the application config to the older version of the Microsoft.SqlServer.Types.dll (in this case 10). Apparently the 2012 SP3 provided some additional functionality in the dll and this had a conflict with Entity Framework 4 for my situation (and according to online posts EF5 also had some issues)</p>","tags":["entity-framework","sql-server"]},{"location":"tfs-work-item-fancy-filtering/","title":"TFS Work-Item Fancy Filtering","text":"<p>If you want to create a TFS query that would identify work items that have changed, but were not changed by the person working it, there is a nifty way to do this.The filtering field can be set to &lt;&gt; another field that is available, but the syntax/setup in Visual Studio is not intuitive. It's in the dropdown list, but I'd never noticed it before!</p> <p></p> <pre><code>AND ' Changed By ' &lt;&gt; [Field] ' Assigned to\n</code></pre> <p>Note that you don't include brackets on the assigned to field, and that the &lt;&gt; [Field] is not a placeholder for you to type the field name in, it's actually the literal command for it to parse this correctly.</p> <p></p>","tags":["development","sql-server","tfs","ramblings"]},{"location":"failed-to-initialize-sql-agent-log-not-worthy/","title":"Failed to Initialize SQL Agent Log... not worthy","text":"<p>Moving system databases in SQL Server takes a bit of practice. I got that again, along with a dose of SQL humility (so tasty!), today after messing up some cleanup with sql agent server log files.</p> <pre><code>Failed to initialize SQL Agent log (reason: Access is denied).\n</code></pre> <p>I was creating a sql template when this came about. SQL Server Agent wouldn't start back up despite all the system databases having very little issues with my somewhat brilliant sql commands. I had moved all my databases to the new drive location, and changed the advanced startup parameters for sql server and SQL Agent... or so I thought.</p> <p></p> <p>I apparently missed the order of operations with SQL Server Agent, and so it was unable to start. MSDN actually says to go into the SQL agent in SSMS to change this, and I thought I was smarter than msdn....</p> <p>MSDN</p> <ul> <li>Change the SQL Server Agent Log Path From SQL Server Management Studio, in Object Explorer, expand SQL Server Agent.</li> <li>Right-click Error Logs and click Configure.</li> <li>In the Configure SQL Server Agent Error Logs dialog box, specify the new location of the SQLAGENT.OUT file.</li> <li>The default location is C:\\Program Files\\Microsoft SQL Server\\MSSQL . \\MSSQL\\Log. Found the registry entry and changed here... all fixed!</li> </ul> <p> I also updated the WorkDirectoryEntry to ensure it matched new paths.</p> <p>Thanks to this article I was saved some headache. I also learned to read directions more carefully :-)</p>","tags":["sql-server"]},{"location":"cool-tools-powershell-ise-steroids/","title":"Cool Tools: Powershell ISE-Steroids","text":"<p>Disclaimer: I have been provided with a free license because I am reviewing. This doesn't impact my assessment of the tool. I have a passion for finding tools that help developers, and even more specifically SQL DBA/Developers improve their workflow and development. I don't recommend tools without actually using them and seeing if I'd end up adding to my roster of essentials!</p> <p>Cool Tool: ISE Steroids</p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#expanding-powershell-ise","title":"Expanding Powershell ISE","text":"<p>Powershell ISE is simple. Not much fluff, but it gets the job done. You already know you have it on most of your machines, so expanding ISE seems like a logical step if buying something like Sapien's powershell studio or installing Idera's/Dells/etc studios are not something you want to do. I ran across Powershell ISE-Steroids as a recommended \"must have\" and decided to investigate further.</p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#intelligent-code-parsing","title":"Intelligent Code Parsing","text":"<p>For those of us that don't live daily in powershell, having some explanations on best practice and some guidance is a welcome change.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#variable-explorer","title":"Variable Explorer","text":"<p>This is a great implementation of a variable explorer that can dramatically help when debugging scripts.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#diving-into-variable-explorer","title":"Diving into variable explorer","text":"<p>I was having trouble finding the right property and objects with Amazon Powershell SDK for AWS. So I broke up a query to get instances into several steps and then explorered the objects. This made things much easier to explore.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#intelligent-error-checking","title":"Intelligent Error Checking","text":"<p>Helpful description on error when I clicked on the warning icon</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#help","title":"Help","text":"<p>I need powershell help a lot. I don't work enough it in it to have it all memorized, so having a helpful syntax reference guide is a great plus. I'm a big fan of more help...that is easily accessible. I'll take as much \"powershell for dummies\" guidance as possible. I know you can find help with built in powershell functionality, but again, it's the combination of all the little shortcuts and pieces together that help so much. Small benefit is context menu selection of help. With regular ISE, you can run <code>UPDATE-HELP</code>  and then press f1 over a cmdlet to get details, this is just a shortcut on the context menu. However, it's a helpful reminder for those newer to working with Powershell ISE.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#more-help","title":"More help","text":"<p>Help as you've seen it is triggered on executing help against specific cmdlets. However, ISE-Steroids has a context sensitive help that's pretty cool.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#example-of-contextual-help","title":"example of contextual help","text":"","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#refactoring-utility","title":"Refactoring Utility","text":"<p>Lots of great best practice refactoring can be automatically applied.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#refactoring","title":"refactoring","text":"<p>This is a very small example of what it would do. A better case would be a more complex powershell query.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#versioning-of-powershell-scripts","title":"Versioning of powershell scripts","text":"<p>Instead of having to constantly comment and uncomment for fear of losing work, you can keep things nice and clean with the versioning built in. This is great, I wish more editors had it. In this case there is a shortcut to pull up a compare and you can open up the code comparison quickly to see what variations in the script have occurred. Additionally, commenting is possible.</p> <p></p>","tags":["cool-tools","powershell","sql-server"]},{"location":"cool-tools-powershell-ise-steroids/#thoughts","title":"thoughts","text":"<p>If you are working with powershell then something like ISE Steroids can provide some smart help, ensuring better script quality, and hopefully saving some time. It's a recommended tool for my #cooltools list.</p>","tags":["cool-tools","powershell","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/","title":"Automating SSMS 2016 Updates & Install","text":"<p>{{&lt; admonition type=\"info\" title=\"update 2016-04-27 11:20 - Red Gate SQL Toolkit\" &gt;}} This also is a great help for folks using Red Gate SQL Toolkits. It can help ensure all items are up to date. When a new bundle installer is identified, it would download the new one and you could then trigger the updates of each of the apps you desire, without having to keep run the download later through Red Gate's tool. {{&lt; /admonition &gt;}}</p> <p>Figured I'd share a way to automate the SSMS 2016 updates until it gets it's own fancy self updater. I love staying up to date, but with Power BI, SSMS, and others updating monthly or more and not having any automation for keeping up to date, this is something I find a waste of time that I'd rather automate.</p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#ssms-2016-install","title":"ssms 2016 install","text":"<p>I think this gets a win, as it's by default in a dark theme. If contains the future possibility of dark theme just like Visual Studio, it gets my stamp of hearty approval. According to some social media posts I've read, it's not yet implemented, but bringing the theming and extension capabilities to SSMS is a goal, and some of it should be here soon.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#currently-using-2015-shell","title":"currently using 2015 shell","text":"","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#updates-applied-seperately-from-sql-service-packs","title":"Updates applied seperately from sql service packs","text":"<p>Of course, the main benefit to having the SSMS install as it's own installer/update is we can get regular updates and improvements without it having to align with sql server service packs. This should allow Management Studio to have more rapidly developed and improved product with more frequent releases.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#changelog","title":"changelog","text":"<p>Finally have a changelog to easily review Sql Management Studio updates. As I recall, previously you had to sort through all the changes with sql bug fixes to find what was updated. SQL Management Studio - Changelog (SSMS)</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#ketarin-to-the-rescue","title":"Ketarin to the rescue","text":"<p>Ketarin is one of my favorite tools for automating setup and maintenance of some tedious software products. It takes a little practice to get the hang of it, but it's pretty awesome. It's sort of like a power user version of Ninite. You can automate setup and install of almost anything. The learning curve is not too bad, but to fully leverage you want to benefit from the regex parsing of the webpage to get the download link that changes with version, such as what we might deal with on version changes with SSMS.</p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#download-latest-ssms-version","title":"Download latest SSMS Version","text":"<p>MSDN Installer Location Hopefully, they'll improve the process soon by trimming the size and allowing ssms to autoupdate. Just like Power BI, you have to download the installer for the new version and run the installer to upgrade. As a solution in the meantime, you could leverage the power of Ketarin** ** I created a installer package for running the update automatically, so you could have this setup to check upon startup, and then when a download is detected, download the update, and run silent install. Perhaps this will help you if you want to stay up to date.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#ketarin-passive-install","title":"Ketarin passive install","text":"<p>The version parsing I added into this means you shouldn't need to download the installer unless it detects a new version applied.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#update-ready-to-download-and-apply","title":"Update ready to download and apply","text":"<p>This is what you'd see on computer startup with a fresh update ready and waiting for you.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"automating-ssms-2016-updates-%26-install/#last-setup-note","title":"Last setup note","text":"<p>If you setup Ketarin, to make the app portable, copy the jobs.db from appdata folder, into the application folder and restart. This will make it portable so you can actually put this on a USB, clouddrive, or however you want to make it easily usable on other machines.</p> <p></p>","tags":["automation","cool-tools","sql-server"]},{"location":"google-search-only-results-from-the-last-year/","title":"Google Search Only Results from the last year","text":"<p>Tech changes quick. Reading google postings from something in 2009 is not my first choice.I found an option after digging through some google discussion posts on how to setup the default search in chrome (also applies to other browsers) to automatically apply the advanced filter option for \"results in last year\".</p> <ol> <li> <p>Go to chrome settings menu </p> </li> <li> <p>Navigate to manage search engines </p> </li> <li> <p>Enter the new search engine option . Here is the snippet that sets the new default.</p> </li> </ol> <ol> <li> <p>Set the entry as default </p> </li> <li> <p>Review the great results of your hard work :-) </p> </li> </ol>","tags":["ramblings","tech"]},{"location":"red-gate-dependency-tracker-making-databases-into-moving-art/","title":"Red Gate Dependency Tracker - Making Databases Into Moving Art","text":"","tags":["cool-tools","photography","sql-server"]},{"location":"red-gate-dependency-tracker-making-databases-into-moving-art/#data-structures-are-art","title":"data structures are art","text":"<p>If anyone thinks that working with complex data structures is boring... I don't know what world they live in. The problem is often that sql tables and data structures are just script files and lists of tables in an object explorer.</p> <p>However, once you crack open the visual aspect of database diagramming and data architecture, you can see some interesting patterns emerge.</p>","tags":["cool-tools","photography","sql-server"]},{"location":"red-gate-dependency-tracker-making-databases-into-moving-art/#red-gate-dependency-viewer-tracker","title":"red gate dependency viewer tracker","text":"<p>I've long enjoyed playing around with Red Gate Dependency Tracker. I've found it a great tool for interacting with dependencies and visually working through their relationships. I figured I'd share with others a more artistic side of database architecture, as I've never seen it done, and I found it uniquely beautiful.</p>","tags":["cool-tools","photography","sql-server"]},{"location":"red-gate-dependency-tracker-making-databases-into-moving-art/#visual-review-of-data-structures","title":"visual review of data structures","text":"<p>This is the architecture of the application I work on everyday. I inherited care of a system that shows the attention and design that was put into it. The order of the key relationships is obvious. I've worked on some systems that this diagram would be completely different story without the order and structure. The interesting part about the visual dependency view is you can often see orphaned objects that aren't correctly linked to their parent objects, so it can serve as a useful tool to help improve existing designs. My previous article about tools I've use for this is here...Documenting Your Database with Diagrams Enjoy!</p> <p></p> <p></p> <p></p> <p></p>","tags":["cool-tools","photography","sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/","title":"The Mysterious Black Box of R - For the SQL Server Guy","text":"<p>Took a class from Jamey Johnston @ SQLSaturday #516 in Houston. Lots of great information covered. Follow him for a much more detailed perspective on R. Jamey Johnston  on Twitter @StatCowboy. Did a basic walkthrough of running an R query, and figured I'd share it as it had been a mysterious black box before this. Thanks to Jamey for inspiring me to look at the mysterious magic that is R....</p>","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#setup-to-run-query","title":"Setup to Run Query","text":"<p>Simple-Talk: Making Data Analytics Simpler SQL Server and R This provided the core code I needed to start the process with R, recommend reading the walkthrough for details. To get started in connecting in RStudio to SQL Server run this command in the RStudio console.</p> <p><pre><code>install.packages(\"RODBC\")\n</code></pre> Verify the library is installed by running from the console</p> <pre><code>library()\n</code></pre>","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#running-select-from-view","title":"Running Select from View","text":"<p>This was run against StackOverflow database</p> <pre><code>library(RODBC)\nstartTime1\n&lt;- Sys.time() cn &lt;- odbcDriverConnect(connection=\"Driver={SQL Server Native Client 11.0};server=localhost;database=StackOverflow;trusted_connection=yes;\") dataComment &lt;- sqlFetch(cn, 'vw_testcomments', colnames=FALSE,rows_at_time=1000) View(dataComment) endTime1 &lt;- Sys.time() odbcClose(cn) timeRun &lt;- difftime(endTime1,startTime1,units=\"secs\") print(timeRun)\n</code></pre> <p>I created a simple view to select from the large 15GB comments table with top(1000)</p> <pre><code>USE [StackOverflow]\nGO\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n    CREATE view [dbo].[vw_testcomments] as\n    select top(10000) * from dbo.Comments as C\nGO\n</code></pre> <p></p>","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#viewing-the-results-of-basic-query-in-r-studio","title":"viewing the results of basic query in r studio","text":"","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#running-r-script-in-powerbi","title":"running R script in PowerBi","text":"","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#execute-r-script","title":"execute r script","text":"","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#results-preview","title":"results preview","text":"","tags":["sql-server"]},{"location":"the-mysterious-black-box-of-r-for-the-sql-server-guy/#visualized-in-power-bi","title":"Visualized in Power Bi","text":"","tags":["sql-server"]},{"location":"continual-deployment-of-visual-studio-sql-proj/","title":"Continual Deployment of Visual Studio SqlProj","text":"","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#unveil-the-inner-workings-of-the-esoteric-build-system","title":"Unveil the inner workings of the esoteric build system...","text":"<p>As a data professional, I've never worked extensively with msbuild or other pipelines. I'd been mostly focused on just running schema comparisons and publishing. However, I've had the needed to try and deploy a database project from visual studio automatically, and this is my process through it.</p> <p>There are benefits for those who don't necessarily want to run this against production, but instead for those who want to continually deploy check-ins and run tests, documentation, or other tasks against the deployed schema. This was my personal goal.</p> <p>There are other solutions that plug into this that can make the process easier, such as Red Gate's DLM Automation with SQL Continuous Integration. For this case, since migrating to a new project format wasn't possible until later, I worked with the native sql project from the SQL Server Data Tools in Visual Studio 2015.</p>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#terminology","title":"Terminology","text":"<ul> <li>Build Controller: This is like Service broker. It handles the queuing and assignment of the defined builds to the appropriate agents. It doesn't do any actual building, but instead handles the delegation of the work.</li> <li>Build Agent: This is the \"wrapper\" for msbuild. It does the actual work of building whatever you pass to it.</li> </ul>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#initial-setup-install","title":"Initial Setup &amp; Install","text":"<ol> <li> <p>Configure Build Service Wizard Choose the configure option of just the build service </p> </li> <li> <p>Configure collection </p> </li> <li> <p>Choose configuration options for the build services </p> </li> <li> <p>Setup build service account </p> </li> <li> <p>Finished with configuration. After running checks and resolving any issues (I thankfully had none for this simple install of just build controller/agent; you can proceed </p> </li> <li> <p>Create Build Agents </p> </li> </ol>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#future-configuration-of-build-agents","title":"Future Configuration of Build Agents","text":"<p>Once you've finished the install of the build service and agents, you need to configure them. After completing the install the Team Foundation Server Administration Console should open, if not open manually (start menu)</p> <ol> <li>Review build service configuration </li> <li>Build Configuration Pane </li> <li>Reviewing Build Agent Properties </li> </ol>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#setup-of-build","title":"Setup of Build","text":"<ol> <li> <p>Create new build </p> </li> <li> <p>set trigger to continous integration </p> </li> <li> <p>map your working folder to the database project </p> </li> <li> <p>setup the process details Make sure to the map the project to the .sqlproj file to only build the sqlproj. You can adjust other items as you desire, but this should cover the core settings. </p> </li> <li> <p>Copy Local Make sure to have the new profile copied locally as part of the build or it won't have any publish profile copied when the build is triggered, and therefore might result in hours of you wondering why it is ignoring your profile target settings (true story). You can configure to not do this of course, if you want to have a fixed file on your drive instead of copying from source control each time. Recommendation - Prepare a Publish Profile file </p> </li> <li> <p>Setup Parameters for Build The arguments I choose to use were: (note the publishprofile parameter which wasn't in older versions of SSDT)</p> <p>/t:Build /t:Publish /p:SqlPublishProfilePath=foobar.publish.xml\" /p:PublishScriptFileName=foobar.publish.sql /p:TargetDatabaseName=\"foobar\";TargetConnectionString=\"Data Source=localhost;Integrated Security=True;Pooling=False\" /p:PreBuildEvent= /p:PostBuildEvent=  /p:VisualStudioVersion=14.0</p> </li> </ol> <p>The infuriating thing about working through this is that when it doesn't find the publish profile, it defaults to the \"Deploy\" settings, so in my case it kept trying to deploy the changes to a local database named the same thing as my project. Please don't waste the same amount of time grinding your teeth at MSBuild as I did, and watch out for the paths to be correct! Make sure you have it set to copy the publish profile over or it will not exist and default to the deploy settings.</p> <p>When I omitted the VisualStudioVersion option, it had an error \"unable to connect to target server\"...... 2 days of work later I realized the version parameter was critical for it to use the right msbuild version and deploy. I didn't go into more research on this, so feel free to comment if you more details on this.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#running-msbuild-manually","title":"Running MSBuild Manually","text":"<p>A few tips from my work with launching the process to build the database locally, if you experience an issues. This is work I was doing with MSBUILD 14 (visual studio 2015). I would trigger msbuild and it would deploy the database, but never exit the process to report success to powershell allowing my script to continue.</p> <pre><code>/p:UseSharedCompilation=false               --&gt; (Roslyn compiler bypassed http://bit.ly/1WmMVzx)\n/m:4                                        --&gt; (limit to only 4 cores http://bit.ly/1VSl9uF)\n/nr:false                                   --&gt; same link above\n/verbosity:quiet\n/p:Configuration=Release                    --&gt; don't need debugging for this output, so just output release\n/p:DebugSymbols=false                       --&gt; no need for extra debugging, potential improvement in timing to get rid of this\n/p:DebugType=None\n/t:Build;Deploy                     --&gt; optional. Could rebuild if you want to\n/p:PreBuildEvent=                   --&gt; bypass any prebuild/post build events if you have something doing copying of files around (optional)\nupdate after got everything working:\n/p:VisualStudioVersion=14.0 --&gt; ensure you match the version of msbuild you need\n</code></pre> <p>Optional: If you have further issues with 2015 and want to disable globally the node reuse settings then you can do this with a registry entry. Following the directions from TechDocs I did this.</p> <p></p>","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#resources","title":"Resources","text":"","tags":["sql-server","cool-tools"]},{"location":"continual-deployment-of-visual-studio-sql-proj/#msdn-documentation-tasks","title":"MSDN Documentation Tasks","text":"<p>Creating tasks is documented in</p> <ul> <li>Task Writing: Visual Studio 2013</li> <li>MsBuild Tasks</li> </ul> <p>The MSBuild XML project file format cannot fully execute build operations on its own, so task logic must be implemented outside of the project file. The execution logic of a task is implemented as a .NET class that implements the ITask interface, which is defined in the Microsoft.Build.Framework namespace.</p> <p>The task class also defines the input and output parameters available to the task in the project file. MSBuild Tasks</p>","tags":["sql-server","cool-tools"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/","title":"OmniCompare: A free tool to compare SQL Instances","text":"","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#comparing-instances","title":"comparing instances","text":"<p>When working with a variety of instances, you can often deal with variances in configuration that might impact the performance. Without digging into each instance you wouldn't know immediately that this had happened. There are fantastic tools, like Brent Ozar's SP_Blitz, but this doesn't focus on every single configuration value and cross instance comparison. To supplement great material like that a tool like OmniCompare is great.</p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#side-by-side-comparison","title":"side by side comparison","text":"<p> I am definitely adding to my list of great sql tools. OmniCompare provides a side by side comparison of various configuration and system related values in a side by side format so you can easily see variances in basic configuration.</p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#release-post","title":"release post","text":"<p>I read the post about this by Phil Grayson here . He's got some great examples of what you could use it for, such as auditing, performancing tuning, synchronizing server settings, and more. The link for OmiCompare to try it out</p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#synchronization","title":"synchronization","text":"<p>Apparently, it's got the ability to synchronize some of the configuration settings so you could use a template to help setup a new configuration of sql server quickly. I haven't had a chance to try that piece out, but I will be exploring it for sure! It might be something that is in the works, as I couldn't find the options to synchronize currently in the tool. Sounds a lot more elegant than my homebrewed scripts that a nest of code needing some cleaning. Well done Aireforge team!</p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#easy-visual-summary-of-differences","title":"Easy Visual Summary of differences","text":"<p>Quite a few ways to filter and sort down to the information you care about.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#listing-of-the-servers-you-want-to-compare","title":"Listing of the servers you want to compare","text":"<p>Add more or import a list of them to easily do a comparison on configuration in the environments.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#configuring-a-server","title":"Configuring A Server","text":"<p>Simple and quick to add a server, as well as tag them so you can easily compare based on whatever grouping you see fit. For example, you could compare all common versions, all UAT type environments, etc.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"omni-compare-a-free-tool-to-compare-sql-instances/#configuration-differences","title":"Configuration Differences","text":"<p>Makes it very easy to immediately just view configuration scoped differences between each</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/","title":"SSMS Tools Pack - A Handy Tool for generating CRUD","text":"<p>So I've had this tool around for a while, but never found much usage out of it to be honest. I didn't end up writing a review as I had other tools that did text replacements, and history/session saving. I've always considered this tools implementation of SQL History/Tabs saver the best period, even over Red Gate Tab History, SSMSBoost, etc. However, recommending the tool solely based on it's fantastic history saver wasn't really something I was going to do.However, having to generate some CRUD procs lately I found a new reason to appreciate this tool. I dusted it off, updated to the latest, and looked for the CRUD option I remember it having. Sure enough I ended up saving myself a lot of time and generated procs that were all standard with what I wanted to create. This gets my hearty approval to avoid tedious grunt work on creating procs. Since the tool throws in a great history/session saver to avoid losing work, it's even more of a recommended tool!</p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#crud-generator","title":"CRUD Generator","text":"<p>Disclaimer: I was provided with a license to give me time to fully review. This doesn't impact my assessment of the tool. I don't recommend tools without actually using them and seeing if they'd actually benefit me in my work</p> <p>SSMS Tools Pack First, I know there are some great stored procs/scripts that people have written to do this. I appreciated those, but found I was going to spend a lot of time trying to customize to get the error handling and other scripted pieces in, so I revisted SMSS Tool Pack.</p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#create-crud-from-context-menu-for-entire-database","title":"create CRUD from context menu for entire database","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#general-options","title":"general options","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#replacement-text-options","title":"replacement text options","text":"<p>This has some potential to be very helpful! You could generate the user, date and time, and more to generate some comment headers and more.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#replacement-text-example","title":"replacement text example","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#select-template","title":"select template","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#insert-template","title":"insert template","text":"<p>I prefer a begin try and error catch output syntax. I was able to encapsulate the CRUD generator statements with the syntax I preferred, and no longer had to manually manipulate each file to get it where i wanted it. This was a lot of time saved!</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#update-template","title":"update template","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#warning-case-sensitive-parameters","title":"warning - case sensitive parameters","text":"<p>Make sure to keep the case correct on the variables. This is case sensitive.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#saving-you-a-lot-of-work","title":"saving you a lot of work...","text":"<p>Once you've clicked the generate CRUD the magic happens. The results were a large list of prebuilt stored procedures for doing all the CRUD operations needed, with no extra work required. Win!</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#other","title":"Other","text":"<p>Searching the history of previously executed queries, versions of files edited, and sessions of tabs is all excellently handled in the SQL History Search extension. My favorite part is the useful status messages such as</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#history-of-execution","title":"History of Execution","text":"","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#updates","title":"updates","text":"<p>Some recent updates were released with version 4 that might be beneficial for your workflow such as support for SSMS 2016, renaming of tabs, better insert generator, and some other things. Check out the website for more details.</p>","tags":["cool-tools","sql-server"]},{"location":"ssms-tools-pack-a-handy-tool-for-generating-crud/#its-not-crud-its-quite-nice","title":"its not CRUD.... it's quite nice!","text":"<p>While it doesn't really do full formatting or other things, the organized query execution, CRUD generator, and some other features make it a nice tool if you have the budget to purchase. It's a especially a good tool for those who want to generate CRUD procs easily.</p>","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/","title":"SQL Compare 12: Initial Look","text":"<p>I know there have been a few other folks going into more detail on SQL Compare 12 (beta), but I thought I'd share just a few looks at the new design. Looks pretty slick, and I like where the design is going. Just a quick look, as I'm sure there will be more to cover when it's finally released. Until then....</p>","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/#some-gui-improvements","title":"Some GUI Improvements","text":"<p>Very clean. Not too drastic of a change to mess with current workflows. Still, nice to see some clean design like this.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/#options-dialogue","title":"Options dialogue","text":"<p>Same options, just slightly different navigation/ui design to get there.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/#comparing-databases","title":"Comparing databases","text":"<p>Comparing running... saving me a massive amount of work in not creating scripts by hand :-)</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/#comparison-matches","title":"Comparison matches","text":"","tags":["cool-tools","sql-server"]},{"location":"sql-compare-12-initial-look/#final-thoughts","title":"Final thoughts","text":"<p>Again, this was just a quick peek at what I've looked at. Overall, I like the design choices. UI Design improvements are nice. I would say the next stage would be seeing this better design roll-out into better report styling &amp; formats, making it consistent. Additionally, while the UI improvements are nice, the code diff viewer at the bottom looks pretty similar, and I'd love the ability to have this improved a little. Perhaps even the option to use \"external diff viewer\" and pipe it into my preferred diff viewer (Araxis Merge for me). Officially, I'm part of the Friend's of Red Gate community now, so I'll probably be posting a bit more on their tools as I've got the chance to work with more of them now. I've always been a big fan of their products, and now I get a chance to be part of their community and help with testing, feedback, and advocating their great stuff when it would help folks out. Final thought... why can't all great tools have a full Solarized Dark theme? ;-)</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/","title":"Glasswire: (Giveaway Included) Networking Monitoring even a caveman could","text":"<p>Giveaway details at the bottom for those interestedDealing with development &amp; sql servers, I like to know what type of network traffic is happening on my machine. What is the overhead of monitoring on network bandwidth, what is communicating across servers or even externally?</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#what-is-phoning-home","title":"What is phoning home?","text":"<p>You can create perfmon counters, but realistically sometimes I just want a easy quick overview of network traffic with minimal overhead. I have been using a utility I came across called Glasswire, and found this tremendously helpful. I'd highly recommend taking a look at this. I've installed on a couple of the sandbox sql servers I have worked with and found it really great for evaluating the network traffic occurring from the monitoring services running on them. Disclaimer: I have been provided with a free license to allow me to review the product in detail. This doesn't impact my assessment of the tool. I just love great tools and try to help other developers find them.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#network-traffic-monitoring","title":"Network traffic monitoring","text":"<p>Sounds boring, but Glasswire changed my perspective on this. I always want to know what's \"phoning home\" and using up my bandwidth, but until I tried this app, I never have found something that tracked and reported on it in a clean user friendly way. Process Hacker is my preferred task manager, and it can provide some metrics when running, but not a long term history, and not in a user friendly format for analysis.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#comparing-timeline","title":"Comparing timeline","text":"<p>I found this great as a simple way to compare two SQL server monitor tools traffic against each other. I wanted to know the network traffic load they were generating, and this was a great way to get some quick transparency on the network impact. In this example, the test wasn't perfect as they had slightly different detail level tracking configuration, so just take this as an example.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#comparing-usage-metrics","title":"Comparing Usage Metrics","text":"","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#see-activity","title":"See activity","text":"<p>I really like the transparency of seeing what network activity is occurring on my system. I've found myself evaluating why apps would need to connect rather than just allowing everything through.</p> <p></p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#easiest-firewall-tweaking-ive-seen","title":"Easiest firewall tweaking I've seen","text":"","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#would-i-recommend","title":"Would I recommend","text":"<p>Completely! I've found this to give me a transparency into the network ativity in a great way. PROS</p> <ul> <li>Beautifully thought out design</li> <li>Creates a great awareness of network activity, allowing you to be more proactive on what you allow to send data out CONS</li> <li>Some \"power user\" functionality would be nice, such as being able to customize or get details from the desktop widget, add special alerting on specific apps whenever they request network access, etc. These are small things though. I think the overall design is very elegant and well thought out</li> <li>Price. For normal users this is really expensive for the pro version. However, if you are just interested in some basic monitoring without a long history and desktop toast alerts, you can get the free version and still get great value from it.</li> </ul>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#interview-with-developer","title":"Interview with Developer","text":"<p>I thought the app was unique enough in design and function that it would be great to get a short interview with the owner to share a bit about his development approach, goals, and overall story.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#tell-me-a-little-bit-about-yourself-and-your-company","title":"Tell me a little bit about yourself and your company.","text":"<p>Before GlassWire we made a webcam virtual driver software that allowed you to use your webcam with multiple applications simultaneously.  That company was acquired a couple years ago.  Making a sophisticated driver gave us experience with making drivers, so we used that experience to make our network monitoring driver. Since launch we have been surprised by how many people use GlassWire to keep their data usage low and save money and resources.  For example some of our customers have boats and they are on very strict data plans out on the ocean, so they use GlassWire to see what's wasting their data and also block apps they don't want to use while out at sea. After we launched we were surprised by how feedback was so positive right away.  I was worried that nobody would want the software at all, but people seem to like it and we have now had close to 4 million downloads.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#what-made-you-want-to-build-a-network-application-like-glasswire","title":"What made you want to build a network application like Glasswire?","text":"<p>I always felt that I couldn't see what was happening with my PC's network usage so I built GlassWire for myself so I could instantly see what was happening.  I also had some relatives who lived in a remote area and could only use Satellite Internet access.  Satellite only gives you a little data, then throttles you so it's very useful to see what apps are wasting your data. GlassWire also has a built in bandwidth overage monitor to help with that.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#why-was-qt-chosen-as-your-graphsdesign","title":"Why was QT chosen as your graphs/design?","text":"<p>I tested a lot of different tools to see what was happening on the network but I found them difficult to use, plus I couldn't find any that could go back in time and show me past network usage so our team worked together to build GlassWire. QT allows us to build a beautiful UI and make changes easier over time.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#whats-been-some-of-your-hardest-decisions-in-designing-the-application","title":"What's been some of your hardest decisions in designing the application?","text":"<p>After launching we found that Bittorrent users were causing GlassWire to use too much memory/resources on their PC because Bittorrent communicates with so many hosts simultaneously in such a short time period.  We had to redesign GlassWire to use less resources for these users, and I blogged about it here https://blog.glasswire.com/2016/03/29/how-glasswire-1-2-saves-your-memory-and-resources/.  There were a lot of different hard decisions we had to make, like adding \"loading...\" in some places in the UI to take the load off of GlassWire for users who had too many hosts.  I was worried users may find these short delays annoying bit fortunately nobody seemed to be upset about it and we are continuing to grow, and GlassWire now uses significantly less resources for everyone.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#do-you-have-a-design-philosophy-that-helps-balance-more-features-with-simplicity","title":"Do you have a design philosophy that helps balance more features with simplicity?","text":"<p>I try to look at other popular applications and study what makes them successful.  Currently we are working on our Android application and the work on Android has helped me come up with some ideas on how to improve yet simplify our GlassWire desktop software.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#whats-your-thoughts-on-implementing-user-feedback-vs-bringing-design-choices-that-no-one-even-thought-about","title":"What's your thoughts on implementing user feedback vs bringing design choices that no one even thought about?","text":"<p>We love to get user feedback in the forum and on Twitter, etc...  For me it's easy to come up with feature/design choices because I want GlassWire for myself.  I think I'm a pretty normal person and usually the things that I want others want too.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#any-future-projects-you-want-to-accomplish-not-a-roadmap-but-general-things-youd-love-to-tackle","title":"Any future projects you want to accomplish (not a roadmap, but general things you'd love to tackle)","text":"<p>We're excited about bringing GlassWire to mobile and Mac, but it's not easy so we're trying to make sure our Mac/mobile versions have the same high quality as our desktop software.  We don't want to cut corners, so I hope our fans will be patient!</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#can-you-tell-me-a-few-of-your-design-philosophy-decisions-that-drove-some-of-those-simple-ui-differences-that-are-a-bit-uncommon-like-the-graph-refreshing-smoothly-without-1-second-gaps","title":"Can you tell me a few of your design philosophy decisions that drove some of those simple UI differences that are a bit uncommon? (like the graph refreshing smoothly without 1 second gaps)","text":"<p>Since we built the software for ourselves we tried to create a simple layout that we preferred, kind of like a web browser.  We tried some 1 second intervals but it made the graph look jerky and it hurt the eyes so the team spent a lot of time making the smooth graph we ended up with. One of the main things I wanted myself was to be able to see what caused a spike on the graph and with GlassWire you can just click the spike, then see what hosts/apps were involved in the spike.</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#giveaway-details","title":"Giveaway Details","text":"<p>Glasswire was kind enough to provide me with a license for their Elite version (currently $199) which is a onetime fee. You can get the hookup! To get the hookup on this... I'm making it technical since this is not for sweepstake surfers but my techy SQL friends.... Reminder: This is a Windows application, not Mac. Drawing will occur end of July and winner name will be posted here and notified in Twitter direct message</p>","tags":["cool-tools","sql-server"]},{"location":"glasswire-%28giveaway-included%29-networking-monitoring-even-a-caveman-could-use/#giveaway-result-2016-08-11","title":"Giveaway Result 2016-08-11","text":"<p>Congrats to Tim! I'll be sending you the license details. Hope you enjoy and thanks to Glasswire for sponsoring this. Give the free version a shot, even if you aren't planning on going pro. It's a great tool for anyone to increase transparency on what's really happening with their system.</p>","tags":["cool-tools","sql-server"]},{"location":"improvements-with-ssms-2016/","title":"Improvements with SSMS 2016","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2019-01-24\" &gt;}} Improved options to install through Chocolatey package. Use command <code>choco upgrade sql-server-management-studio</code> and you'll simplify the installation process greatly. Also for servers, consider Azure Data Studio as much smaller download and might provide what you need to do basic management without a length install and download. {{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"info\" title=\"Updated: 2018-03-30\" &gt;}} Use SSMS 2017 when possible now. It has continued to be improved. Current download page for SSMS 2017 If you want a shortcut to install, check out this post: Update SSMS With PS1 {{&lt; /admonition &gt;}}</p> <p>The staple of every SQL Server developer's world, SSMS has been impossible to usurp for the majority of us. However, it's also been behind the development cycle of visual studio, and didn't have continual updates. That changed recently, as I've previously posted on. SSMS (Sql Server Management Studio) now being decoupled from the SQL Server database releases.I've been enjoying some of the improvements, especially as relatest to the built in execution plan viewer. I use SQL Sentry Pro, but for a quick review, any improvements to the default viewer is a welcome addition!</p>","tags":["sql-server"]},{"location":"improvements-with-ssms-2016/#live-statistics-view","title":"Live Statistics View","text":"<p>You can see the statistics update as it's running.</p> <p></p>","tags":["sql-server"]},{"location":"improvements-with-ssms-2016/#side-by-side-comparison-of-plans","title":"Side by Side Comparison of Plans","text":"<p>This is something that is fantastic. A good step in the right direction for helping compare plans quickly. This is a feature I'd love to see added to other tools like SQL Sentry Plan Explorer.  When plans don't vary significantly in their structure, this type of view is great for quickly viewing variances.</p> <p></p>","tags":["sql-server"]},{"location":"improvements-with-ssms-2016/#usability","title":"Usability","text":"<p>You can actually drag your mouse to pan a plan... enough said. This should have been there a long time ago.</p>","tags":["sql-server"]},{"location":"improvements-with-ssms-2016/#comparison-of-properties","title":"comparison of properties","text":"<p>The properties pane also has an overhaul with some really useful comparison information, helping you identify what is now different.</p> <p></p>","tags":["sql-server"]},{"location":"improvements-with-ssms-2016/#overall","title":"overall","text":"<p>Really liking the improvements I've seen. There are a lot of things about SSMS I'd like to see improved, and with a regular release cycle the future for SSMS looks promising! I'll be really happy once the Visual Studio dark theme has made it's way over... I swear everything just runs faster with a dark theme ;-)</p>","tags":["sql-server"]},{"location":"regex-with-sql-server-sql-sharp/","title":"Regex With SQL Server - SQLSharp","text":"<p>In the context of my developer machine, I had log files I wanted to parse through. I setup a log library to output the results on a test server to a sql table instead of text files. However, this meant that my \"log viewers\" that handled regex parsing weren't in the picture at this point. I wanted to parse out some columns from a section of message text, and thought about CLR as a possible tool to help this.Ideally, I wanted to feed the results for analysis easily into power bi, and avoid the need to create code to import and parse out fields. Since I knew the regex values I wanted, I thought this would be a good chance to try out some CLR functionality for the first time with SQL Server 2016 + CLR Regex parsing. I ran across SQL# and installed. The install was very simple, just downloaded a SQL script and ran it, adding a final \"reconfigure\" statement to ensure everything was good to go.</p>","tags":["regex","sql-server","cool-tools"]},{"location":"regex-with-sql-server-sql-sharp/#sqlsharp-sql","title":"SQLSharp (SQL#)","text":"<p>I used the free version which provided great regex parsing functionality.</p> <p></p>","tags":["regex","sql-server","cool-tools"]},{"location":"regex-with-sql-server-sql-sharp/#simple-to-use","title":"Simple to use","text":"<p>Constructing the following query parsed the results easily, with no extract coding/import process required.</p> <p></p>","tags":["regex","sql-server","cool-tools"]},{"location":"regex-with-sql-server-sql-sharp/#performance","title":"Performance","text":"<p>This was just an isolated 1000 record test, so nothing exhaustive. I compared it to a table function that parsing strings (could probably be optimized more). For the purpose of running a simple log parsing search on 1000 rows it did pretty good! For better work on parsing of strings, there are detailed postings out there by Aaron Bertrand, Jeff Moden, and others. My scope was specifically focused on the benefit for a dba/developer doing adhoc-type work with Regex parsing, not splitting delimited strings. The focus of most of the articles I found was more on parsing delimited string. However, I'm linking to them so if you are researching, you can be pointed towards so much more in-depth research on a related topic.</p> <p></p>","tags":["regex","sql-server","cool-tools"]},{"location":"regex-with-sql-server-sql-sharp/#thoughts","title":"Thoughts","text":"<p>The scope of my review is not covering the proper security setup for CLR with production, CLR performance at high scale, or anything that detailed. This was primarily focused on a first look at it. As much as I love creative SQL solutions, there are certain things that fit better in code, not SQL. (heresy?) I believe Regex/advanced string parsing can often be better handled in the application, powershell, or other code with access to regex libraries. In the case of string parsing for complex patterns that are difficult to match with LIKE pattern matching, this might be a good resource to help someone write a few SQL statements to parse out some log files, adhoc ETL text manipulation, or other text querying on their machine without having to add additional work on importing and setup.</p>","tags":["regex","sql-server","cool-tools"]},{"location":"does-sp-rename-on-a-column-preserve-the-ms-description/","title":"Does sp_rename on a column preserve the ms_description?","text":"<p>Did some checking as couldn't find help in the MSDN documentation. My test on SQL 2016 shows that since the <code>column_id</code> isn't changing, the existing mapping of the description for the column is preserved.</p> <p>I know it's probably pretty obvious, but I had someone ask me, so figured proving the mapping for ms_description is maintained would be a good thing to walk through. Score another point for Microsoft, for design practices</p>","tags":["sql-server"]},{"location":"install-ready-roll-via-command-line/","title":"Install ReadyRoll via Command Line","text":"","tags":["cool-tool","redgate","sql-server"]},{"location":"install-ready-roll-via-command-line/#command-line-install-options","title":"command line install options","text":"<p>ReadyRoll has some great features, including the ability to use without cost on a build server. If you want to ease setup on multiple build servers you could create a simple command line install step against the EXE.</p>","tags":["cool-tool","redgate","sql-server"]},{"location":"install-ready-roll-via-command-line/#future-changes","title":"future changes","text":"<p>ReadyRoll was recently acquired by Redgate, so the installer options may change in the future to be more inline with the standard Redgate installer. For now, this is a way to automate an install/updates.</p>","tags":["cool-tool","redgate","sql-server"]},{"location":"install-ready-roll-via-command-line/#autoupdating-via-ketarin","title":"autoupdating via Ketarin","text":"<p>I personally use Ketarin to help me manage automatically updating apps like SQL Server Management Studio. I've uploaded a public entry for ReadyRoll to automate download and install of the latest ReadyRoll version when available. For more detail on how to use Ketarin see my earlier post on Automating SSMS Upgrades</p>","tags":["cool-tool","redgate","sql-server"]},{"location":"install-ready-roll-via-command-line/#command-line-options","title":"command line options","text":"<ol> <li>Find the path of the installer</li> <li>Run <code>ReadyRoll.msi /exenoui /qn</code> for a silent install.</li> </ol> <p> - For automated setup and install use the following code with Ketarin</p>","tags":["cool-tool","redgate","sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/","title":"SQL 2016 - Brief Overview on some new features","text":"<p>These are notes taken from the Houston SQL Pass User group from July. This presentation was given by John Cook, (Data Platform Solution Architect Microsoft) who did a great job with limited time on providing some great details on the new functionality with SQL 2016. To follow him, take a look at sqlblog.com where he posts or follow him on twitter. Thanks to him for the overview.</p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#_1","title":"SQL 2016 - Brief Overview on some new features","text":"<p>JohnPaulCook (@JohnPaulCook) on Twitter</p> <p>Microsoft Data Platform specialist and Registered Nurse</p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#_2","title":"SQL 2016 - Brief Overview on some new features","text":"<p>John Paul Cook Sql Blog (No Link)</p> <p>SQL Blog - Blogs about SQL Server, T-SQL, CLR, Service Broker, Integration Services, Reporting, Analysis Services, Business Intelligence, XML, SQL Scripts, best practices, database development, database administration, and programming</p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#cloud-first","title":"cloud first","text":"<p>Most of the new features included in 2016 have been tested in the cloud. They are implementing cloud-first with features. Therefore, most on prem features have been throughly tested in the new world, sometimes even up to months.</p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#dynamic-data-masking","title":"dynamic data masking","text":"<ol> <li>If you know a specific value you could get the results back in a specific query by putting in where clause. This would retrieve the row masked, but you still knew the results due to this \"brute force attack\". Dealing with security means you'd prevent adhoc queries anyway. You want to ensure that that scenario doesn't happen.</li> <li>This would be categorized more as obfuscation. This is not the same as encryption.</li> <li>New grant permission for UNMASK</li> </ol>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#encryption","title":"encryption","text":"<ol> <li>Encryption at column level</li> <li>Deterministic: need this for being able to search/join among different tables</li> <li>Random: Good for increasing the difficulty of breaking the encryption.</li> <li>Encryption increases to the size of the data, taking up more size</li> <li>Has some limitations on Collation. The example was COLLATE Latin_General_BIN2</li> <li>This is offloaded to the client which converts the value with ado.net 4.6.1. This means a certain compatibility would need to be maintained to use this with legacy applications. This is done on the client. Unless you give the key to the dba, they can't see the information.</li> <li>Additional connection string value is required, per SSMS has to convert and interpret this value.</li> </ol> <p><code>column encryption setting=enabled</code></p> <p></p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#stretch","title":"stretch","text":"<p>Stretch is more \"stretch table\" to Azure. This means you'd bind a function to your sql server with the logic to archive. This would let you store very cold data without having to maintain locally. Another positive to this is that each table is contained as it's own \"database\" in Azure. They maintain the backups for you, so your backup windows are not impacted. You only have to backup the local data.</p>","tags":["sql-server"]},{"location":"sql-2016-brief-overview-on-some-new-features/#temporal-database","title":"Temporal Database","text":"<p>Microsoft keeps track of all your changes in a table. You have to enable on each table individually. This functionality stores the history of all changes to ensure this history is tracked. This used to require a lot of coding.</p>","tags":["sql-server"]},{"location":"ssms-connection-color-with-sql-prompt-%26-ssms-boost/","title":"SSMS - Connection Color with SQL Prompt & SSMSBoost","text":"<p>If you haven't explored the visual color coding of tabs based on pattern matches with SQL Prompt, I'd suggest you check this out. Earlier iterations of Red Gate's SQL Prompt did not change tab color immediately when the connection was changed. Red Gate's tab color could get out of sync occasionally, so I stopped depending on it.</p> <p>Apparently this has been improved on and my testing now shows that the tab recoloring for connections is changing when the connection is updated immediately. This is a great visual indicator of what your query window is connected to.</p> <p></p> <p></p> <p>I'm still a fan of SSMSBoost combined with SQLPrompt, but I'm finding myself needing less addins as SQLPrompt has been enhanced. The ease of configuration and setup is a major factor as well. SSMSboost Preferred connections are great for when needing to quickly context switch between different connections in the same window (and sync with object explorer)</p> <p></p> <p>Resulting textbox overlay</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"ssms-connection-color-with-sql-prompt-%26-ssms-boost/#possible-improvements-for-sqlprompt","title":"possible improvements for SQLPrompt","text":"<ol> <li>I think that SQLPrompt would have some great productivity enhancements by implementing something similar to SSMSBoost preferred connections. Here is a UserVoice item on it. Add your vote</li> <li>SQLPrompt enhancements to synchronize object explorer connections based on the current query window would be another great option. I created a user voice item on this here.</li> </ol> <p>Overall, I'm finding myself depending on SQLPrompt more. As a member in the Friend of Redgate program, I've had access to try some of the new beta versions and find the team extremely responsive.</p> <p>Disclaimer: as a Friend of Redgate, I'm provided with app for usage, this doesn't impact my review process.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"ssms-connection-color-with-sql-prompt-%26-ssms-boost/#_1","title":"SSMS - Connection Color with SQL Prompt & SSMSBoost","text":"<p> SSMSBoost add-in - productivity tools for SSMS 2008 / 2012 / 2014 (Sql Server Management Studio)</p> <p>Productivity SSMS add-in packed with useful tolls: scripting, sessions, connection management. Plug-in works with SSMS 2008 and SSMS 2012 SQL Server Management Studio: SSMS add-in with useful tools for SQL Server developers</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"ssms-connection-color-with-sql-prompt-%26-ssms-boost/#_2","title":"SSMS - Connection Color with SQL Prompt & SSMSBoost","text":"<p>Code Completion And SQL Formatting In SSMS ' SQL Prompt</p> <p>Write and format SQL with SQL Prompt's IntelliSense-style code completion, customizable code formatting, snippets, and tab history for SSMS. Try it free</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"remote-desktop-workflow-improvements/","title":"Remote Desktop Workflow Improvements","text":"<p>Remote server management is a fact of life for folks involved with sql server. Since we work so often with remote machines I looked into a few tools that provided a better workflow than the default Microsoft tools. This one came out as the winner by far.</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#a-better-remote-desktop-manager","title":"A better remote desktop manager","text":"<p>First off, if you are using RDC.... why? At least move up to RDCMan, a microsoft tool that allows for much quick context switching between machines, inherited password settings for a group of servers, and more. Next, if you are using RDCMan... and perhaps you work with a lot of remote machines, perhaps there is an even better option? There are some great alternatives out there, and in my experience, with a little learning curve, the one I've worked on with Devolutions is fantastic! Link to app: Devolutions Remote Desktop Manager </p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#devolutions-remote-desktop-manager","title":"Devolutions Remote Desktop Manager","text":"<p>Disclaimer: I have been provided with a free license because I mentioned I might review the product. This doesn't impact my assessment of the tool. I'm a big fan of this app after having used it for months to get a feel for it. I've found it helpful when working with remote desktop machines, as well as some basic credential management. For folks dealing with lots of remote machines, I think this tool is well worth the investment and it has my stamp for one of my top essential tools in my cool-tools toolkit. There is a free version that is less powerful, but a still a good upgrade from the standard RDM from microsoft. The free version comparison is located HERE</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#synchronizer-some-potentially-powerful-automation-here-for-managing-connections-to-remote-machines","title":"Synchronizer... some potentially powerful automation here for managing connections to remote machines","text":"<p>Having to deal with a lot of remote machines, especially ones that change IP addresses periodically can be annoying for a DBA trying to remotely connect again. Devolutions Remote Desktop Manager (RDM) has some really cool functionality that can help automate refreshing these lists from a variety of sources.</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#synchronized-session-listing","title":"Synchronized Session Listing","text":"<p>You can setup a synchronized session listing based on csv, activedirectory, spiceworks, and more. Eventually, I believe they'll have an amazon ec2 synchronizer as well. In the meantime, with some powershell magic we can create a synchronized listing of remote machines to work with, no longer having to update ip's manually in a Amazon EC2 system.</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#output-results-from-powershell-into-csv-source","title":"Output Results from Powershell into CSV Source","text":"<p>setup a powershell script that would obtain EC2 instances and output into a csv file. I found pieces of the needed code from various sources and modified to work for me. It's not elegant, and much better ways are available I'm sure. This was helpful to me though and got the job done!</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#automatically-keeping-things-up-to-date","title":"Automatically keeping things up to date","text":"<p>Synchronizing automatically gives us the flexibility to have a scheduled script to run the powershell command to get a new list of machines, and have the synchronized list run automatically maintain the latest connection information. In my case, I setup the powershell script to run every X hours so my connection information was always up to date.</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#organized-results","title":"organized results","text":"<p>Thanks to the synchronizer, I know have two folders with separated instances for production and QA, allowing me to quickly access with minimal effort! This would allow me to set credentials as well, to reduce the effort in logging in for each of the sets of instances</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#other-cool-stuff","title":"Other Cool Stuff","text":"","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#multiple-monitor-support","title":"Multiple Monitor Support","text":"<p>Microsoft remote desktop connections have support for multiple monitors by spanning display. I tested this out with an unconventional setup. I have 3 24inch monitors, with 1 landscape in the middle surrounded by 2 in portrait mode. It had some problems with this as it's trying to create a spanned clone, however, if I had a typical setup, I think this would work fine, (such as a dual screen setup with the same orientation).</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#continual-updates","title":"Continual Updates","text":"<p>I'm continually getting updates on this product. In any inquiries on their forum, I've seen helpful responses from staff within the day, with great support help (such as powershell tips on accomplishing what I needed with synchronizers)</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#better-local-password-management","title":"Better Local Password Management","text":"<p>I'm a big fan of Lastpass, but as I use it for personal password management, I wanted to keep my work related passwords entirely separate. Devolutions RDM offers some nice password management options and credential inheritance setup.</p> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#jump","title":"Jump","text":"<p>If you have a scenario where you need to remote into one machine and then remote from that machine to another, things can get very confusing with copying/pasting/navigating. RDM solves this by having a \"RDM Jump Agent\", basically a service that allows you to set a remote desktop connection as a \"jump point\" and then connect through that connection to the next destination, while using one remote desktop window in the app. For those scenarios, I found it incredibly helpful.  Best scenario... just avoid having to jump in the first place :-)</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#handles-remote-desktop-connections-the-kitchen-sink","title":"Handles Remote Desktop Connections + The Kitchen Sink","text":"<ul> <li>Handles a breadth of different types of remote connections, such as Chrome remote desktop manager, Hyper V, remote command line, powershell sessions, Amazon S3, Amazon AWS console, Citrix ICA/HDX, and more.</li> <li>Can wrap up the trick of running SSMS (Sql Management Studio) with \"RunAs\" as a different domain and user, allowing locally run SSMS to be connected to AWS, or other environments.</li> </ul>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#other-odds-and-ends","title":"Other Odds and Ends","text":"<p>Lots of features, so I'm just covering some of the highlights that are of interest to me.</p> <ul> <li>Quickly ping and get status</li> <li>View event logs from remote machine loaded in your local event viewer</li> <li>List services</li> <li>Run powershell script remotely with RDM-Agent (executes the script as if running locally, and could do this in parallel with other instances)</li> </ul> <p></p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#summary","title":"Summary","text":"<p>I review quite a few apps, but this one is really difficult to review in detail as it covers such a range of functionality I've never even touched. The only con to the app I'd say is it can have a bit of a higher learning curve than using the plain old Remote Desktop Manager from windows due to the breadth of functionality it covers. However, once using this app, and discovering little pieces of functionality here and there, this is in my permanent \"essential tools\" toolkit.</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#free-version","title":"Free version","text":"<p>They have a free version that will suffice for many people. The pro version has more enterprise level focus, so give the free one a shot if you are looking for a basic improvement to your RDM workflow</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"remote-desktop-workflow-improvements/#_1","title":"Remote Desktop Workflow Improvements","text":"<p>Remote Desktop Manager - Remote connection and password management software</p> <p>Remote Desktop Manager is an all-in-one remote connections, passwords and credentials management platform for IT teams trusted by over 270,000 users in over 120 countries.</p>","tags":["cool-tools","sql-server","ramblings"]},{"location":"lack-for-nothing/","title":"Lack for Nothing","text":"<p>The message of this song is one that that has been driving itself home as more and more important to me. We are not viewed in our frailty as we come before God to worship Him. We are not judged and condemned. He doesn't look and tell us to come back when we are in a better place. Instead, just as the author of Hebrews writes (Heb 10), we can come with bold confidence, knowing that the way has been made for us in Christ, with us being viewed through the \"curtain\" of Christ. This is critical to every person wanting to draw closer to God, as without a realization that we should be able to approach with confidence and no judgement, we will hold back from fully diving into our relationship with Him.</p> <p>Hebrews 10:19-22   We have, then, my friends, complete freedom to go into the Most Holy Place by means of the death of Jesus. He opened for us a new way, a living way, through the curtain---that is, through his own body. We have a great priest in charge of the house of God. So let us come near to God with a sincere heart and a sure faith, with hearts that have been purified...   This has shaped my paradigm a lot. It's hard for me to hear about others feeling unworthy in coming before God, like they have to get things together before coming to Him. This is so antithetical to the nature of God and to all of what we see Christ exemplifying in His life. Christ never demonstrated superiority, judgement against those who don't demonstrate perfection. When He did bring out anger and judgement, it was focused on those who, in their misguided concepts of God, put barriers between God and His people.   So, just a friendly reminder that you \"Lack for Nothing\". Cheers! </p> <p>disclaimer</p> <p>This is my first full production attempt with Ableton. I used Ableton Live Lite, with EzDrummer 2 to help generate some drums. Due to running out of time (and patience :-) I just wrapped this up instead of worrying about fine tuning every little piece. I've got some errant vocal parts, guitar parts, and all, but hope you can just look past this. I ran out of time and .... and patience to keep reworking.   I actually am working on a series to help fellow worship leaders and musicians just getting into recording learn more about utilizing VSTs, DAW's (like Ableton, Studio One 3), and more. It's bit confusing getting started, so hopefully as I share a little of my journey it might save you some work if you are wanting to try your hand at some creativity.</p>","tags":["music","ramblings","worship","follower-of-Jesus"]},{"location":"ableton-live-%26-lemur-setup-%28from-a-windows-user%29/","title":"Ableton Live & Lemur Setup (From a Windows User)","text":"<p>Had a chance to look at this program thanks to the generosity of the developer. They had a promotion that ended prematurely and they sent me a license as a goodwill gesture. Pretty fantastic service, and thanks to them for this.The documentation was a little sparse for the Windows setup, so I ran into some complications getting it to work.</p> <p>I have used TouchOSC, and had some difficulty with configuring it for windows. True to other's postings, Lemur didn't make this easier. Especially as a windows user, this is not a simple plug and play and go type of tool. If you are looking for that then you'd be better served by looking a Touchable, Conductr, or something similar that provides the full package. The difference is that Lemur offers a complete \"development\" environment for templates and automation. The sky is the limit. You can even have a single ipad controlling multiple computers based on the midi being mapped on one controller to various destinations. To get Lemur talking to Ableton Live (9.6), this is pretty much what worked for me after experimenting.</p> <ol> <li>Install Lemur</li> <li>Install Live Control 2 (basically just a set of template and automation scripts)</li> <li>Copy Live Control Template onto ipad (you can drag the file directly to the app in Itunes and hit sync)</li> <li>Install LoopBe. Note I tried rptMidi as well as LoopMidi, and this is the only one that worked without issue for me on Windows 10.</li> <li>Setup Lemur Daemon service as shown in screenshot. </li> <li>In ableton I setup as the following: </li> <li>If you drag a drum track into a channel and activate for recording, you can tap in the \"play\" tab and change \"key\" to \"drum\". Then you can find the appropriate drum machine field by scrolling up or down. This got the initial setup going so I was able to drop a drum track in, and tap out a rhythm. I think I'm going to have invest more time into it to get what I want, so the jury is out for me as a casual user whether or not I'll be able to leverage this vs TouchOSC or touchable. It's pretty powerful, just pretty complex! As a person primarily focused with Live for DAW and home recording/experimentation, I'm probably not going to leverage fully at this time like others would. If I gain some more traction with it, I'll try and post some more useful updates. Both TouchOSC and Lemur are designed for customization. If you want to get going with limited setup, look at Touchable, Conductr, or another similar to that.</li> </ol>","tags":["music","ramblings","cool-tools"]},{"location":"setting-dbcc-1222-on-startup/","title":"Setting DBCC 1222 on startup","text":"<p>The following command is run to gain details on deadlocks.</p> <pre><code>DBCC TRACEON (1222,-1)\n</code></pre> <p>However, once the SQL instance is restarted this flag is set back to disabled.</p> <p> To enable it on the instance upon startup:</p> <ol> <li>Open SQL Configuration Manager</li> <li>Services &gt; Sql Service Instance &gt; Properties &gt; Startup Parameters</li> <li>Add the following statement: <code>-T1222</code></li> <li>Confirm the change by navigating to Advanced &gt; Startup Parameters. This should be grayed out and display the new value that was added at the end with a delimited semicolon.</li> </ol> <p></p>","tags":["sql-configuration","sql-server"]},{"location":"daw-dive-1-getting-started-studio-one-3/","title":"DAW Dive 1 - Getting Started - Studio One 3","text":"<p>I'm a geek, software nerd, and a musician. As I've been stagnating lately in my musical progression I decided to dive into trying to produce some recordings of some of my music, hoping to spice up the old brain.I have a lot to learn. It's pretty humbling, but despite being a musician and a developer with some decent tinkering ability, I've found getting into recording and production a bit challenging on certain things. If you come from a background in which you understand terms such as pre/post, bus, gain structure, compression ration, and other terms then your learning curve will be much easier. For me, my extent of dealing with these terms was focused on guitar pedal settings and an analogue mixing board. This has made things much tougher, as I've been learning terminology along with methodology. I've also been pretty astonished with some of the technology solutions I had never explored that allow a much richer musical creation for a musician. I figured I'd share my progress as I work through some of these things in case it helps some others on the road. This is not attempting to be a profound treatise, full step by step walkthrough, but more focusing on sharing some of the problems as well as things I've discovered. Maybe you'll save some time, or get some new ideas if this is a new venture for you as well!</p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#presonus-studio-one-3-professional","title":"Presonus Studio One 3 Professional","text":"<p>This is probably going to be my primary DAW. My initial impressions are extremely positive. The only competitor in my current focus is Ableton since I enjoy live looping, and the session/arrangement integration. However, for pure DAW focus, I'm not sure I'd be as comfortable long-term with Ableton since I'm not primarily an electronic loop/sample based writer. The details I'll be posting are on the Professional Edition, so some of the functionality might not be available in the other versions. See their version comparison for exact details.  Disclaimer: Presonus generously provided a license for my evaluation. However, please note that I am not biased in my review, as I believe in providing unfiltered feedback.</p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#getting-content-installed","title":"Getting Content Installed","text":"<p>I ran into some serious download issues when using the Presonus content installer. In reading forum posts, I see that this has occasionally cropped up. If this happens to you, run just go to the presonus account page and download directly with your internet browser, as it ran much faster for me. This was the only major hitch in my install. Maybe a better CDN is in order?</p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#first-impressions","title":"first impressions","text":"<p>So far, very intuitive interface. I'm not a DAW expert by any means, so I've been looking for something that had a natural feel to someone familiar with the basics of recording, yet more powerful functionality that will help me create and arrangement more complex pieces. So far, it feels like Presonus Studio One 3 is living up to the reputation I've found online for being getting out of the way and enabling creative workflow. I haven't even dived into the scratchpad functionality, but even my first project has been a very pleasant experience, with none of the \"stumped\" moments I had in getting this to work with Ableton Live (though I still prefer this for live looping right now).</p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#novation-launchkey-49-setup","title":"Novation Launchkey 49 Setup","text":"<p>Setting up a midi device has been a learning experience. Working with Studio One required a bit of experimentation, as it wasn't preset, but I finally got it to work. It would be nice if they offered more presets, but at least I got it working! Not all the controls seemed to be mapped, but Launchkey has \"InControl\" functionality, so I plan on evaluating this soon. To find the midi port a device is working on with Windows, go to device manager, and you can see the midi port displayed so you know what to configure.</p> <p></p> <p></p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#clean-interface","title":"Clean interface","text":"<p>The interface is clean, designed to be a single screen view without a lot of navigation required. The context menu options are kept organized and clean. For me, Reaper was a great DAW, had a lot of functionality for the price, but was very context menu driven and this was a lot to get used to. I think Presonus has done an admirable job of organizing the many options to keep things from getting too messy.</p> <p> One thing I liked was the focus on \"songs\", as it offered the creation of a new \"song\" instead of \"Files/projects\". This felt like such a great design decision as it focused on the creation aspect of new songs.</p> <p></p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#presence-xt-and-virtual-instruments","title":"Presence XT and Virtual Instruments","text":"<p>A package of instruments are included in Studio One. I enjoyed the Cello and Pizzicato string sections in particular. I'll probably post more on this later as I discuss Melodyne integration.</p> <p></p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#vst-configuration","title":"VST configuration","text":"<p>Nice and easy to find. No problems here. Was happy to see it supports 32, 64, VST3 and more. Ableton Lite didn't support 32bit, so I had a lot of VST's downloaded that I couldn't use.</p> <p></p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#other-stuff-for-the-future","title":"other stuff for the future","text":"<p>I plan on discussing the Arrangement functionality as it helps to bridge the gap of organizing complex tracks, as well as scratchpad, which was the primary function I was excited to try, as it seems a win for someone experimenting with various versions. I've plan on showing how to convert guitar riffs into a cello, quantizing notes for more precision on a fast run, folding/unfolding multiple takes, and more. It's crazy how much is packed in here, but I'm completely loving it.</p>","tags":["music","ramblings","cool-tools"]},{"location":"daw-dive-1-getting-started-studio-one-3/#any-cons","title":"any cons?","text":"<p>So far, the biggest thing I'm missing is a drum generator such as EzDrummer 2. I had planned on doing a review of this, but my trial ran out and I haven't heard from them if they do any licenses for press reviews. Addictive drummer seems ok, but the trial is so limited, I'm really not able to leverage, and the single few tracks I was able to try didn't hit what I needed (demo removes all cymbals). EzDrummer 2 was pretty darn good in helping me create some believable tracks. Superior Drummer also seems promising but I couldn't find any trial for it. If Presonus had something equivalent to EzDrummer 2 to generate believable drum tracks and rhythms based on building the drums that would be awesome. As it is, it seems to focus on some drum samples and loops, but not a full built engine like EzDrummer 2 offers. If I end up hearing back from Toontracks on their products and obtain copies, I'll be sure to post up more details at that point for the aspiring non drumming arrangers among us.</p>","tags":["music","ramblings","cool-tools"]},{"location":"sql-2016-configuration-manager-not-showing-in-start-menu/","title":"SQL 2016 - Configuration Manager Not Showing in Start Menu","text":"<p>{{&lt; admonition type=\"info\" title=\"update\" &gt;}} 2020-04-29 broken image links removed {{&lt; /admonition &gt;}}</p> <p>Didn't see SQL 2016 Configuration manager in the start menu. Ran a quick search to see if this was a common issue and found an article: Quick Trick Where is SQL Server for SQL 2012I looked and found the SQL Configuration Manager for 2016 in the same location: <code>C:\\Windows\\System32\\SQLServerManager13.msc</code></p> <p>As I'm running windows 10, the location for the start menu entries were located here: <code>C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Microsoft SQL Server 2016</code></p> <p>Create a shortcut for SQLServerManager13.msc in the start menu folder and you'll be good to go!</p> <p>Thanks to @marnixwolf for providing that previous walkthrough that helped me resolve this so quickly.</p>","tags":["sql-server"]},{"location":"fixing-non-deterministic-error-when-creating-indexed-view/","title":"Fixing non-deterministic error when creating indexed view","text":"<p>I discovered a bit of info on working with float values, while creating a hash value that contained a float value, and a date value.</p> <pre><code>create unique clustered index ix_clustered_ViewK_Catfood_K\nMsg 1901, Level 16, State 1, Line 1517\nCannot create index or statistics 'ix_clustered_ViewK_Catfood_K' on view 'compareCatfood' because key column 'ViewK' is imprecise, computed and not persisted. Consider removing reference to column in view index or statistics key or changing column to be precise. If column is computed in base table consider marking it PERSISTED there.\n</code></pre> <p>And...</p> <pre><code>Msg 2729, Level 16, State 1, Line 38\nColumn 'Hash' in view 'compare.Catfood_test' cannot be used in an index or statistics or as a partition key because it is non-deterministic.\n</code></pre> <p>Stack Overflow to the rescue... The issue is with float values. http://stackoverflow.com/a/19915032/68698</p> <p>Even if an expression is deterministic, if it contains float expressions, the exact result may depend on the processor architecture or version of microcode. To ensure data integrity, such expressions can participate only as non-key columns of indexed views. Deterministic expressions that do not contain float expressions are called precise. Only precise deterministic expressions can participate in key columns and in WHERE or GROUP BY clauses of indexed views. MSDN Restrictions also apply to formatting dates when you are calculating a checksum. This is because every region has variations on the way the date may be displayed. This makes dates non-deterministic in a hash, unless the convert format is explicitly defined.</p>","tags":["sql-server"]},{"location":"fixing-non-deterministic-error-when-creating-indexed-view/#ensuring-date-is-converted-with-style","title":"ensuring date is converted with style","text":"<pre><code>+ isnull(convert(nvarchar(max), do oe.Somedate ,102), ''') + N'''\n+ isnull(convert(nvarchar(max),la.SomeDater ,102), N''') + N'''\n+ isnull(cast  (ft.ToBeOrNotToBe as nvarchar(max)),''') + N'''\n+ isnull(cast  (t2.Fooey as nvarchar(max)),''') + N'''\n</code></pre> <p>If you can resolve these issues then you are on your way to resolving the other thousand restrictions on indexed views.... :-)  Good luck!</p>","tags":["sql-server"]},{"location":"powershell-ise-updating-theme/","title":"Powershell ISE: Updating Theme","text":"<p>For all the dark theme aficionados, or those who just want a better theme than the default, here's a quick set of directions to update your ISE.1. Go to download a theme from Github &gt; PowerShell_ISE_Themes 2. Unzip 3. Go to ISE &gt; Tools &gt; Options &gt; Colors &amp; Fonts &gt; Manage Themes 4. Import selected theme 5. For consistency, adjust the <code>background</code> and <code>forecolor</code> of the <code>console pane</code> as well as the <code>text background</code> to match if you want to. In my case I took the RGB values from the theme on the <code>script pane background</code> and applied to the <code>console pane</code> below it. 6. If you want the background for the error, warning, and other output streams to match, update the RGB background as well.</p> <p> Your eyes will now thank you, especially if you are jumping from Visual Studio dark theme to ISE with it's previously glaring white screen.</p> <p></p>","tags":["development","powershell","tech","cool-tools"]},{"location":"syncovery-%26-arq-syncing-%26-backup/","title":"Syncovery &amp; Arq 5I've tried a lot of file sync/backup utilities.","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>The primary definition to get right is that there are two main functions people try to achieve with this type of software.</p> <ul> <li>File Syncing: Syncing Files Between Cloud and Local</li> <li>File Backup: Preserving Files, sometimes with versioning, in order to protect against loss. These two approaches require a diffferent solution many times, as focusing on file syncing means you are more at risk, as handing conflicting sync scenarios might cause loss. However, file backup doesn't give you flexibility to grab files to sync to another machine in many cases (for example CrashPlan does a great job of being quiet and backing things up, however, it is not designed for syncing, rather a single machine archive).</li> </ul>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#syncovery","title":"Syncovery","text":"<p>Disclaimer: They provided a license for me to evaluate and provide feedback. This doesn't bias me, as I just love finding great software!</p> <p>I actually obtained Syncovery  back at the beginning of the year and so have had quite a bit of time to utilize.</p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#tldr","title":"TL;DR","text":"<p>Pros</p> <ol> <li>It's a powerhouse of customization, which provides an incredibly customizable set of options</li> <li>Typically well documented and easy for a power user to figure out the options</li> <li>It can solve backups and file syncing inside the same app. Cons</li> <li>It's a powerhouse of customization. This isn't something I'd recommend for a non-technical user.</li> <li>I ran into errors syncing with Amazon Cloud Drive with deletes. Wasn't able to figure that piece out completely, but for the most part everything ran smoothly. So, would I recommend? If you are looking to solve some file syncing options between multiple systems, as well as backup files/folders and are willing to deal with tweaking it to get it just like you want, it's awesome. If you want something like plug and play, then you need to look at Arq instead. Arq provides an incredibly simply alternative for those focused on backup, and not on file syncing.</li> </ol>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#profile-overview","title":"Profile Overview","text":"<p>This provides an overview of all activity. Even though it's not necessarily a styled gui, and has a lot of detail, I think it's well designed for the information it's providing. Having tried some other apps I think I found the majority of what I needed pretty quickly here. I had a huge backup to Amazon Cloud Drive of my entire lightroom catalog (600-800GB) and Syncovery handled the majority of this backup with no issues. I have run into some issues as you can see.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#profile-settings","title":"Profile Settings","text":"<p>I won't go into every option, read their documentation for the full details. At a high level, some of the powerful options I appreciated where the exact mirror vs smart tracking. Smart Tracking looks try and resolve the conflicts that can happen when syncing on several machines by choosing which version wins.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#running-backup-in-attended-mode","title":"Running backup in attended mode","text":"<p>Shows the current progress. I have used some backup apps that froze when running large backups in the past. So far, I've had good experinces with Syncovery's stability.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#detailed-logs","title":"Detailed Logs","text":"<p>The app provides the output via powershell console, or in your native editor. Since I prefer Sublime Text 3, this was perfectly fine with me. Nice detailed logs give me a chance to figure out any issues.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#a-few-other-thoughts","title":"A few other thoughts","text":"<p>Again, the options are too massive to cover them all. However, a few stood out to me.</p> <ul> <li>Versioning deletes: Could have deleted files archived into a relative root folder, or a main archive folder, and then removed after a certain period. This provides a safety net for deletes to be reviewed.</li> <li>Safety Checks: Deletes or overwrites over a certain percentage of the files will require manual run. This ensures something accidental doesn't cause a mass deletion of files in your cloud storage.</li> <li>Zip versioning. If you want, you can version your backups with zipped contents</li> <li>Remote agent: If you are using another computer and syncing between them, you can setup the destination to have an agent so a zipped copy could be unzipped locally on the system by the agent, or file scans could be run locally by this agent instead of a remote agent having to do all the work.</li> <li>Run as a Service: I enabled to run as a service to ensure this always was running in the background</li> <li>Change Detection: You can setup near real-time file sync based on monitoring a folder. With additional customization, you can tell it to batch up the changes if over a certain number are detected and do them in a batch run instead.</li> </ul>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#arq-5","title":"Arq 5","text":"<p>Disclaimer: Arq also provided me with a license to evaluate. Arq is on the app list for Amazon Cloud Drive supported applications.</p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#arq-approaches-things-differently","title":"Arq approaches things differently","text":"<p>This tool is focused on file backup, so the options are going to be much different in focus. However, the approach reminds me a lot of the \"Apple\" approach with simplifying things.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#filtering-backup-selection","title":"Filtering Backup Selection","text":"<p>This is pretty straight forward. However, I was happy to see the folder filter options actually provide Regex matching as well for power users.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#advanced-options","title":"Advanced Options","text":"<p>Again, the options are much more limited... and less daunting</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#arq-approaches-as-encrypted-backup","title":"Arq approaches as encrypted backup","text":"<p>One big different to note is that if you are focused on backing up files like photography/video, then you probably want the cloud drive to have those files in their native format, to ensure they are usable to view from the cloud drive. Arq approaches things from a different standpoint. Your cloud drive will have encrypted blocks that this app can download and interpret. For privacy, this is fantastic. For media not so much. You have to decide if you want everything encrypted or \"open\".</p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#windows-user","title":"Windows User","text":"<p>This probably is just me, but I've had some issues with Arq 5 on Windows 10. They could probably use some better error messages, as this error detail wasn't very user friendly. When I reached out to support in the past, I got an answer in 2 days, so their support has been responsive.</p> <p></p>","tags":["cool-tools","ramblings"]},{"location":"syncovery-%26-arq-syncing-%26-backup/#what-would-i-recommend","title":"What Would I Recommend?","text":"<p>For the power user wanting to implement file sync and backup in a single utility, Syncovery all the way. For anyone looking to do pure backups, with no configuration or tweaking, and ok with it being completely encrypted, then Arq. They both have different focuses. For me, I've migrated to a hybrid approach. For personal code snippets I use Gists, as I can version control them. For media and settings I use Syncovery because I like the customization options. If I was focusing on something like CrashPlan for simplicity and simple configuration, I'd probably go with Arq for that. One last one that I hope eventually is supported by Amazon Cloud Drive is Stablebits Cloud Drive. It has a lot of promise, but my tests a while back had it peforming really slow (not due to them, but due to Amazon's throttling) Another similar to that was ExpanDrive. I wasn't able to contact them for a license to evaluate in my review, but my short trial seemed promising, as it tries to add the cloud provide as a drive, allowing you to manage in explorer... or Xyplorer (yes I still use it!)</p>","tags":["cool-tools","ramblings"]},{"location":"migrating-database-collation-the-red-gate-way/","title":"Migrating Database Collation - The Red Gate Way","text":"<p>I had some cross database comparisons that I wanted to simplify, but ensuring the collation matched. The amount of objects that I would have had to drop and recreate was a bit daunting, so I looked for a way to migrate the database to a different collation.Using the Red Gate toolkit, I was able to achieve this pretty quickly. There are other methods with copying data built in to SSMS that could do some of these steps, but the seamless approach was really nice with the SQL Toolbelt.</p> <ol> <li>First I created the database with the collation I wanted to match using SQL Compare 12.</li> <li>I deployed the original schema to the new location.</li> <li>Ran SQL Data Compare 12 and migrated all the data to the new database. Since the new database was created with the desired migration, I was good to go!</li> </ol> <p>Note: I'm a member of Friends of Redgate program, and am provided with licenses for testing and feedback. This doesn't impact my assessments, as I just love finding good tools for development, regardless of who makes them!</p>","tags":["redgate","sql-server","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/","title":"DAW Dive 02 - BFD3 - Drumming for the Rest of Us","text":"<p>Creating tracks at home can be very intensive in time, so I'm always looking for a better way to bring a larger sound to a track with less effort. One of the big gaps for me has been the drum parts. I've evaluated a few options, and finally dived into BFD3 for this latest project that is a post-rock style track. This was my first time diving into BFD3 for a full song, so I had a bit to learn. Disclaimer: BFD3 generously provided me with a copy to evaluate. This doesn't affect my reviews, as I just love dig in and recommend good software!</p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#bfd3-drumming-plugin-extraordinaire","title":"BFD3 - Drumming Plugin Extraordinaire","text":"<p>I was using this in Presonus Studio One. I created a drum track and then started to explore. I started my exploration based on using the Groove Editor, as I wanted to benefit from the library of preset grooves that were already created, and use these with some modifications to be the drum parts for me song, allowing me to hopefully create the track I wanted quickly.  There is a lot of functionality I'm not going to dive into, such as the vast array of microphone adjustments, compression options, and other mixing options. My focus is going to be as a songwriter, how could I use this to help me generate a believable drum track for my song without it being an ordeal.</p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#modifying-existing-kit","title":"Modifying Existing Kit","text":"<p>One thing that was really easy to do with BFD3 was modify the kit configuration. I liked the sound of Pork Pie Kick for this song, so I just played a groove, and then switched it over with the checkbox (on hover).</p> <p></p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#grooves","title":"Grooves","text":"<p>The Grooves section provides a preset set of associated rhythms to allow you to easily prepare a song based on related rhythms. This is ideal for helping you create a song quickly by finding associated rhythms.</p> <p></p> <p></p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#copy-and-replicate-a-groove-to-tweak","title":"Copy and Replicate a Groove to Tweak","text":"","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#change-velocity-for-realism","title":"Change Velocity for Realism","text":"<p>This is really important to a natural sounding drum part. You definitely do not want everything at the same velocity as no real drummer would do this. There are some easy ways to adjust the velocity as well as the humanization of the rhythm in both Presonus Studio One 3 and BFD3 plugin. In this example, I adjust it in BFD3 directly.</p> <p></p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#humanization-of-rhythm","title":"Humanization of Rhythm","text":"","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#practice-your-rudiments","title":"Practice Your Rudiments","text":"<p>One other cool feature for quickly pumping out some grooves is the ability to \"paint\" the rudiment you've selected. This means you could easily pick some drag paradiddles on the snare without having to click each point in time. I found this incredible helpful for creating some interesting grooves.</p> <p></p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#quick-note-on-versions","title":"Quick Note on Versions","text":"<p>I tried the BFD3 Eco, but due to wanting flexibility to edit the grooves and do more tweaks, the Eco version was not a good fit for for me. If, however, you are looking for an experience that is more groove oriented, with less tweaking/adjustments, then you should consider starting with the Lite version. If you are looking for the full flexibility then you might want to evaluate the full one instead. FXpansion indicates on their support page in the comparison on the differences that:</p> <p>What are the differences between BFD Eco and BFD2/BFD3? BFD Eco is optimized for ease of use and fast results - it is deep enough for deeply sculpting drum sounds into all manner of shapes but is streamlined enough so that you won't be overwhelmed with options. It has less detailed sounds than BFD2 and BFD3 but is much more light on system resources. BFD2 and BFD3 contain far more control over each aspect of each kit-piece as well as a configurable mixing engine with custom aux channels and much more. They also feature full editing of Grooves, more control over exports and many other features too numerous to list. FXpansion Support Article</p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"daw-dive-02-bfd3-drumming-for-the-rest-of-us/#final-thoughts","title":"Final Thoughts","text":"<p>The power and flexibility of this software is pretty amazing. I've only touched on a small fraction of what it is supposed to be capable of, as I'm using a lot of the simple functionality to \"paint\" some rhythms. I was a little disappointed in the initial groove library as far as the single 6/8 groove test goes, as it felt more difficult to get a groove I liked than when I demoed Ez Drummer</p> <p>However, the flexibility in editing with the easy humanization and editing of the kit made up for this. I think the vast sound library of plugin packs is what is designed to expand this, so I'll probably cover some of those later to evaluate if they fill in the gap on providing a even larger variety of preset grooves for the wannabe composer. Note that the library is around 40GB for the initial load, so my isolated 6/8 groove test isn't reflective of the rest of the available library. There is a lot to work with, and the expansion libraries seem to be pretty vast in number.</p> <p>Another area that would be awesome to enhance is providing something similar to \"song finder\" that EzDrummer 2 had, where a certain rhythm was tapped, and related matches found. EzDrummer 2 then provides something similar to Grooves where the song structures for verse, chorus, bridge, etc are laid out. I'm not partial to the format that EzDrummer used, I actually prefer the Groove library format of BFD3, but I sure would love seeing the search capability added to help quickly match grooves fitting the feel you are trying for. I'm pretty happy with the documentation and support, as they have a wide range of well made videos showing demonstrations on the product, how to use, etc. I look forward to incorporating this into my workflow and going through some more training videos to better understand a solid workflow, as I'm just starting to wrap my head around it, along with learning how to navigate Presonus Studio One</p> <p>Check out a trial if you want to give it a shot and post some feedback below on what you think. Remember, I'm not an affiliate, or getting paid for anything on this, I just like reviewing and referring folks to good software. Good luck!</p>","tags":["daw","music","ramblings","cool-tools"]},{"location":"centralized-management-server-101/","title":"Centralized Management Server 101","text":"<p>I've used Central Management Server registered servers in SSMS for primarily one purpose, saving connections. :-)  This is definitely not the intended usage. As I've explored the benefits of using this a little more, I put a few notes together to help share the concepts I came across. I was brand new to this feature in SQL server, and found some of the functionality pretty powerful, especially if you are in an environment that has a lot of servers to manage and ensure consistent configuration among all of them.</p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#moving-cms-entries-to-a-new-cms-server","title":"Moving CMS Entries to a new CMS server","text":"<p>If you need to move your Central Management Server (CMS) entries to a new CMS, then use the export and import functionality located under the CMS &gt; Tasks context menu.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#run-queries-across-multiple-instances-at-once","title":"Run queries across multiple instances at once","text":"<p>Right click on the CMS group and choose new query. This tab will now execute the same query in parallel across all the selected instances, allowing quick adjustments. One creative use of this is to register two databases in the server group, then click new query. They could be on the same server if you wish. Once you start a new query on this group you could run the same query on two separate databases with no extra effort. An alternative to using this is Red Gate's SQL Multiscript which offers a bit more customization in the behavior and project file saving for multi-server, multi-database query running. You can identify a multiserver query at the bottom identified by</p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#highway-to-the-danger-zone","title":"Highway to the danger zone","text":"<p>It's easy to forget you are running a server group query. Use some type of visualization and don't leave the query window open longer than you need to, especially in a production environment. One hint can be setting up the Red Gate tab color if you have SQL Prompt. You can see the connection details on the tab are a little different, listed with the CMS server group name + database, such as the image below</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#create-a-policy","title":"Create a policy","text":"<p>In reviewing technet article on Policy-Based Management </p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#administration-functionality","title":"Administration Functionality","text":"<p>From the CSM context menu you can perform some nice functionality such as start, start, restart of SQL services, view error logs, and even pull up the SQL configuration manager for that instance! Take advantage of this to easily adjust settings across instances without having to log into remote machines.</p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#policies","title":"Policies","text":"<p>There are a few different types of policy behaviors to know about. From MSDN article Administer Servers by Using Policy-Based Management I found that there were a few ways the evaluation of a policy is handled.</p> <ol> <li>On Demand</li> <li>On Change: Prevent</li> <li>On Change: Log Only</li> <li>On Schedule One interesting comment from MSDN indicating that: \"IMPORTANT! If the nested triggers server configuration option is disabled, On change: prevent will not work correctly. Policy-Based Management relies on DDL triggers to detect and roll back DDL operations that do not comply with policies that use this evaluation mode. Removing the Policy-Based Management DDL triggers or disabling nest triggers, will cause this evaluation mode to fail or perform unexpectedly.\"</li> </ol>","tags":["sql-server"]},{"location":"centralized-management-server-101/#create-policy","title":"Create Policy","text":"<p>This suprised me a little. The policy functionality wasn't available in the CMS registered server tab. Instead, go to the server in Object Explorer and expand Management &gt; Policy Management &gt; Policies</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#creation-dialogue","title":"creation dialogue","text":"<p>Add new condition, there is a large list of policies to evaluate. You can detailed information on them on MSDN here.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#configure-the-rules","title":"Configure the rules","text":"<p>You'll see a huge list of Facets to evaluate and then you can easily setup logic to evaluate this.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#description-details-on-policy","title":"Description Details on Policy","text":"<p>In this case, I linked back to my favorite resource for server configuration... the Ozar! Providing some detail back on this could be great for quickly providing details later back to someone reviewing the results.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#all-your-hard-work","title":"All your hard work","text":"<p>For all this hard work, you'll get two fancy new icons in object explorer. With this work, I'm thinking saving your policies for backup with scripts would be a great idea.... scripting this would be much faster than all these steps to check one setting. I wish the dialogue had been focused on setting up multiple conditions quickly instead of all that work for a single Fact to be evaluated.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#evaluate-policy-results","title":"Evaluate Policy Results","text":"","tags":["sql-server"]},{"location":"centralized-management-server-101/#start-the-evaluation","title":"Start the evaluation","text":"","tags":["sql-server"]},{"location":"centralized-management-server-101/#results-were-not-what-i-expected","title":"Results were not what I expected","text":"<p>My first run gave me a failure, despite to my eyes being the right match. I had to change my approach from being @FillFactor != 0 to being Policy should be @FillFactor = 0 and it would pass, else it would fall. I was thinking in reverse. MSDN indicates to be aware that: IMPORTANT! The functions that you can use to create Policy-Based Management conditions do not always use Transact-SQL syntax. Make sure that you follow the example syntax. For example, when you use the DateAdd or DatePart functions, you must enclose the datepart argument in single quotes.</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#prebuilt-best-practice-rules","title":"Prebuilt Best Practice Rules","text":"<p>Thankfully, I found that there were a lot policies already presetup by Microsoft. The default location I had for the 2016 installation was C:\\Program Files (x86)\\Microsoft SQL Server\\130\\Tools\\Policies\\DatabaseEngine\\1033 . You can navigate to these by right clicking on the Server Group you want to evaluate, and then  Evaluate Policies &gt; Choose Source &gt; Files &gt; SQL Server Best Practices folder &gt; Database Engine &gt; 1033 &gt; Rule to explore</p> <p></p>","tags":["sql-server"]},{"location":"centralized-management-server-101/#some-final-thoughts","title":"Some final thoughts","text":"<p>I can see the value for enforcing policies across a vast number of servers, and monitoring for compliance. For my environment, primarily dealing with a lot of developer sandboxes, the effort this requires is a bit too much. For my scenario, I'll probably stick with some home grown queries, powershell SMO checks, and the awesome OmniCompare tool that is getting better and better each iteration. A previous article I wrote discussed the functionality of this tool here: OmniCompare: A Free Tool to Compare SQL Server Instances</p>","tags":["sql-server"]},{"location":"cannot-generate-sspi-context/","title":"Cannot Generate SSPI Context","text":"","tags":["sql-server","mysteries"]},{"location":"cannot-generate-sspi-context/#troubleshooting","title":"Troubleshooting","text":"<p>I ran into an error: The target principal name is incorrect.  Cannot generate SSPI context. (Microsoft SQL Server, Error: 0)</p> <p>I evaluated the sql server configuration manager protocols for sql server and saw that named pipes was disabled. I tried ensuring that this wasn't causing the issue, but enabling but it didn't fix. Thankfully, Andrew on StackOverflow had the answer here:</p> <p>First thing you should do is go into the logs (Management\\SQL Server Logs) and see if SQL Server successfully registered the Service Principal Name (SPN). If you see some sort of error (The SQL Server Network Interface library could not register the Service Principal Name (SPN) for the SQL Server service) then you know where to start. We saw this happen when we changed the account SQL Server was running under. Resetting it to Local System Account solved the problem. Microsoft also has a guide on manually configuring the SPN. Andrew 3/19/2014</p> <p>When I went into the configuration manager I changed the format from the DOMAIN\\USER to searching with advanced and matching the user. The username was applied as USER@DOMAIN.COM instead. When I applied, and restarted the sql service, this still didn't fix.</p> <p>I read some help documentation on this on smatskas.com but it didn't resolve my issue as I had the correct permissions, and I verified no duplicate SPN by running the command setspn -x I ran gupdate /force to ensure was properly in sync with the policies and it did get the time updated. However, the problem persisted. I went back to checking for a specific conflict by running</p> <p>Still no luck....</p> <p>Finally, I switched the account to use LocalSystem (this was in a dev environment) following the directions by Bob Sullentrup  and this allowed it to successfully register the SPN.</p> <p>I'll update my blog post when I have a better understanding on exactly why this occurs, but for now, at least I was able to proceed.</p>","tags":["sql-server","mysteries"]},{"location":"easy-sql-maintenance-with-minionware/","title":"Easy SQL Maintenance with Minionware","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated 2020-07-06\" &gt;}} After a great chat with Sean today (creator), I did see some pretty cool benefits that for those looking for more scalability, will find pretty interesting.</p> <ul> <li>Backup tuning: based on the size or specific database, use striped backups to enhance performance of backup jobs</li> <li>Enterprise edition, centrally manage and report on all backups across all servers.</li> <li>Override defaults by just including additional sql files in the InitialLoad configuration. Review the docs for the specifics. This means no need to loop and override like I did below now. Just deploy and your final steps can be setting up your default configuration options.</li> </ul> <p>Overall, great conversation and found out some really cool things about postcode commands that could be PowerShell driven. Definitely worth a further look if you want an alternative to the commonly used Ola Hallengren solution, and especially if you are wanting more table driven configuration options over the need to customize the commands in the agent steps. {{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"info\" title=\"Updated 2017-01-25\" &gt;}} While I think the minionware solution is pretty awesome, I think it takes more work for the value, and can be a bit confusing to correctly setup, vs the Ola Hallengren solution, esp since you can install this quickly with dbatools now. I'd lean towards Ola Hallengren for simple implementations, and consider MinionWare's option if you are looking at their flexibility in the table based configuration. The learning curve seems higher to me, but more for those looking to tweak options a lot. Both are great solutions, just be aware MinionWare will require a little more digging to leverage it fully. {{&lt; /admonition &gt;}}</p> <p>Here's my personal tweaked settings for deploying Minionware's fantastic Reindex &amp; Backup jobs. In the development environment, I wanted to have some scheduled jobs running to provide a safety net, as well ensure updated statistics, but there were a few default settings I wanted to adjust. In particular, I tweaked the default fill factor back to 0/100. I also installed all the objects to a new \"minion\" database instead of in master, as I'm beginning to be a fan of isolating these type of maintenance jobs with logging to their own isolated database to easy portability. I also adjusted the default retain days on backups to 30.</p> <p></p> <p>You can use this template as a guide to help you adjust the default backup settings to fit your environment a little better. There has been various forms of discussion on the adjustments of Fill Factor for example on the defaults. For more detailed explanation, see Brentozar.com post An Introduction to Fillfactor in SQL Server. For my usage, I wanted to leave the fill factors as default, so the install scripts flips these back to my desired settings. I also run the sp_config command to ensure backup compression is enabled to save some space.</p> <p>Maybe this will help you get up to speed if you want to try out this great solution, but tweak a few defaults. The ease of installation across multiple instances makes this my current favorite solution, followed by the fantastic Ola Hallengren solution.</p>","tags":["powershell","sql-server","dbatools"]},{"location":"an-internet-with-less-ads-adguard/","title":"Ads...I know they employ people, make the world go round, gave us google.... but seriously I hate almost all ads.","text":"<p>If my ranking in google search drops to the end because of this post... well.... I'm ok with that.</p>","tags":["ramblings","cool-tools"]},{"location":"an-internet-with-less-ads-adguard/#adguard","title":"Adguard","text":"<p>I've been using this for over a year as a beta tester (they provided license for me to test and use latest versions). I had a perfect case to demo the craziness of some sites with ads vs using Adguard the other day and figured I'd share it if you... like me... hate the clutter they provide on many sites. Now, note that other factors come into play here. For example, the site I hite should have been using some optimization for images presented. This site was pretty insane on the ad content, so others aren't as dramatic. I just saw this as a Google Now recommended article and checked it out.</p>","tags":["ramblings","cool-tools"]},{"location":"an-internet-with-less-ads-adguard/#site-with-ads","title":"Site With Ads","text":"<p>WITH NO ADBLOCKING 679 Requests 11.3 MB for a single webpage load Finish Time 1.1min</p> <p></p>","tags":["ramblings","cool-tools"]},{"location":"an-internet-with-less-ads-adguard/#site-with-less-ads","title":"Site With Less Ads","text":"<p>WITH ADGUARD 116 Requests 2.2 MB for a single webpage load Finish Time 7.23sec This was loaded with the mobile emulator. If you are paying for data (for instance I'm on Project Fi), this could be a huge difference in your browsing bandwidth.</p> <p></p>","tags":["ramblings","cool-tools"]},{"location":"an-internet-with-less-ads-adguard/#side-by-side","title":"Side by Side","text":"<p>This blew my mind. According to Adguard metrics it had saved me over 2GB. Now, even taking this with a grain of salt, I was still pretty impressed by the results. I can immediately tell when I'm not running Adguard on android as the ads are everywhere. Adguard android has some additional functionality that provides the ability to create a local VPN and filter apps as well, so if you are using some app that has a annoying banner add right near the menu, this will most likely eliminate it. This is a pretty big image, and I blurred out text/ads to avoid any issues. Any guess at which one was the one with Adguard running?</p> <p></p>","tags":["ramblings","cool-tools"]},{"location":"an-internet-with-less-ads-adguard/#final-thoughts","title":"Final Thoughts","text":"<p>There are options to allow some \"acceptable\" ads. Not interested in this personally, but those who are should know it's offered. Cost can be a little higher than some options due to yearly cost, but the reward of a constantly developing product seems worth it, especially for folks that browse a lot.</p> <p>Only con I've come across with this is custom filtering options are a bit confusing for a non-technical user. Hopefully this will improve in the future to offer a much easier ad editing experience like some other similar toolkits. Lastly, a better notification of potentially blocking issues would be nice. I've come across a few sites that Adguard has blocked on various scripts or other \"needed\" actions that prevent the site from working. Disabling temporarily is acceptable for me, as this is quick with the chrome extension (pause for 30 secs). I'd say a better notification system, if even possible, on potentially site disrupting scripts/cookies blocked would be great enhancement. Overall, highly recommend this cross platform solution if you are looking for a better way to browse... with less ads!</p>","tags":["ramblings","cool-tools"]},{"location":"fixing-untrusted-foreign-key-or-check-constraint/","title":"Fixing Untrusted Foreign Key or Check Constraint","text":"<p>Untrusted constraints can be found when you alter/drop foreign key relationships and then add them back without the proper syntax.If you are deploying data through several tables, you might want to disable foreign keys on those tables during the deployment to ensure that all the required relationships have a chance to insert their data before validation.</p> <p>Once you complete the update, you should run a check statement to ensure the Foreign Key is trusted. The difference in the check syntax is actually ridiculous.... This check would not ensure the actual existing rows are validated to ensure compliance with the Foreign Key constraint.</p> <pre><code>alter table [dbo].[ChickenLiver] with check constraint [FK_EggDropSoup]\n</code></pre> <p>This check would check the rows contained in the table for adherence to the foreign key relationship and only succeed if the FK was successfully validated. This flags metadata for the database engine to know the key is trusted.</p> <pre><code>alter table [dbo].[ChickenLiver] with CHECK CHECK constraint [FK_EggDropSoup]\n</code></pre> <p>I originally worked through this after running sp_Blitz and working through the helpful documentation explaining Foreign Keys or Check Constraints Not Trusted.</p> <p>Untrusted Check Constraints and FKs can actually impact the performance of the query, leading to a less optimal query plan. The query engine won't know necessarily that the uniqueness of a constraint, or a foreign key is guaranteed at this point.</p> <p>I forked the script from Brent's link above and modified to iterate through and generate the script for running the check against everything in the database. This could be modified to be server wide if you wish as well. Original DMV query credit to Brent, and the the tweaks for running them against the database automatically are my small contribution.</p> <p>Note: I wrote on this a while back, totally missed that I had covered this. For an older perspective on this: Stranger Danger... The need for trust with constraints</p>","tags":["sql-server"]},{"location":"data-compare-on-temporal-tables/","title":"Data Compare on Temporal Tables","text":"<p>I hadn't seen much talk on doing data comparisons on temporal tables, as they are a new feature. I went through the exercise to compare current to historical to see how Red Gate &amp; Devart handled this. I'm a part of the Friends of Red Gate program, so love checking out their latest updates, and I'm also a regular tester on Devart which also provides fantastic tools. Both handled Temporal Tables with aplomb, so here's a quick walk through on how I did this.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#ssms-2016-view-of-temporal-table","title":"SSMS 2016 View Of Temporal Table","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>With the latest version of SSMS, you can see the temporal tables labeled and expanded underneath the source table.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#red-gate-sql-data-compare-12","title":"Red Gate SQL Data Compare 12","text":"<p>To begin the comparison process, you need to do some custom mapping, which requires navigating into the Tables &amp; Views settings in SQL Data Compare</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#unmap-the-existing-options","title":"Unmap the existing options","text":"<p>To remap the Customers to Customers_Archive, we need to select this in the tables and choose to unmap the Customer and the Customer-Archive Tables from each other. This is 2 unmapping operations.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#setup-compare-key","title":"Setup Compare Key","text":"<p>Go into the comparison settings on the table now and designate the key as the value to compare against. For the purpose of this example, I'm just doing key, you can change this however you see fit for your comparison scenario.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#remove-any-columns-from-comparison-desired","title":"Remove any columns from comparison desired","text":"<p>In this example, I'm removing the datetime2 columns being used, to instead focus on the other columns.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#compare-results","title":"Compare results","text":"<p>If you run into no results coming back, look to turn off the setting in compare options for Checksum comparison, which helps improve the initial compare performance. With this on, I had no results coming back, but once I turned off, the comparison results came back correctly.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#conflict-row","title":"Conflict Row","text":"<p>This entry was matched in DbForge SQL Data Compare as a conflict due to matching the key in a non-unique manner. The approach the two tools take is a little different. In RG Data Compare</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#conflict-entry-only-in-destination","title":"Conflict Entry Only In Destination","text":"<p>The entry identified as potential conflict  by DbForge is identified in the Only In Destination.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#diff-report","title":"Diff Report","text":"<p>Both tools report differences. RG's tool has focused on the diff report being simple CSV output. This is fine in the majority of cases, though I'm hoping for additional XLSX and HTML diff reports similar to DbForge eventually. In the case of the CSV output, you could consume the information easily in Power-BI, Excel, or even... SQL Server :-) No screenshot on this as it's just a csv output.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#devart-sql-data-compare","title":"Devart SQL Data Compare","text":"<p>Going into the mapping, you can see support for Customers and Customers_Archive, which is the temporal history table for this. In this case, I mapped the current table against the temporal table to compare the current against the change history.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#choose-the-key-column-to-compare-against","title":"Choose the key column to compare against","text":"<p>As a simple example, I just provided the primary key. You could get creative with this though if you wanted to compare specific sets of changes.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#handling-conflicts-differently","title":"Handling Conflicts differently","text":"<p>Looks like the conflict is handled differently in the GUI than Red Gate, as this provides back a separate tab indicating a conflict. Their documentation indicates: Appears only if there are conflict records (records, having non-unique values of the custom comparison key). DbForge Data Compare for SQL server Documentation - Data Comparison Document</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#diff-report_1","title":"Diff Report","text":"<p>The diff reports provided by DbForge Data Compare are very well designed, and have some fantastic output options for allowing review/audit of the rows.</p> <p></p>","tags":["cool-tools","redgate","sql-server"]},{"location":"data-compare-on-temporal-tables/#diff-report-details","title":"Diff Report Details","text":"<p>Here is a sample of a detail provided on the diff report. One feature I found incredibly helpful was the bold highlighting on the columns that had diffs detected. You can trim down the report output to only include the diff columns if you wish to further trim the information in the report.</p> <p></p> <p>Overall, good experience with both, and they both support a lot of flexibility with more specialized comparisons.</p>","tags":["cool-tools","redgate","sql-server"]},{"location":"pizzicato-pizza/","title":"Pizzicato Pizza","text":"<p>Had a blast making this song. Unfortunately, creating video of slow motion pizza cutting was going to be a bit awkward to do, as I didn't feel like asking a pizza place to let me stand behind the counter and video pizza assembly. :-)Best listened to with headphones, not a phone speaker :-)</p> <p>This was an experimentation with several new tools for me.</p>","tags":["music","ramblings"]},{"location":"pizzicato-pizza/#song-creation","title":"song creation","text":"<p>I first used Presonus Studio One 3 to record some basic track parts like the guitar. I then added some drums using BFD3 (which I've previously reviewed). Using the tools to adjust and randomize the velocity and humanize the rhythm helped create something that sounds more realistic. Then I recorded a fun guitar riff, and used Melodyne Essentials, which is included in Studio One, to convert the guitar part into MIDI. This allowed me to then change the guitar part I had recorded into a MIDI song part and I choose to replace with Pizzicato strings, as I really liked the sound of Presence XT string section. I've never heard a post-rock style song with pizzicato strings, so figured I might bring a little variety to the post-rock mix.</p>","tags":["music","ramblings"]},{"location":"pizzicato-pizza/#video-editing","title":"video editing","text":"<p>The video editing was extremely simplistic. I used Edius 8.2, which is a fantastic NLE for broadcast video editing. I plan on writing or creating a video presentation on that soon to help others interested in video editing to see a little more on that process. I graded with Magic Bullet Looks as I liked the grain and dark feel of the image. Using stabilization effect in Edius, and slowing the speed to 35% (originally 1080p 60fps Panasonic GH3) the resulting imagery was great. I recorded this imagery handheld from this camera on a rainy day in my backyard. I thought the close up of the rain puddling would be interesting for something, and finally found a use. Hope you enjoy my experiment, I'll work later on doing some walkthroughs for anyone interested more in converting audio to MIDI, basic editing with Edius, and some workflow examples with Presonus Studio One. Music:</p> <ul> <li>Presonus Studio One 3</li> <li>BFD3 Drums</li> <li>Presence XT Strings</li> <li>Guitar: Epiphone Wildkat</li> <li>Amp: Vox AC15C2</li> <li>Zoom H6 Interface</li> <li>5 string bass   Video</li> <li>Panasonic GH3</li> <li>Editing with Edius 8.2 </li> <li>Graded with Magic Bullet Looks</li> </ul>","tags":["music","ramblings"]},{"location":"get-backup-history-for-all-databases-in-server/","title":"Get Backup History for All Databases in Server","text":"<p>Here's a quick snippet to get a listing of the database backups that last occurred on a server. Most solutions provided a single backup listing, but not the brief summary of the last backup details I was looking for.</p>","tags":["sql-server"]},{"location":"parallel-powershell-for-running-sql/","title":"Parallel Powershell for Running SQL","text":"<p>This is just a quick look. I plan on diving into this in the future more, as I'm still working through some of the changes being made in the main parallel modules I utilize for SQL server. In the meantime, if you are looking for a quick way to leverage some parallel query running, take a look at PSParallel. I've avoided Powershell Jobs/Workflow due to limitations they have and the performance penalty I've seen is associated with them.For my choice, I've explored PSParallel &amp; PoshRSJob. I've found them helpful for running some longer running queries, as I can have multiple threads running across server/database of my choice, with no query windows open in SSMS. Another great option that is under more active development is PoshRsJob. Be clear that this will have a higher learning curve to deal with as it doesn't handle some of the implicit import of external variables that PSParallel does. You'll have to work through more issues initially to understand correctly passing parameters and how the differents scope of runspaces impact updating shared variables (ie, things get deeper with synchronized hashtables and more :-) ) Hope this helps get you started if you want to give parallel query execution a shot. Here's a function using PSParallel to get you started. Let me know if it helps</p>","tags":["powershell","sql-server"]},{"location":"attaching-database-using-smo-and-powershell/","title":"Attaching Database Using SMO & Powershell","text":"<p>Steve Jones wrote a great article on using this automation titled <code>The Demo Setup-Attaching Databases with Powershell</code>. I threw together a completed script and modified it for my functionality here. MSDN documentation on the functionality is located here Server.AttachDatabase Method (String, StringCollection, String, AttachOptions)I see some definitive room for improvement with some future work on this to display percentage complete and so on, but did not implement at this time.</p> <p>For the nested error handling I found a great example of handling the error output from: Aggregated Intelligence: Powershell &amp; SMO-Copy and attach database. If you don't utilize the logic to handle nested errors your powershell error messages will be generic. This handling of nested error property is a must to be able to debug any errors you run into. http://blog.aggregatedintelligence.com/2012/02/powershell-smocopy-and-attach-database.html</p> <p>If you want to see some great example on powershell scripting restores with progress complete and more I recommend taking a look at this post which had a very detailed powershell script example. SharePoint Script - Restoring a Content Database</p>","tags":["automation","powershell","sql-server"]},{"location":"scan-folder-of-dlls-to-identify-x86-or-x64-compiled-assemblies/","title":"Scan folder of dlls to identify x86 or x64 compiled assemblies","text":"<p>Point this at a directory of dlls and you can get some of the loaded assembly details to quickly identify what type of processor architecture they were compiled for.I did this as I wanted to explore a large directory of dlls and see if I had mixed assemblies of x32 and x64 together from a visual studio build. Some dlls with invalid assembly header information were found, and this skips those as warnings.</p>","tags":["powershell"]},{"location":"the-traditional-birthday-song-is-terrible/","title":"The Traditional Birthday Song Is Terrible","text":"<p>The traditional birthday song is terrible.It's never really changed. It's like singing a dirge. It's really really hard for people to sing anywhere close to on key. We all sing it because we have to, but there is this feeling of regret, like \"I'll do it for you, but just because I love you\". It is followed by \"Many mourns\" by the closest available family clown. Apparently, the roots were back in the 19th century, and wikipedia says:</p> <p>In 1988, Warner/Chappell Music purchased the company owning the copyright for US$25 million, with the value of \"Happy Birthday\" estimated at US$5 million....In February 2016 Warner/Chappell settled for US $14 million, paving the way for the song to become public domain.[18] WIKI Let's leave this tainted legacy behind. I propose a radical change. Ditch it for something fun. Make a new family tradition.</p> <p>This is much more like Rend Collective's reminder to be always practicing the Art of Celebration :-) It's super easy to sing the first time, promotes foot stomping and hand clapping, promotes dancing and jumping, and overall, I feel conveys the feeling a birthday should have. New lyrics:</p> <pre><code>**The Art of Celebration Birthday Song**\n- created after much deliberation and refinement in 1 min at the house of Sheldon Hull\nHey hey hey, It's your birthday day\nHey hey hey, It's your birthday day\nHey hey hey, It's your birthday day\nSing Loud, Sing Proud, It's Your Birthday\n- Creative Commons Zero CC0, so do whatever you want with this bit of musical genius\n</code></pre>","tags":["music","rambling","ramblings","music"]},{"location":"dynamically-set-powershell-variables-from-json/","title":"Dynamically Set Powershell Variables from json","text":"<p>I created this small snippet to allow a list of values from a json file be turned into variables to work with. For working with a fixed list of configuration values, this might be helpful to reduce some coding effort.</p>","tags":["powershell","tech","sql-server"]},{"location":"manictime-timetracking-automation-done-right/","title":"ManicTime - Timetracking Automation Done Right","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>Tracking time is always a beast. With the amount of context switching many developers do, it can be tough to remember how much time went to each project. With companies looking to track effort on sprints, hours on a client project, or (as in my case) just a dev wanting to better evaluate the productive use of time, this app fills some gaps that others don't.For instance, I've tried tools such as Toggl, and similar. I found them useful, but requiring a lot of diligence to work with. There is very little \"automatic\" categorization of time. Many of those tools are focused on a timer based approach that requires you to start and stop the timer. ManicTime approaches this differently. It has the typical stop watch, countdown, Pomodoro type functionality a time tracking tool might offer, but in addition to this it provides a captured timeline of activity with various forms of meta data to easily review and parse for categorization.</p> <p>This categorization of information can take the form of details such as:</p> <ol> <li>Development</li> <li>Personal</li> <li>Browsing or be specific based on user input such as</li> <li>Development, Reticulating Splines</li> <li>Personal, Contemplating Navel</li> <li>Project Manhattan, Task 666: Optimizing Nuclear Db Performance</li> </ol> <p>Manually entered information is a big driver for better tagging accuracy, but it expands this to allowing dynamic tags based on matching of applications, titles of documents, titles of web pages, calendar entries, and more. This offers a vast range of meta data captured on your system to generate more accurate allocation of your time. The only real negative to this is that it is not as simple as something like Toggle. However, with a little work, you can work an entire day and quickly recap at the end, categorizing your effort with very little fuss, and a high amount of accuracy.</p> <p>If you find yourself forgetting to hit the stop/start button on your time tracker and want to consider a better option for tracking your effort, then look no farther.</p> <p>I've used this application over the last year and found a lot of value in it, figured I'd share a little on it, as it's become on of the tools in my essentials pack. Disclaimer: I was provided with a license a while ago on this. This doesn't impact my review, as I just like finding great tools to help devs have a better workflow. I only end up reviewing the ones that I've really found useful</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#overview","title":"Overview","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#time-categories","title":"Time Categories","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#overview-of-days-activities","title":"Overview of Day's Activities","text":"<p>The list of all activities is organized into several timelines, whether it be the applications, extracted document titles from applications, calendar, or even calendar feeds. This allows a variety of ways to go back through and easily organize and track time. One recent improvement that I completely love is the integrated screenshots into the application timeline. This allows you to keep a running screenshot log of activity to easily go back through existing applications and remember exactly what was being done at the time. A very useful implementation!</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#tracking-exclusion","title":"Tracking Exclusion","text":"<p>Note that you can choose to go off record, as well as specifically only track certain times of day. This is a good option for those that have a work laptop that they might leave running and only want to report on certain periods of active work time.</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#autotagging","title":"Autotagging","text":"<p>Autotagging is where this tool gets powerful. Basically, the concept is to allow automatically tagging based on application, window title, url, or other parsed value. This means you can categorize your effort much more easily, with minimal effort.</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#regex-parsing","title":"Regex Parsing","text":"<p>I've yet to figure out the dynamic tags based on regex parsing as it doesn't seem to give you a preview to test and refine results. Once I figure this out, or the app improves the ability to use this I think the additional timelines will be very handy as you could have one timeline focused on dynamic parsing and grouping of projects based on doc/chrome titles that doesn't interfer with the categorization that the other timeline might use. This is a usability issue that I hope to see improved in the future. It has a lot of potential.</p> <p></p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#multiple-autotag-timelines","title":"Multiple Autotag Timelines","text":"<p>This is someone I've recently been exploring as it provides the capability to create an automatic tagging of apps, but for different purposes. For instance, you might setup one rule for parsing project numbers and send to a AutoProject timeline that aggregates the totals, but another timeline for categorization of the apps/websites. Another use might be a timeline focused on categorizing web usage, while another focuses on app usage.</p> <p></p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#tagging","title":"Tagging","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#away-time-tagging","title":"Away Time Tagging","text":"<p>You can have ManicTime prompt you when you return from your computer, or when a timer has detected minutes of idle on your system. This can help ensure that if you are gone to a meeting, or away from your PC you are still tracking the time you used.</p> <p></p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#narrow-down-untagged-time-quickly","title":"Narrow down untagged time quickly","text":"<p>There is a variety of ways to filter down the timeline to only untagged activities as the selected, or untagged as what's actually shown. This can help identify gaps in what you've reviewed.</p> <p></p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#statistics-reports","title":"Statistics &amp; Reports","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#generate-timesheet-report","title":"Generate Timesheet Report","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#some-nice-visual-statistics-available","title":"Some Nice Visual Statistics Available","text":"","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#other-statistics-available","title":"Other Statistics Available","text":"<p>These are listed based on the selected groups, tags and more.</p> <p></p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#manic-time-server","title":"Manic Time Server","text":"<p>Manic time offers server functionality to allow this tool to be used to help generate reports for staff members and usage. This functionality is not for the non-technical user. I found it a little challenging to get things setup, so this current iteration wasn't designed as a simple \"central\" solution for all devices. With a better setup/configuration experience (no domain user logins etc) and perhaps more of a Google Drive/Dropbox type sync, I think the solution would be fantastic for tracking time on various devices. Due to the setup issues I had on server, I wasn't able to include tracking from the new ManicTime android client. I would say that homegrowing your own tracking solution with Tasker and a custom timeline here might not be a difficult project to consume through the app due to the documented format for consuming external timeline information. I haven't gone to that effort, but it's an intriguing concept.</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#daily-retrospective","title":"daily retrospective","text":"<p>Being able to retroactively tag and categorize effort at the end of the day, without having to constantly toggle the stopwatch. You can approach with a stop watch/pomodoro/countdown built in, but if you get pulled in multiple tangents, this tool makes it easy to go back and categorize throughout the day... IF your primary work is driven on using your computer. Since I'm approaching this from a developer tool point of view, it's a perfect fit!</p>","tags":["development","ramblings","cool-tools"]},{"location":"manictime-timetracking-automation-done-right/#last-thoughts","title":"Last Thoughts","text":"<p>Fantastic app with a unique approach. Cost is a little high, but it's an independent app so supporting the development can be a good thing as a really specialized tool. Not sure they'd be able to continue development if it was a lifetime purchase (those seem to have gone away over time). As a good office tool for better tracking and reporting on time (for instance if working with clients), then I believe it might just pay for itself. I'd like to see a smoother integration with the server components to being a better cloud tracking mechanism, allowing android, pc, mac, all to provide a solid reporting mechanism for families on the go. The app seems more focused on enterprise/business tracking though, so this might not be implemented. I'll continue using and finding great value in helping track my time with the least amount of work. For those looking for a solution, give it a shot. They have a lite version available as well with less features, so you can take a swing at it.</p>","tags":["development","ramblings","cool-tools"]},{"location":"bad-idea-jeans-query-optimization-through-minification/","title":"Bad Idea Jeans: Query Optimization Through Minification","text":"<p>SQL is pretty verbose compared to some languages. It's a pretty big disappointment that I have to type out <code>select customer from dbo.customers where id = 2</code> instead of a much simpler syntax like <code>dbo.Customers ' Customer</code> like Powershell might offer. As I considered the disappointing verbosity of sql server, I considered that perhaps one way to reduce network traffic, save electricity, and aid the garrulous language known as sql might be to require all code running to be minified.Think about the potential savings in bandwidth and having to scroll. Anyone who complains about this should just realize there is a thing called word-wrap which will solve all the readability problems. No more need for Red Gate Sql Prompt's beautiful yet wasteful formatting options. (sorry RG). In fact, no more debates on readability of formatting standards at all!</p> <p></p> <p>In a file size comparison on this small small query I found a full 1KB size savings.</p> File Size WITH-LINE-BREAKS.sql 9 KB NO-LINE-BREAKS.sql 8 KB <p>If you extrapolate this over a larger query I found a 20% reduction.</p> File Size LONG-NO-LINE-BREAKS.sql 129 KB LONG-WITH-LINE-BREAKS.sql 160 KB <p>With a heavy traffic OLTP system, this might reduce traffic IO tremendously and reduce server energy costs. I did consider trying to calculate the wattage savings this might entail, but I plead laziness. I also considered running wireshark to analyze packets to compare the size to validate what magic the compression and other parser functions perform on the sent query, but decided I had better uses for my time. .... Maybe the next brilliant idea would be to name all your tables consecutive numbers like dbo.1, dbo.2, dbo.3 and so on. Since these names are stored in a nvarchar format in system tables, it might optimize the storage and performance of the system tables.... So many good ideas, so little time......</p> <p>this post was inspired by the wonderful contributions to bad ideas that Brent Ozar and his team of eccentric consultants donated to the community to cause mayhem for those stupid enough to try them out. Thanks Brent :-)</p>","tags":["bad-idea","sql-server","tech"]},{"location":"implicit-transactions/","title":"Implicit Transactions","text":"<p>Never messed around with this setting in the server configuration, so I was unfamilar with the impact it would have.If I ran a statement with something like the following:</p> <pre><code>insert into foo\nselect bar\ninsert into foo\nselect bar\n</code></pre> <p>I know that if the first had an aborting error, such as text was too long, the second statement would not complete as the batch would have failed. If you instead did</p> <pre><code>insert into foo\nselect bar\nGO\ninsert into foo\nselect bar\n</code></pre> <p>and had the same error, the second would be completed, since the first would throw an error, but the GO separates the second statement explicitly into another batch, and therefore another transaction. Interestingly, the <code>Implicit Transactions</code> option changes the behavior to making each statement act as if it was encapsulated by <code>begin transaction --- commit transaction</code> instead of requiring this to be defined. So if you set <code>implicit transactions</code> on and ran the statement below with no go statement:</p> <pre><code>insert into foo\nselect bar\ninsert into foo\nselect bar\n</code></pre> <p>It is really operating as if:</p> <pre><code>begin transaction\ninsert into foo\nselect bar\ncommit transaction\nGO\nbegin transaction\ninsert into foo\nselect bar\ncommit transaction\n</code></pre> <p>MSDN - Implicit Conversions is a resource that further documents the behavior, indicating that a rollback for the particular transaction is handled automatically. This means that since that each statement is treated as a transaction that it will not abort the second statement and terminate execution if the first experinces the error, since by \"implicit conversions\" this would be handled separately. MSDN article with example code to walk through it https://technet.microsoft.com/en-us/library/ms190230(v=sql.105).aspx</p> <pre><code>/*******************************************************\nSTEP 1: SETUP\n*******************************************************/\nuse tempdb;\nset nocount on;\nset xact_abort off;\nset implicit_transactions off;\nif object_id('dbo.TestImplicitTrans','U') is not null\nbegin\nprint 'Dropped dbo.TestImplicitTrans per existed';\ndrop table dbo.TestImplicitTrans;\nend;\nprint 'create table dbo.TestImplicitTrans';\ncreate table dbo.TestImplicitTrans\n(\ntest_k int primary key\nidentity(1,1)\nnot null\n,random_text varchar(5) not null\n);\ngo\n/*******************************************************\nTEST 1:\nxact_abort off\nset implicit_transactions off\nResults in:\n- first transaction fails\n- second transaction succeeds (this is due to xact_abort off not being activated)\ntest_k  random_text\n2           12345\n*******************************************************/\nuse tempdb;\ngo\nset nocount on;\nset xact_abort off;\nset implicit_transactions off;\ntruncate table dbo.TestImplicitTrans;\nprint 'Statement 1 START';\ninsert into dbo.TestImplicitTrans (random_text) values ('00001x');\nprint 'Current trancount: ' + cast(@@trancount as varchar(100));\ninsert into dbo.TestImplicitTrans (random_text) values ('00002');\nprint 'Current trancount: ' + cast(@@trancount as varchar(100));\ninsert into dbo.TestImplicitTrans (random_text) values ('00003');\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 1 END';\nprint char(13) + char(13) + 'Statement 2 START';\ninsert  into dbo.TestImplicitTrans\n(random_text)\nvalues\n('12345'  -- random_text - varchar(5)\n);\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 2 END';\nselect\n*\nfrom\ndbo.TestImplicitTrans as TIT;\ngo\n/*******************************************************\nTEST 2:\nxact_abort on\nset implicit_transactions off\nResults in:\n- first transaction fails\n- second transaction doesn't execute due to xact abort being set on\ntest_k  random_text\nNONE\n*******************************************************/\nuse tempdb;\ngo\nset nocount on;\nset xact_abort on;\nset implicit_transactions off;\ntruncate table dbo.TestImplicitTrans;\nprint 'Statement 1 START';\ninsert  into dbo.TestImplicitTrans\n(random_text)\nvalues\n('12345x'  -- random_text - varchar(5)  ONE CHARACTER TOO LARGE\n);\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 1 END';\nprint char(13) + char(13) + 'Statement 2 START';\ninsert  into dbo.TestImplicitTrans\n(random_text)\nvalues\n('12345'  -- random_text - varchar(5)\n);\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 2 END';\nselect\n*\nfrom\ndbo.TestImplicitTrans as TIT;\ngo\n/*******************************************************\nTEST 2:\nxact_abort off\nset implicit_transactions off\nResults in:\n- first transaction fails\n- second transaction doesn't execute due to xact abort being set on\ntest_k  random_text\nNONE\n*******************************************************/\nuse tempdb;\ngo\nset nocount on;\nset xact_abort on;\nset implicit_transactions on;\ntruncate table dbo.TestImplicitTrans;\nprint 'Statement 1 START';\ninsert  into dbo.TestImplicitTrans\n(random_text)\nvalues\n('12345x'  -- random_text - varchar(5)  ONE CHARACTER TOO LARGE\n);\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 1 END';\nprint char(13) + char(13) + 'Statement 2 START';\ninsert  into dbo.TestImplicitTrans\n(random_text)\nvalues\n('12345'  -- random_text - varchar(5)\n);\nprint 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\nprint 'Statement 2 END';\nselect\n*\nfrom\ndbo.TestImplicitTrans as TIT;\ngo\n</code></pre>","tags":["sql-server"]},{"location":"quick-way-to-run-powershell-tasks-in-parallel/","title":"Quick Way to Run Powershell Tasks in Parallel","text":"<p>Running tasks in parallel can be a bit difficult in powershell. However, there are a few projects out there that optimize the performance and provide a better experience of running tasks in parallel with less effort.#cool uses A few cool uses of this might be running parallel sql queries across multiple servers or databases while maintaining a throttled limit to avoid saturation of the target environment. Additionally, long running queries might benefit in running in parallel if running on multiple objects in the same database or in different databases.</p>","tags":["powershell","sql-server"]},{"location":"quick-way-to-run-powershell-tasks-in-parallel/#module-magic","title":"module magic","text":"<p>I've utilized two main modules to advance this. PSParallel and PoshRSJobs. Both are fantastic options. The Invoke-Parallel is not steadily maintained, so I try to use PoshRSJob when possible. However, for ease of use the Invoke-Parallel option is pretty awesome as it automatically imports variables, functions, and modules into the block to allow for less work in defining parameters, having to use the <code>$using:variablename</code> clause, etc.</p>","tags":["powershell","sql-server"]},{"location":"quick-way-to-run-powershell-tasks-in-parallel/#lots-of-gotchas","title":"lots of gotchas","text":"<p>However, be prepared to deal with some complications in doing this with powershell. For instance, write-host, write-verbose, write-error, at this time can throw errors in PoshRSJob or not provide any output, as these streams are not incorporated the same as your local ISE session. In fact, at the time of this post, for output to stream from the PoshRSJob module, I had to change my output from:</p> <pre><code>write-host 'I know a kitten dies every time writehost is used, but I just cannot stop myself'\n</code></pre> <p>to</p> <pre><code>\"I know a kitten dies every time writehost is used, but I just cannot stop myself\"\n</code></pre> <p>Yes... no write-host/write-error/write-verbose is used here, just quotes for it. The developer and github community is looking to improve this, but at this time, don't expect logging or error messages to come through the same way.</p> <p>Be prepared to deal with some complications on error handling when dealing with runspaces, as even though they are more performant, there is a lot of issues with scope to deal with in those isolated runspaces. Once you start increasing the size of the script blocks things can get hard to debug.</p> <p>I think the simpler the task to pass into the parallel tasks, the better.</p> <p>However, for some basic tasks that would benefit in parallel, you can definitely give it a shot. This task focused on iterating through a directory recursively and cleaning up each of the files by stripping out comments and blank lines. The following results were a simple example and interesting to compare.</p> <pre><code>    -------- Summary with PoshRSJobs--------\n    File Size:   9.59 MB\n    Total Count: 4,600.00\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\n    Total Original Lines: 1221673\n    Total Lines: 1,201,746.00\n    Total Lines Saved:  21,959.00\n    TOTAL TIME TO RUN: 08:43\n\n    -------- Summary with Invoke-Parallel --------\n    File Size:   6.69 MB\n    Total Count: 4,447.00\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\n    Total Original Lines: 1221436\n    Total Lines: 854,375.00\n    Total Lines Saved:  360,045.00\n    TOTAL TIME TO RUN: 05:22\n</code></pre> <p>PoshRSJobs seemed to approach creating the job list first, which took a long time, and then processed the output very quickly. Overall, this took longer for this type of task. Invoke-Parallel gave an almost instant response showing the progress bar with estimated time remaining, so for this type of job it actually ran faster.</p> <pre><code>    -------- Summary - Native ForEach --------\n    File Size:   6.69 MB\n    Total Count: 4,621.00\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\n    Total Original Lines: 1227408\n    Total Lines: 861,600.00\n    Total Lines Saved:  365,808.00\n    TOTAL TIME TO RUN: 04:52\n</code></pre> <p>Surprising to me, the native foreach which was single threaded was faster. I believe in this case, the overhead of setting up the jobs was not worth parallel task processing. Since the task was a lot of small tasks, this probably wasn't a good candidate for parallel tasks. Based on this small test case, I'd venture to look into parallel tasks when longer run times are involved, such as perhaps copying large files that aren't oversaturating your IO. In this case, slow long copies would probably benefit from parallel tasks, while small text file copies as I showed wouldn't. A simple example of the difference in syntax for using PSParallel would be just counting lines in files in a directory.</p> <pre><code>$Folder = 'C:\\Temp'\n$startTime = get-date\n[int]$TotalManualCount = 0\nGet-ChildItem -Path $Folder -Recurse -Force ' where { ! $_.PSIsContainer } ' % { $TotalManualCount += (Get-Content -Path ($_.FullName) -Force ' Measure-Object -Line).Lines}\nwrite-host ('Total Lines: {0:N2}' -f $TotalManualCount)\nWrite-host ('FOREACH: Total time to process: {0}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('mm\\:ss'))\n#Using Invoke-Parallel#\n$ManualCount = [hashtable]::Synchronized(@{})\n$ManualCount = @{\nTotalCount     = 0\n}\n$Folder = 'C:\\Temp'\n$startTime = get-date\nGet-ChildItem -Path $Folder -Recurse -Force ' where { ! $_.PSIsContainer } ' Start-RsJob -Throttle 4 -ArgumentList $ManualCount -ScriptBlock {\n[cmdletbinding()]\nparam($ManualCount)\n$ManualCount.TotalCount += (Get-Content -Path ($_.FullName) -Force ' Measure-Object -Line).Lines\n}\nwrite-host ('Total Lines: {0:N2}' -f $ManualCount.TotalCount)\nWrite-host ('INVOKE-PARALLEL: Total time to process: {0}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('mm\\:ss'))\n</code></pre> <p>Note that this simple code example might have had some issues with counts due to locking with the synchronized hash table usage. Based on a few searches, it looks like you need to implement a lock on the hash table which ensures that particular thread is able to safely update. I didn't find clear proof that the synchronized hash table was working or failing, but it's something to be aware of. There are some active efforts on improving in PoshRSJob github issues. Hopefully you'll have a few new ideas on working with Parallel tasks in powershell now, and think about leveraging it for some tedious tasks that might benefit with SQL server or other administrative jobs.</p>","tags":["powershell","sql-server"]},{"location":"redgate-sql-data-compare-%26-devart-db-forge-data-compare/","title":"Redgate SQL Data Compare & Devart DBForge Data Compare","text":"<p>I'm a big fan of Redgate, as I'm in the Friend of Redgate program. However, I do also utilize some other toolkits. One competitor that I find has some , but I do dabble with some other toolkits (I know heresy :-) . One of the competitors that I find has some brilliant features, but many time lacks the refinement and ease of use of Redgate is Devart tools. The tools they offer are often really nice, and continually updated based on feedback. As a general rule, I'd say the Devart tools feel less \"refined\" in some areas, but then offer some really nice usability features that RG hasn't yet implemented. Both have their place in my toolbelt depending on the need.Having just completed some very large data comparisons on views, generating over 15GB of network traffic in last few days, I've been really impressed with the usability and output from Devart DbForge Data Compare. The performance seems great.</p> <p>I've evaluated their schema compare before and found it fantastic for the price if I was strapped on a budget, but when able to pay for an overall more flexible and refined product I'd definitely choose SQL Compare. The differences are much smaller on the data compare tool though due to the much less complex nature of what it's performing. I ran across a few features in that I thought would be great to mention for the team working on Data Compare to provide some enhanced functionality.</p> <p>Diff Report: They provide a fantastic option of outputting a diff report not only in CSV but in XLS format. The formatted report is much more usable than the CSV I get from RG Data compare because they format, and apply bold to the _S and _T cells that actually have a difference, enabling a much easier review process to find the diffs. This is far more usable for an end audience that might want to view differences in data detected on a table. I've had the case to provide this report to analysts to look at differences. The typical use case of DBA's syncing data from one database to another probably would just use the tool and never need this. My particular use case has found a better report output would have been a major benefit. Cached schema object definitions/mapping. They load up previous mappings so you can go and tweak without the need to refresh immediately. This would be nice when you are fine tuning the comparison results and keep needing to tweak to the figures. Other suggestions based on my recent work w/large table comparison. Since table size has a direct impact on the compare due to local caching of the data, consider providing a column that shows estimated &amp; total space required for the comparison. This way if I compared a lot of small tables I'd see the rowcount/size (<code>sp_spaceused</code>) and then added a large table (3GB for example), I'd see the approx local storage and network transfer impact with total size of \"7GB total storage/transfer required\".</p> <p>If I setup a comparison on a view with custom key (due to no index on the view), and I drop and recreate the view for a new definition, the comparison options are not persisted (for example the custom key). I'm not sure if this is due to the underlying changes on the <code>object_id</code> and lack of clustered index for explicit mapping, but persisting this would be really nice when the columns used for key comparison still exist. Overall, as a friend of Redgate I'm</p>","tags":["sql-server"]},{"location":"ssms-2016-object-explorer-read-uncommitted/","title":"SSMS 2016 - Object Explorer Read Uncommitted","text":"<p>I ran through some directions from others, including the very helpful post from SqlVariant, but I had issues locating the correct keys. For my Windows 10 machine, running SSMS 2016, I found the registry keys related to the object explorer located in a different path.</p> <p>I found matches for read committed/uncommitted string at: <code>HKCU\\SOFTWARE\\Microsoft\\VisualStudio\\14.0\\SSDT\\SQLEditorUserSettings</code></p> <p>Running the following powershell command: <code>get-itemproperty -path 'Registry::HKCU\\SOFTWARE\\Microsoft\\VisualStudio\\14.0\\SSDT\\SQLEditorUserSettings' ' select SetTransactionIsolationLevel ' format-list</code></p>","tags":["sql-server"]},{"location":"track-creation-of-databases/","title":"Track Creation of Databases","text":"<p>Sys.Databases has some create information, but I was looking for a way to track aging, last access, and if databases got dropped. In a development environment, I was hoping this might help me gauge which development databases were actually being used or not.</p> <pre><code>/*******************************************************\n    run check on each constraint to evaluate if errors\n*******************************************************/\nif object_id('tempdb..##CheckMe') is not null\n    drop table ##CheckMe;\n\nselect\n    temp_k =    identity(int, 1, 1)\n    ,X.*\ninto ##CheckMe\nfrom\n    (select\n\n            type_of_check =                                            'FK'\n            ,'[' + s.name + '].[' + o.name + '].[' + i.name + ']'    as keyname\n            ,CheckMe =                                                'alter table ' + quotename(s.name) + '.' + quotename(o.name) + ' with check check constraint ' + quotename(i.name)\n            ,IsError =                                                convert(bit, null)\n            ,ErrorMessage =                                            convert(varchar(max), null)\n        from\n            sys.foreign_keys i\n            inner join sys.objects o\n                on i.parent_object_id = o.object_id\n            inner join sys.schemas s\n                on o.schema_id = s.schema_id\n        where\n            i.is_not_trusted = 1\n            and i.is_not_for_replication = 0\n        union all\n        select\n            type_of_check =                                            'CHECK'\n            ,'[' + s.name + '].[' + o.name + '].[' + i.name + ']'    as keyname\n            ,CheckMe =                                                'alter table ' + quotename(s.name) + '.' + quotename(o.name) + ' with check check constraint ' + quotename(i.name)\n            ,IsError =                                                convert(bit, null)\n            ,ErrorMessage =                                            convert(varchar(max), null)\n        from\n            sys.check_constraints i\n            inner join sys.objects o\n                on i.parent_object_id = o.object_id\n            inner join sys.schemas s\n                on o.schema_id = s.schema_id\n        where\n            i.is_not_trusted = 1\n            and i.is_not_for_replication = 0\n            and i.is_disabled = 0) as X\n</code></pre>","tags":["sql-server"]},{"location":"life-hack-when-you-need-a-mouse-pad/","title":"Life Hack: When you need a mouse pad","text":"<p>My Logitech Master mouse went on the fritz... Requiring me to use my trusty Microsoft mouse. This is an optical based mouse based mouse that started driving me insane with the lack of precision. This exhibited random skips and overall lack of agreeability. I ran across some posts mentioning optical mice have issues with certain surfaces making it hard to detect precise movements and suggested using a surface with a pattern or irregularity, even a piece of paper being scribbled on. I was using a white desk, very little pattern to work with. The MX Master worked perfectly, but it had spoiled me.</p> <p>After searching the office and failing to find a usable mousepad (seriously!)... I googled diy mousepad.</p> <p>Lo and beheld, a fantastic website provided the World's original cheapest Mousepad. Well worth visiting despite its self disparaging remarks. Thank you for your contribution to the Internet's vast amalgamation of priceless documents.</p> <p>After feeling like an idiot, I printed, taped to my desk and the magical precision returned. Life hack!</p>","tags":["lifehack","tech","ramblings"]},{"location":"red-gate-sql-clone-1-initial-setup/","title":"Red Gate SQL Clone (1) - Initial Setup","text":"<p>Note this was during earlier beta usage, so some of the UI and other features will have been updated more. I plan on writing more on this promising tool as  I get a chance to dive into it more, especially the powershell cmdlets for database cloning automation. In the meantime, I believe the permissions issue is still relevant, so I'll post this as a reminder in case someone is working through the initial setup.</p> <p>It seems like a real promising toolkit for testing and reducing storage requirements for testing database automated deployment pipelines.</p> <p></p> <p></p> <p></p>","tags":["redgate","sql-server"]},{"location":"red-gate-sql-clone-1-initial-setup/#error-starting-service","title":"Error starting service","text":"<p>The Redgate SQL Clone service on Local Computer started and then stopped. Some services stop automatically if they are not in use by other services or programs.</p> <p>I wasn't using for a while due to error message I couldn't figure out. I then read through the help documentation again and found that the permissions required for the service account should be a local admin. Once I added the service account to local admins, it correctly allowed the service to start.</p> <p></p>","tags":["redgate","sql-server"]},{"location":"tfs-custom-task-service-actions-%28for-tfs-2015-update-21-or-before%29/","title":"TFS Custom Task - Service Actions (for TFS 2015 Update 2.1 or before)","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>Apparently, boolean values for custom VSTS tasks for versions prior to TFS 2015 Update 3) require some special handling as they don't pass the checkbox values as actual powershell <code>$true</code> or <code>$false</code>. Instead the task passes this information along as <code>true</code> or <code>false</code>. To properly handle this you'll need to pass in the value as a <code>string</code> then convert to <code>boolean</code>.</p> <p>I found a great start on working on this solution in a blog post by Rene which has more detail, so check it out. In addition, some reading on promiscuous types with powershell can be helpful to understand why special handling is needed with conversion. For example, in the task.json file you'll have:</p> <pre><code>    \"inputs\": [\n            {\n                \"defaultValue\": \"MyServiceName*\",\n                \"label\": \"ServiceName\",\n                \"name\": \"ServiceName\",\n                \"required\": true,\n                \"type\": \"string\"\n            },\n            {\n                \"defaultValue\": \"true\",\n                \"helpMarkDown\": \"issue restart command\",\n                \"label\": \"ChangeCredentials\",\n                \"name\": \"ChangeCredentials\",\n                \"required\": true,\n                \"type\": \"boolean\"\n            }\n</code></pre> <p>This boolean value provides a checkbox on the custom task window.</p> <p>To properly work with the boolean value, you have to bring it in as a script then convert it to a boolean value.</p> <pre><code>    param(\n             [string]$ServiceName\n            ,[string]$ServiceAccount\n            ,[string]$RestartService\n            ,[string]$StartService\n            ,[string]$StopService\n            ,[string]$ChangeCredentials\n    )\n</code></pre> <p>once you have the parameters, use .NET convert functionality to</p> <pre><code>[bool]$_RestartService    = [System.Convert]::ToBoolean($RestartService)\n[bool]$_StartService      = [System.Convert]::ToBoolean($StartService)\n[bool]$_StopService       = [System.Convert]::ToBoolean($StopService)\n[bool]$_ChangeCredentials = [System.Convert]::ToBoolean($ChangeCredentials)\n</code></pre> <p>Below I've included a custom TFS Task for basic start/stop/restart/change credentials with a custom tfs task. It's not super refined, but it's a good start to get you on your way.</p>","tags":["tfs","build-tasks","powershell","tech"]},{"location":"programming-fonts-for-the-newb/","title":"Programming Fonts For The Newb","text":"","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#the-camps","title":"the camps","text":"<p>Once you get into coding fonts, you'll find that there are two primary camps.</p> <ol> <li>Don't give a crap about it. \"I'll use defaults for everything and probably wouldn't care if I was coding in Arial\". If this is you, then this post is definitely not for you. Please continue to enjoy Comic Sans with my pity. :-)</li> <li>Font aficionados \"Your world will change forever once you use this specific font! It will increase your productivity 300%\"</li> </ol> <p>Inside the font afficiando realm, you have various subcultures.</p> <ul> <li>Fixed Font Only</li> <li>Elastic Tabstops are the future, why can't anyone get with the program? (Elastic tabtop fonts allow proportional fonts with better alignment )</li> <li>Ligature Fonts changed my world</li> </ul>","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#cool-resource","title":"cool resource","text":"<p>One really cool resource for exploring these various types of fonts is Programming Fonts - Test Drive. This is a pretty cool resource to preview various fonts and find links and resources for them.</p>","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#monospaced","title":"monospaced","text":"<p>Monospaced fonts ensure that every character take up the same amount of space regardless. This means a period takes up the same space as any other letter of the alphabet.</p> <p>The goal in recommending this for code editing has to do with the purpose of what's being written and read. In reading your eyes flow over words, and punctuation, while important, supports the words. It doesn't need to take up the same space. In code, every punctuation character is just as important as every single letter written. If you have a bunch of nested formulas for example, reading</p> <pre><code>('....Total time to process: {0:g}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('hh\\:mm\\:ss'))\n</code></pre> <p>becomes harder than ensuring all the punctuation and special characters are easily readable like this:</p> <pre><code>('....Total time to process: {0:g}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('hh\\:mm\\:ss'))\n</code></pre> <p>Visual Studio, SSMS, and other editors by default choose a monospaced font in code editing. However, there are additional options besides the built in fonts.</p>","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#some-ive-explored","title":"some i've explored","text":"<ol> <li>Bitstream Vera Sans Mono: My go to for a long time. It's aesthetically nice, and has a bit of the Ubuntu styling with some rounder edges.</li> <li>Fira Code Retina: Very nice with ligature support. This has become my current favorite due to the very nice style with the added perk of the ligatures. That's a nice little typography enhancement that really makes special combinations of characters stand out for readability. This is just a rendering feature that doesn't impact the underlying text per documentation: <p>This is just a font rendering feature: underlying code remains ASCII-compatible. This helps to read and understand code faster. FiraCode Github</p> </li> </ol>","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#what-to-what-to-look-for","title":"what to what to look for","text":"<p>As you dive into the world of exploring fonts, here's a couple things I'd look for.</p> <ol> <li>Characters that can hide problems are easily identified such as a period, or dash, most monospaced fonts are great for this, but some have smaller symbols that might make them a little less readable.</li> <li>Resizes well for your target zoom. I've tried some fonts that don't seem to look right once you change your zoom level or the size of the font. I looked up some details on this and apparently some fonts are bitmapped, and some vector images. If you are using bitmapped fonts, then the target size is ideal, while adjusting zoom level can cause blurriness or fuzzy quality as it's not going to rescale like a vector based font would. This isn't bad if you are ok with the normal font size levels.</li> </ol> <p>{{&lt; fancybox-gallery     \"fontgallery\" \"testcaption\"     \"fira-code-mono.png\"     \"source-code-pro.png\"     \"bitstream-vera-sans-mono.png\"</p> <p>}}</p> <p>So far my personal favorite is Fira Code, so check that one out if you are looking for something interesting to try.</p>","tags":["tech"]},{"location":"programming-fonts-for-the-newb/#resource-links","title":"resource links","text":"<p>FiraCode Github</p>","tags":["tech"]},{"location":"automate-windows-updates-for-development/","title":"Automate Windows Updates for Development","text":"<p>I've run into the case where I wanted updates continually applied, while the machine still was part of the GPO that didn't automatically install updates. For this developer and test oriented machine I wanted every update applied.</p> <p>I utilized a great module for this and created a script to setup the task and logging to make this an easy task.</p> <p>If you experience an issue with the WindowsUpdate Vs Microsoft update as the configured update provider, then you can just change the switch in the script for  <code>-MicrosoftUpdate</code> to  <code>-WindowsUpdate</code></p> <p>This isn't something I'd run in production, but I've found it helpful to updating a development server with the latest SQL Server updates, as well as a development machine, allowing me to keep up with any latest changes with minimal effort.</p> <p>Change the reboot parameter to your preferred option in the script. I left as <code>autoreboot</code> for the purpose of a low priority dev server being updated.</p>","tags":["powershell","tech","sql-server"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/","title":"Setting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>There are some beautiful ways to visualize time series data with the tools I'm going to go over. This post is purely focused on the initial setup and saving you some time there. In a future post, I'll show how some of these tools can help you visualize your server performance in a powerful way, including taking metrics from multiple types of servers that be working with SQL Server, and combining the metrics when appropriate to give a full picture of performance.</p> <p>A beautiful way to visualize performance across a variety of machines</p> <p>It's pretty epic to combine information across a variety of sources and be able to relate the metrics to the \"big picture\" that individual machine monitoring might fail to shed light on.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#downloading","title":"Downloading","text":"<p>I started by running this quick powershell script to download the stable toolkit.</p> <p>Once extracted, I moved the influx extracted subfolder into the InfluxDB folder to keep it clean. Now all the binaries rested in <code>C:\\Influx\\InfluxDB</code> folder with no nesting folders. I referenced the documentation for getting started with InfluxDB.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#setup-local-influxdb","title":"Setup Local InfluxDb","text":"<p>Started up the local influxdb binary.</p> <p>Initializing the new database was simple as documented: <code>create database statty</code></p> <p>{{&lt; admonition type=\"warning\" title=\"Case Sensitivity\" &gt;}} InfluxDB is case sensitive. Make sure to check your case if something isn't working, such as use \"DatabaseName\" instead of use \"databasename\" {{&lt; /admonition &gt;}}</p> <p>Also, if you get an error with access to the file, try running as admin.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#more-enviromental-variable-fun","title":"More Enviromental Variable Fun","text":"<p>A simple fix to errors related to paths and the HOME variable these tools often need, per a Github issue, was to ensure the current path was available as a variable. I did this quickly with a simple batch file to launch the consoles as well as one option, as well as updated the Start-Process script to include a statement to set the env variable for the processes being started. This eliminated the issue. For more details see github issues</p> <pre><code>SET HOME=%~dp0\nstart influxd.exe\nstart influx.exe\n</code></pre> <p>An additional snippet for launching the console version via a bat file:</p> <pre><code>set HOME=C:\\influx\ncmd /k influx.exe -host \"MyInfluxDbHost\" -database \"statty\" -precision \"s\" -format column\n</code></pre>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#quick-start-for-telegraf","title":"Quick Start for Telegraf","text":"<p>Once you have this running you can take the telegraf binaries and run them on any other server to start capturing some default preset metrics. I launched with the following script and placed this in <code>C:\\Influx</code> directory to make it easy to access for future runs.</p> <p>Edit the conf file to add some tags, change default sampling interval and more. I'll post another article about setting up telegraf to run as a service in the future so search for more info</p> <p>You can also apply the same bat file in the startup directory such as:</p> <pre><code>@REM alternative is using variable\n@REM set TELEGRAF_CONFIG_PATH=C:\\telegraf\\telegraf.conf\n\nstart %~dp0telegraf.exe -config %~dp0telegraf.conf\n</code></pre>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#run-chronograf","title":"Run Chronograf","text":"<p>One these metrics began to run, I ran Chronograf. This is Influx's alternative to Grafana, another more mature product.</p> <p>Upon loading and opening up the instance monitor, I found immediately that I was able to get some metrics from the defaults.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#get-grafana","title":"Get Grafana","text":"<p>My preferred visualization tool, this was far more robust and well documented than Chronograf which has promise, but is a relatively new project.</p> <p>When starting Grafana, you can run the following script. It creates a copy of the default ini to copy for the user to edit if not already there.</p> <p>Once you open the localhost page, if you don't see datasources in the left hand drop down, create an organization and ensure you are an admin, you'll then see the option to add datasources. I simple pointed the page to InfluxDB console running on the server I had setup previously.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev/#summary","title":"summary","text":"<p>This is just a quick guide on getting started as I found a lot of little bumps in the road since the projects are written in <code>GO</code> and not an easily run .NET project. Getting through this will hopefully give you a way to get started. I'll blog a bit more soon on visualization of the metrics captured, some custom annotations to help make metrics come alive with real-time event notifications (like \"load test started\" and \"build ended\" etc). It's a really promising solution for those who want some really nice flexibility in using perfmon and related metrics to visualize Windows and SQL Server performance.</p>","tags":["sql-server","influxdb","performance-tuning","tech","powershell","cool-tools"]},{"location":"running-influx-db-as-a-service-in-windows/","title":"Running InfluxDB as a service in Windows","text":"","tags":["influxdb","powershell","configuration","tech","sql-server"]},{"location":"running-influx-db-as-a-service-in-windows/#run-as-a-service","title":"Run as a Service","text":"<p>As part of the process to setup some metrics collections for sql-server based on perfmon counters I've been utilizing InfluxDB. Part of getting started on this is ensuring InfluxDB runs as a service instead of requiring me to launch the exe manually. For more information on InfluxDb, see my other post: Setting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev</p> <p>This of course, did not go without it's share of investigation since I'm working with a compiled executable that was originally built in <code>GO</code>. I had issues registering InfluxDB as a service. This is typically due to enviromental/path variables. In my powershell launch of <code>InfluxD.exe</code> I typically used a script like the following:</p> <p>I investigated running as a service and found a great reminder on using NSSM for this: Running Go executables ... as windows services ' Ricard Clau I went and downloaded NSSM again and first setup and register of the service went without a hitch, unlike my attempt at running <code>New-service -name 'InfluxDB' -BinaryPathName 'C:\\Influx\\influxdb\\InfluxD.exe' -DisplayName 'InfluxDB' -StartupType Automatic -Credential (get-credential)</code>. I'm pretty sure the core issue was the <code>PATH</code> variables and other related enviromental paths were not setup with \"working directory\" being the InfluxDB which would be expected by it.</p>","tags":["influxdb","powershell","configuration","tech","sql-server"]},{"location":"running-influx-db-as-a-service-in-windows/#nssm-non-sucking-service-manager","title":"NSSM - Non-Sucking Service Manager","text":"<p>Using <code>nssm install</code> provided the GUI which I used in this case. Using the following command I was able to see the steps taken to install, which would allow reproducing the install from a .bat file very easily.</p> <pre><code>set-location C:\\tools\n.\\nssm.exe dump InfluxDB\n</code></pre> <p>This resulted in the following output:</p> <pre><code>C:\\tools\\nssm.exe install InfluxDB C:\\Influx\\influxdb\\influxd.exe\nC:\\tools\\nssm.exe set InfluxDB AppDirectory C:\\Influx\\influxdb\nC:\\tools\\nssm.exe set InfluxDB AppExit Default Restart\nC:\\tools\\nssm.exe set InfluxDB AppEvents Start/Pre C:\\Influx\\influxdb\\influx.exe\nC:\\tools\\nssm.exe set InfluxDB AppEvents Start/Post C:\\Influx\\influxdb\\influx.exe\nC:\\tools\\nssm.exe set InfluxDB AppNoConsole 1\nC:\\tools\\nssm.exe set InfluxDB AppRestartDelay 60000\nC:\\tools\\nssm.exe set InfluxDB DisplayName InfluxDB\nC:\\tools\\nssm.exe set InfluxDB ObjectName SERVICENAME \"PASSWORD\"\nC:\\tools\\nssm.exe set InfluxDB Start SERVICE_AUTO_START\nC:\\tools\\nssm.exe set InfluxDB Type SERVICE_WIN32_OWN_PROCESS\n</code></pre> <p>Pretty awesome! It's a nice change to have something perfectly the first time with no issues.</p>","tags":["influxdb","powershell","configuration","tech","sql-server"]},{"location":"powershell-module-improvements-for-sql-server-in-2017/","title":"Powershell Module Improvements for SQL Server in 2017","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2018-03-19\" &gt;}} I don't use these much, if any now. Check out dbatools which is a much better module with a full range of features to save you a ton of time. {{&lt; /admonition &gt;}}</p>","tags":["powershell","sql-server","tech","dbatools"]},{"location":"powershell-module-improvements-for-sql-server-in-2017/#simple-setup","title":"simple setup","text":"<p>A major improvement that seems to have quietly slipped into the sql developers world is an improved SQLServer powershell module. The improved module is finally available in the powershell gallery, allowing a super quick setup on a server. No more installing SSMS to get them!</p> <p>This is very promising, and great if you want to leverage some of the functionality on various build servers, or other machines that might not have SSMS installed.</p> <p>Powershell Gallery - SqlServer</p>","tags":["powershell","sql-server","tech","dbatools"]},{"location":"powershell-module-improvements-for-sql-server-in-2017/#new-cmdlets","title":"new cmdlets","text":"<p>In reviewing, I ran across a few new cmdlet's as well. For instance, you could easily right click on a table and output the results into a powershell object, json, csv, gridview, or anything else you want. This is great flexibility.</p> <p></p> <p>In versions of SQL Server (as of 2012 or earlier) I believe the version SQL Server was utilizing was out of date with the installed version. For instance, on Windows Server 2012 with Powershell ISE reporting PsVersion of 4.0, Sql Server reported version 2.0 being utilized.</p> <p>In 2014 instances I had, the powershell invoked from SSMS shows the matching up to date version, which gives much better capability and functionality.</p>","tags":["powershell","sql-server","tech","dbatools"]},{"location":"powershell-module-improvements-for-sql-server-in-2017/#simple-benefits-for-the-inquiring-mind","title":"simple benefits for the inquiring mind","text":"<p>If you are not familar with the potentional benefits from being able to quickly invoke a powershell prompt and use SQL server cmdlets (prebuilt functionality that is easily called), I can give you a few use cases.</p> <p>If you were asked to run a query, then export the results to a spreadsheet, it would be relatively simple as a cut and paste. However, if you needed to loop through every table in the database, and put each one to it's own excel workbook, powershell would allow you to quickly loop, convert the datatable returned into an excel worksheet, and either append into new worksheets, or create completely seperate new files. For automation possibilities, you've got a tremendous amount of potentional time savings if you can get comfortable with powershell.</p> <p>In my case, I've found Powershell to be a great tool to help me understand more of the .NET framework as I use various cmdlets or .NET accelerators.</p>","tags":["powershell","sql-server","tech","dbatools"]},{"location":"add-user-to-admin-group-on-machine/","title":"Add User To Admin Group on Machine","text":"<p>In setting up some build machines for development, it's tedious to go and add several users to the admin group. Here's a snippet to expedite that task and help you setup more quickly.</p>","tags":["powershell","tech"]},{"location":"best-practices-defining-explicit-length-for-varchar-nvarchar/","title":"Best Practices: Defining Explicit Length for Varchar/Nvarchar","text":"<p>SA0080 : Do not use VARCHAR or NVARCHAR data types without specifying length. Level: Warning</p> <p>When using varchar/nvarchar it should be explicitly defined. This can be a very nasty bug to track down as often nothing will be thrown if not checked in an application. Instead, ensure your script explicitly defines the smallest length that fits your requirements. The reason I rate this as a very dangerous practice, is that no error is thrown. Instead, the results being returned will be shorter than expected and if validation checks aren't implemented this behavior can lead to partial results returned and used. Make sure to always explictly define length!</p> <p>Here's an short example script that demonstrates the behavior.</p>","tags":["sql-server","tech"]},{"location":"edit-in-vscode/","title":"External Tool VSCODE called from SQL Management Studio","text":"<p>Previous Related Post: Split personality text editing in SSMS with Sublime Text 3</p> <p>In this prior post I wrote about how to call Sublime Text 3 from SSMS to allow improved text manipulation to be quickly called from an active query window in SQL Management Studio. Vscode is a newer editor from Microsoft, and the argument calls took a little work to get working. Here is what I found for having your SQL file open in vscode via call from SSMS (I imagine also works in Visual Studio 2017 this way as well).</p>","tags":["sql-server","text-manipulation","vscode","tech","cool-tools"]},{"location":"edit-in-vscode/#external-tools-setup-for-vscode","title":"External Tools Setup for Vscode","text":"<pre><code>Title:  \"Edit In VSCODE\"\nCommand C:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\nArguments: --reuse-window --goto $(ItemPath):$(CurLine):$(CurCol)\n</code></pre> <p>Please note unsaved files such as \"SQLQuery11.sql\" that haven't been explictly saved are not accessible to this, so it will just open an empty file. I have not found any workaround for that, as I believe the tmp files are cached in one of the .DAT files. I've not had luck finding the Autorecover or temp files with the actual contents until saved.</p>","tags":["sql-server","text-manipulation","vscode","tech","cool-tools"]},{"location":"update-ssms-with-ps1/","title":"Update SSMS With PS1","text":"<p>{{&lt; admonition type=\"Update\" title=\"Updated: 2018-03-29\" &gt;}} Use Chocolatey. This page keeps changing it's structure, so the regex to parse for Ketarin and this PS1 script keep breaking. Updated to latest version as of 2018-03-29, but recommend checking out the Chocolately Package created for SSMS for this by flcdrg as chocolately is a much nicer way to keep up to date and more likely long-term to succeed than my gist :-) To use chocolatey (after setup), you only have to use <code>choco upgrade sql-server-management-studio</code> which is much easier to remember than using this gist. Still a nice light-weight tool.</p> <p>Also, for less overhead, investigate SQL Operations Studio instead of SSMS for those situations you need to run some queries on a machine. Less overhead, size, and complexity for some nice basic SQL Server management functionality (even if it is missing my precious SQL Prompt)</p> <p>{{&lt; /admonition &gt;}}</p> <p>With how many updates are coming out I threw together a script to parse the latest version from the webpage, and then provide a silent update and install if the installed version is out of date with the available version. To adapt for future changes, the script is easy to update. Right now it's coded to check for version 17 (SSMS 2017). I personally use Ketarin, which I wrote about before if you want a more robust solution here: Automating SSMS 2016 Updates &amp; Install</p> <p>The bat file is a simple way for someone to execute as admin.</p> <p>Hope this saves you some time. I found it helpful to keep a bunch of developers up to date with minimal effort on their part, since SSMS doesn't have auto updating capability, and thus seems to never get touched by many devs. :-) Better yet adapt to drop the SSMS Installer into a shared drive and have it check that version, so you just download from a central location.</p>","tags":["sql-server","powershell","cool-tools","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/","title":"ANTS Performance Profiler for the SQL Server Dev","text":"<p>{{&lt; admonition type=\"info\" title=\"2019-11-10\" &gt;}} Image links are gone due and due to age of post, unable to recover {{&lt; /admonition &gt;}}</p> <p>There are a few .NET tools that until recently I haven't had the chance to work with as much, specifically ANTS Memory Profiler and ANTS Performance Profiler. The memory profiler is more useful for someone focused on memory leaks which a SQL Dev isn't as focused on for performance tuning. However, there are major benefits for diving into SQL Performance tuning with ANTS Performance profiler. I think I'd say this tool makes the epic category of my #cooltools kit.</p> <p>One of the most challenging processes for profiling activity is really identifying the single largest pain point. Trying to line up timings with the SQL plans and the application side by side is a big timesaver, and Red Gate improved ANTS Performance profiler to include the executed SQL with execution plans, making it a single stop to profile and get some useful information.</p> <p>There are other ways to get useful information, such as running Brent Ozar's First Responder kit queries, Glenn Berry's diagnostic queries, Query Store, and more. These tend to focus on server performance. As someone working in software development, there is something to be said for the simplicity of running the application and profiling the .NET and SQL performance in one captured &amp; filtered result set. It's a pretty quick way to immediately reduce noise and view a complete performance picture of the application.</p> <p>For performance profiling, Visual Studio has an option called Performance Profiler. I found my initial look at it to be positive, just really noisy.</p> <p>Disclaimer: As a member of the Friends of Red Gate program, I get to try out all the cool Red Gate tools. Lucky me! This doesn't bias my reviews as I just like great tools that help me work with SQL server. This is one of them!</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#profiling-net-app","title":"Profiling .NET App","text":"<p>At the time of this articles publishing, there is no 2017 Visual studio extension which makes this process a few clicks less. For now, it still is simple. All you do is go to the bin/debug folder and select the executable you want to profile. Attaching to the .NET excecutable is required for my purpose, as attaching to an existing process doesn't give you the ability to get all the SQL calls which we definitely want.</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#timeline-bookmarks","title":"Timeline &amp; Bookmarks","text":"<p>During the profiling you can perform actions with the application and create bookmarks of points in time as you are performing these actions to make it easier to compare and review results later.</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#reviewing-results","title":"Reviewing Results","text":"<p>This is based on the call tree. It shows code calls, and is a great way to be the database guy that says... \"hey SQL server isn't slow, it's your code\" :-)</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#database-calls","title":"Database Calls","text":"<p>The database calls are my favorite part of this tool. This integration is very powerful and lets you immediately trim down to the calls made with timings and associated executed sql text. RG even went and helped us out by providing an execution plan viewer! When I first saw this I fell in love. Having had no previous experience with Entity framework of other ORMs, I found the insight into the performance and behavior of the application to be tremendously helpful the first time I launched this.</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#exporting-html-report","title":"Exporting HTML Report","text":"<p>A benefit for summarizing some action for others to consume is the ability to select the entire timeline, or narrow to a slide of time, and export the results as a HTML report.</p> <p>This was pretty helpful as it could easily provide a way to identify immediate pain points in a daily performance testing process and focus effort on the highest cost application actions, as well as database calls.</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#automation-in-profiling","title":"Automation in Profiling","text":"<p>RG Documentation shows great flexibility for the profiler being call from command line. I see a lot of potential benefit here if you want to launch a few actions systematically from your application and establish a normal performance baseline and review this report for new performance issues that seem to be arising.</p> <p>I generated some reports automatically by launching my executable via command line, profiling, and once this was completed, I was provided with a nice formatted HTML report for the calls. At the time of this article, I couldn't find any call for generating the SQL calls as their own report.</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#tldr","title":"TL;DR","text":"<p>Pros</p> <ol> <li>Incredibly powerful way to truly get a picture into an application's activity and the true pain points in performance it is experiencing. It truly helps answer the question very quickly of what is the area that needs the most attention.</li> <li>Very streamlined way to get a summary of the SQL activity an application is generating and the associated statements and execution plans for further analysis.</li> </ol> <p>Cons</p> <ol> <li>At times, with larger amounts of profiled data the application could feel unresponsive. Maybe separating some of the panes activity into asynchronous loads with progress indicators would make this feel better.</li> </ol> <p>** Neutral/Wishlist **</p> <ol> <li>More an observation than complaint, but I sure would like to see some active work being released on this with more functionality and SQL performance tuning focus. Seems to be stable and in maintenance mode rather than major enhancements being released. For those involved in software development, this tool is a powerful utility and I'd love to see more improvements being released on it. RedGate... hint hint? :-)</li> <li>I'd like to see even more automation focus, with the option of preset Powershell cmdlets, and team foundation server task integration to help identify changes in performance patterns when scaled up. Leveraging this to help baseline application performance overall and report and develop trends against this might help catch issues that crop up more quickly.</li> </ol>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"ants-performance-profiler-for-the-sql-server-dev/#additional-info-on-more-profiling-focused-apps","title":"additional info on more profiling focused apps","text":"<p>Since the material is related, I thought I'd mention a few tools I've used to help profile activity, that is not focused on a wholistic performance analysis, and more about activity.</p> <ol> <li>For more \"profiling\" and less performance analysis my favorite SQL profiling tool Devart's DbForge Sql Profiler uses extended events and while amazing, isn't as focused a tool for app and SQL performance analysis. If you haven't checked that tool (free!) out I highly recommend it vs running profiler. It uses extended events and provides a nice experience in profiling and reviewing results. Super easy to use and very flexible for filtering/sorting/exporting. The only issues I have with it are the filtering setup is annoying, but tolerable to work with, and no execution plans that I've been able to find built in, unlike running extended events in SSMS directly. Hopefully, Devart will recognize what an awesome tool they've made and continue to push it forward.</li> <li>For just getting Entity framework and other ADO.net calls you can use intellitrace with the option for ADO.NET tracing enabled. I found this nice, but a little clunky to use compared to Linq Insight or other options mentioned. It's included with visual studio so if only using periodically then this would be ok to work with.</li> <li>For a cleaner implementation of Entity Framework Profiling than the Intellitrace route use Devarts dbForge Linq Insight (I love this tool for cleaner profiling of ADO.NET activity when you aren't focused on overall performance of the application) and are working in Visual studio.</li> </ol> <p>If all else fails... you can always succumb to dark side and just use SQL Profiler or worse yet...SSMS activity monitor :-)</p>","tags":["sql-server","cool-tools","performance-tuning","redgate","powershell","tech"]},{"location":"spotify-vs-google-play/","title":"Spotify vs Google Play","text":"<p>I've recently switched over to Spotify for the nth time to explore the Spotiverse world and compare as a long time Google Play Music user. This is a big deal for me, as I was a first adopter of Google Play and have used it before All Access, and subscribed immediately to All Access once it was released. As a ravenous music consumer, I've preferred the subscription model for instant, on demand access. This is my personal comparison of some of the strengths and weaknesses of each, compiled over actual usage. Hopefully, it will help anyone who is interested in the differences and trying to decide.</p>","tags":["music","ramblings"]},{"location":"spotify-vs-google-play/#spotify","title":"Spotify","text":"<ol> <li>Playlists: A wealth of socially driven playlists exist with Spotify. You can even create a adhoc playlist on the fly via their api from your own music app. They also have collaborative lists so it's very easy to build a playlist with like minded folks.</li> <li>Crossfade: It's a pretty smooth experience to have tracks merge seamlessly. Setting 12 secs of crossfade makes it feel like a DJ is mixing up a radio experience for you.</li> <li>Radio is getting better than my first experience of it, at playing related music.</li> <li>Explicit filtering. Spotify doesn't offer as a global setting. They suprisingly seem to have ignored user requests for this. Makes it a little less family friendly if you aren't careful.</li> <li>Ability to follow artists helps with good \"new releases\" I'd be interested in.</li> </ol>","tags":["music","ramblings"]},{"location":"spotify-vs-google-play/#google-play","title":"Google Play","text":"<ol> <li>Radio is ok.</li> <li>Offline: Pinning a radio station gives you a fresh mix of songs, but not too many. Pinning a playlist in spotify for offline = every song downloaded.</li> <li>Quality designated for mobile vs wifi. Spotify just has Download vs Stream, with no designation between streaming on a wifi network and mobile network.</li> <li>Explicit filter is implemented.</li> <li>Terrible new release feed. For a while it never even updated (some bug in sub-genre at the time I believe). No way to mark uninterested, so recommendations are pretty off.</li> </ol>","tags":["music","ramblings"]},{"location":"spotify-vs-google-play/#things-i-cant-believe-arent-fixed","title":"Things I Can't Believe Aren't Fixed","text":"<ol> <li>Global Thumbs Down. Why is this not there! Spotify should add a global thumbs down, instead of just in radio stations. I should be able to dislike a song to remove it from any playlist or the recommended discovery songs. Google Play has a little better experience with this, but still far to limited.</li> </ol>","tags":["music","ramblings"]},{"location":"spotify-vs-google-play/#algorithms-taking-away-my-choice","title":"algorithms taking away my choice?","text":"<p>This seems speak to the mentality today that algorithms and user behavior drive all the results, not requiring user feedback. The negative to this is for people like myself who want to improve the results I'm offered... well we are left out in the cold. Pandora offers the best radio design out there, imo. They rely on feedback to help ensure the results are provided as the user wants. I'd like to see this design choice in more of the music services instead of them trying to assume they've identified my preferred choices.</p>","tags":["music","ramblings"]},{"location":"spotify-vs-google-play/#honorable-mention-pandora","title":"honorable mention Pandora","text":"<p>Overall, the winner for me is Spotify. However, if Pandora's new service wasn't so darn limited in selection, inconsistent with on demand availability, and implemented on desktop as well as mobile I'd recommend them. As it stands, I can't recommend it at this time.</p>","tags":["music","ramblings"]},{"location":"dataedo-first-look/","title":"Dataedo - First Look","text":"<p>{{&lt; admonition type=\"info\" title=\"update 2019-02-22\" &gt;}} Image links broken. Since their product is continually improving, I'm going to just link to their product here instead so you can review their latest demo content there. Dataedo.</p> <p>Overall, I've enjoyed the product and think it has been improved over time. There are SQL scripts for bulk updating certain values on their website that can help improve building a project for an existing database as well. {{&lt; /admonition &gt;}}</p>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#diagramming-and-documentation","title":"Diagramming and Documentation","text":"<p>Previously, I've written up on database diagrammingfor visualization of database structures. Check that out for more detailed review on what I've used.</p> <p>I've recently taken a swing at this newer tool and found it very promising as a database documentation tool that bridged the gap of usability and flexibility.</p>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#tldr","title":"TL;DR","text":"<p>Promising future tool for diagramming, and worth evaluating if looking to build out some documentation for the first time. For my purposes, it was missing some diagramming options that prevent me from leveraging as a replacement for my existing toolkit.</p>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#setting-up-initial-diagrams","title":"Setting Up Initial Diagrams","text":"<ol> <li>This process was very manual, and did not allow filtering a selection of tables and dragging or bulk added to a module/category. I'm sure this will be an improvement quick to be implemented. At the current time, it was very tedious when dealing with a large database structure.</li> <li>Once assigning a module, and then clicking to the next table, a modal pop-up would ask if I wanted to save instead of letting me continue to assign modules. To bypass this I had to hit Ctrl+S to save prior to navigating to the next table or dismiss the dialogue by clicking.</li> <li>Discovered that moving to the Module &gt; ERD tab allowed assignment of multiple tables or views to the ERD diagram. This provided the solution of easily assigning multiple objects to the ERD diagram, but did not add the tables to the Module itself, requiring the full manual step mentioned before. The filter tab was useful, though I was hoping for a basic search filter with a negation clause to help trim down the results selected. Example: CoreTables -ETL to allow easily filtering large amounts of objects. Maybe that would be a future enhancement the development team could add.</li> </ol> <p>The only difference I could see for adding tables to the ERD when adding previously to the Module was that they were highlighted in bold before the other tables</p>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#exporting-customization-is-easy","title":"Exporting Customization Is Easy","text":"<p>Exporting documentation provided immediate feedback on generating a custom template, along with all the required files. This was a definite plus over some other tools I've worked with, as it promoted the customization that would be required by some, with all the necessary files generated. My props to the developers of this, as this showed a nice touch for their technical audience, not forcing the user into a small set of options, or making it complicated to customize.</p> <p>No delete button for the CustomTemplate was a bit confusing, but an easy fix for them in the future. At this time, you'd just delete the folder in Dataedo/Templates/HTML and they won't show up in the template dialogue.</p> <p>During the export process you also have the option of saving the export command options you already setup to a dataedo command file to make it easily automated. That's a nice touch!</p>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#erd-diagrams","title":"ERD Diagrams","text":"<p>PROS</p> <ol> <li>First, the snapping and arrangement of the connectors was excellent. This allows easy rearrangement with clean relationship lines shown.</li> <li>The generated documentation files looked fantastic, and with some improvements and customization, I could see this generating a full documentation set that would make any dba proud :-)</li> </ol> <p>CONS</p> <ol> <li>I could not find any \"auto-arrange\" or \"preferred layout\" options to arrange in a set pattern if I didn't like the way I had changed it or it had laid it out initially</li> <li>No option to show the columns that have FK relationships. The relationship connector could be shown, with a label, but nothing to match Column5 that had a FK but was not part of the primary key to the matching column on another table. The diagram displayed only the PK columns. For my requirements, this was a critical omission as I need to display PK, and FK.</li> </ol>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#core-features-id-like-to-see","title":"Core Features I'd Like To See","text":"<ol> <li>Search box so that I could replace CHM files with the local HTML document. This would require a search mechanism to allow easily finding what the user needed. Currently, no provider I've tested has implemented a local html package that included a static file search that worked well.</li> <li>Improved ERD with FK/PK</li> <li>Improved ERD with auto-layout options. Based on my initial research I'd say this is a tough one to implement, but giving a basic layout option to the user and then allowing customization from there would be a great help.</li> <li>Grouping objects in the ERD to group related elements in part of a larger module</li> <li>Producivity enhancements to allow quickly creating multiple modules, and dragging objects into the modules. Eliminate manual 1 by 1 actions to work with those.</li> </ol>","tags":["sql-server","cool-tools","tech"]},{"location":"dataedo-first-look/#end-comments","title":"end comments","text":"<p>Well done Dataedo team :-) Looking forward to the continued improvements. I've found visualization of database structures is very helpful to design, and a new toolkit out like yours promises to provide even more great tools to use to do this.</p>","tags":["sql-server","cool-tools","tech"]},{"location":"influx-db-and-annotations/","title":"InfluxDB and Annotations","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated: 2020-04-29\" &gt;}} broken image links removed {{&lt; /admonition &gt;}}</p> <p>This post assumes you've already setup InfluxDB and have Grafana running.</p>","tags":["time-series","powershell","influxdb","monitoring","sql-server","cool-tools"]},{"location":"influx-db-and-annotations/#inserting-annotations","title":"Inserting annotations","text":"<p>Annotations are not a special type of resource, instead it's just another metric that you query with a feature in Grafana to display on other metrics. This means the same insert Line Protocol applies to the Annotation.</p> <p>This post on maxchadwick.xyz greatly helped me get started: Creating Grafana Annotations with InfluxDb Max Chadwick</p> <p>Per Max's original post it supports html as well, so you could link for example to a build, test result, or anything else you want to link to from your performance statistics.</p> <p>This provides an annotation on your timeline in a nice format for browsing through the timeline. I can see usage cases for identifying specific activity or progress in tests, helping coorelate the performance metrics with known activity steps from a build, script, or other related tasks. You could have an type of activity trigger this powershell insert, providing a lot of flexibility to help relate useful metrics to your monitoring.</p> <p>My personal use case has been to ensure load testing start/end times and other significant points of time in a test are easily visible in the same timeline I'm reviewing metrics on.</p> <p>Warning: I did experience performance degradation with Grafana and many annotations on a timeline. I found just disabling the annotations kept this from occurring, so you only pull them when youd them.</p>","tags":["time-series","powershell","influxdb","monitoring","sql-server","cool-tools"]},{"location":"influx-db-and-annotations/#adding-annotations-to-grafana","title":"Adding Annotations to Grafana","text":"<p>Now that you have the results being inserted into InfluxDB, you can query these in Grafana as annonations to overlay your graphs.</p>","tags":["time-series","powershell","influxdb","monitoring","sql-server","cool-tools"]},{"location":"influx-db-and-annotations/#potential-uses","title":"Potential Uses","text":"<p>I could see a whole lot of uses for this!</p> <ul> <li>insert at build related activity</li> <li>Windows update</li> <li>Specific Database Related Maintenance like Ola Hallengren's index optimize or database integrity check</li> </ul> <p>Monitoring always loses it's value when you have a limited picture of what is happening. Triggering relevant details for stuff that might help analyze activity might be the key to immediately gaining an understanding on what is causing a spike of activity, or of better evaluating the timeline of a load test.</p>","tags":["time-series","powershell","influxdb","monitoring","sql-server","cool-tools"]},{"location":"capturing-perfmon-counters-with-telegraf/","title":"Capturing Perfmon Counters With Telegraf","text":"","tags":["sql-server","monitoring","grafana","influxdb","cool-tools","powershell"]},{"location":"capturing-perfmon-counters-with-telegraf/#setting-up-telegraf-to-capture-metrics","title":"Setting up Telegraf to Capture Metrics","text":"<p>I had a lot of issues with getting the GO enviroment setup in windows, this time and previous times. For using telegraf, I'd honestly recommend just leveraging the compiled binary provided.</p> <p>Once downloaded, generate a new config file by running the first command and then the next to install as service. (I tried doing through NSSM originally and it failed to work with telegraf fyi)</p> <p>Once this service was setup and credentials entered, it's ready to run as a service in the background, sending whatever you've configured to the destination of choice.</p> <p>In my test in Amazon Web Services, using EC2 with Windows Server 2016, I had no issues once EC2 issues were resolved to allow the services to start sending their metrics and show me the load being experienced across all in Grafana.</p>","tags":["sql-server","monitoring","grafana","influxdb","cool-tools","powershell"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/","title":"SqlServer Powershell Cmdlets 2017 - Initialize Look","text":"<p>Diving into the Sql Server Management Objects library can be a pretty interesting process. You get to work with database objects as in a new way, and begin manipulating and execute code in a much different approach than purely using T-SQL. Powershell offers a unique way to interact with prebuilt cmdlets, and you can explore leveraging .NET in powershell as well to have a powerful toolkit of options. This post is a not focused on a full walk-through, but instead to communicate some of the exploration I've done, to help if you are beginning to explore more database automation and management. I plan on doing some basic walk-throughs for the powershell newbie in the future, so if you are confused about anything powershell related feel free to post a comment and I'll add it to my list of stuff to walk through.</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#cmdlets-vs-net-approach","title":"cmdlets vs .NET approach","text":"<p>What I've found interesting is there are really 2 main approaches to interacting with SQL Server. You can directly invoke the SMO dlls and access the methods, properties, and extensibility this offers. This requires more .NET knowledge as you would be directly working with the SMO namespace, in a way that is almost the same as what you code in C#. The other approach is to leverage cmdlets. The cmdlets try to abstract away a lot of the complexities that working directly with the SMO namespace for ease of use and automation, and to simplify the process for those not as comfortable with coding in C# or directly leverage the SMO namespace in C#</p> <p>If purely focused on automation and little experience working with .NET then cmdlet's will be by far the way to go. There is a serious learning curve in working with .NET directly vs prebuilt cmdlets. If desiring to expand your .NET knowledge, as well find that the prebuilt cmdlets don't offer the behavior you are trying to achieve, then exploring the SMO namespace for directly invoking the methods and accessing properties can be valuable. The learning curve is more intense, so just be prepared for that if you are new to working with .NET directly in Powershell.</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#dbatoolsio-other-sources","title":"dbatools.io &amp; other sources","text":"<p>When possible, I personally am going to recommend to leverage a package like dbatools instead of rolling your own. Dbatools.io is a powerful project that I've recently begun to explore more. This well rounded package gives you a powerful powershell set of commands that can help you set server properties, obtain default paths, backup, restore, migrate entire sets of databases to a new location and more. To code all of this from scratch would be a massive project. I'd recommend considering dbatools.io and just getting involved in that project if you have something to contribute. I found it really helpful to quickly setup some default server options without having to configure manually myself.</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#exploring-sql-path-provider","title":"Exploring SQL Path Provider","text":"<p>Trying to find the path initially can be challenging. However, by opening SSMS up, right clicking, and launching the powershell window you'll be able to easily find the correct path to get the server level object. This allows you to leverage default methods in powershell like Get-ChildItem for iterating through objects. It treats the navigated SQL server path basically as a \"file structure\" allowing some interesting actions to be performed. One of these is a different approach to killing connections to a particular database. I found this great pointer by reading Killing SPIDS in Powershell from MidnightDBA</p> <p>Review that article for scripts focused on the termination of running spids. For an adhoc purpose the scripts MidnightDba provided are excellent and would allow quickly executing a kill script on connections from ssms &gt; powershell prompt.</p> <pre><code>import-module -name sqlserver -disablenamechecking -verbose:$false -debug:$false\nCD SQLSERVER:\\SQL\\$ServerName -Verbose:$false -Debug:$false\ndir ' ?{$_.Name -eq \"$DatabaseName\"} ' %{$_.KillAllProcesses($DatabaseName)}\n</code></pre> <p>I approach this with a different method in one final script using just the SMO server method KillAllProcesses. For some tasks I've found it really helpful to have a simple 1 line kill statement thanks to MidnightDba's pointer with the statements similar to the one above. Using Microsoft's documented method shows another example of how to use to restart the service. This was one modified approach I took. I prefer not to use this type of approach as working with <code>get-childitem</code> with server objects to me as a little unintuitive.</p> <pre><code>&lt;#\n        .LINK https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/start-stop-pause-resume-restart-sql-server-services#PowerShellProcedure\n#&gt;\n$ErrorActionPreference = 'continue'\nif(-not (Get-InstalledModule SqlServer)) { install-package SqlServer -scope CurrentUser -verbose:$false -Force}\n\n[datetime]$StepTimer = [datetime]::Now\n\n$private:ServerName = $env:ServerName\nimport-module -name sqlserver -disablenamechecking -verbose:$false -debug:$false\n\n# Get a reference to the ManagedComputer class.\nCD SQLSERVER:\\SQL\\$private:ServerName -Verbose:$false -Debug:$false\n$Wmi = (get-item -debug:$false -verbose:$false .).ManagedComputer\n$DfltInstance = $Wmi.Services['MSSQLSERVER']\n\n#Display the state of the service.\nwrite-host \"Stopping Instance: $($DfltInstance.ServiceState.value__)\"\n$DfltInstance.Stop()\n\nwhile($DfltInstance.ServiceState.value__ -ne 1) #1 stopped\n{\n    Start-Sleep -seconds 5\n    $DfltInstance.Refresh()\n    write-host \"... state: $($DfltInstance.ServiceState)\"\n}\n## Start the service.\n$DfltInstance.Refresh()\nwrite-host \"Current Service State: $($DfltInstance.ServiceState)\"\nwrite-host \"Initiating Service Start\"\n$DfltInstance.Start()\n\nwhile($DfltInstance.ServiceState.value__ -ne 4) #4 running\n{\n    Start-Sleep -seconds 5\n    $DfltInstance.Start()\n    $DfltInstance.Refresh()\n    write-host \"... state: $($DfltInstance.ServiceState)\"\n}\nwrite-host( \"{0:hh\\:mm\\:ss\\.fff} {1}: finished\" -f [timespan]::FromMilliseconds(((Get-Date)-$StepTimer).TotalMilliseconds),'SQL Service Restart')\n</code></pre>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#database-as-an-object","title":"Database as an Object","text":"<p>Getting the database as an object proved to be easy though, if a little confusing to navigate initially. $s = SqlServer\\Get-SqlDatabase -ServerInstance $ServerInstance -Verbose</p> <p>Once the object is obtained, you can begin scripting objects, change database properties and more very easily. I found this method an interesting alternative to invoking using .NET accelerators as it was a quick way to easily get a database level object to work with. However, some of the limitations of not having the server level object immediately available made me end up preferring the .NET accelerator version which could look like this.</p> <pre><code>param(\n        $ServerName = 'localhost'\n        ,$DatabaseName = 'tempdb'\n)\n$s = [Microsoft.SqlServer.Management.Smo.Server]::New($ServerName)\n$d = [Microsoft.SqlServer.Management.Smo.Database]::New($s, $DatabaseName)\n$s.EnumProcesses() ' format-table -AutoSize\n$d.EnumObjects() ' Out-GridView\n</code></pre> <p>Interestingly, to actually access the many of the database properties you actually would call it via reference to the server object with SMO calls instead of the cmdlet. Trying $d.PrimaryFilePath doesn't work as I believe it's initiating the instance of a new database object for creation instead of referencing the initialization of a new object to an existing database. I found documentation a bit challenging to immediately sift through to get an answer, so YMMV. Someone coming from a .NET focused background might find the process a little more clear, but for me it did take some work to correctly identify the behavior.</p> <pre><code>#doesn't work.\nProbably trying to initialize new object for creating a db\n$d = [Microsoft.SqlServer.Management.Smo.Database]::New($s, $db)\n$d.PrimaryFilePath\n\n#works to access current existing object\n$s.Databases[$db].PrimaryFilePath\n</code></pre>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#exploring-properties","title":"Exploring Properties","text":"<p>If you want to explore properties of an object, try using the ever faithful get-member</p> <p>Depending on the type of object, you can additionally explore them with GetEnumerator, GetProperties, etc. You'll find intellisense helpful as you explore more. For instance, here's a walkthrough on the various ways you might explore the object and find you need to dig into it to get the full detail of what you have access to.</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#comparing-restoring-a-database-with-cmdlet-vs-smo","title":"Comparing Restoring a Database with Cmdlet vs SMO","text":"","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#using-dbatools-cmdlet","title":"using dbatools cmdlet","text":"<p>An example of how simple using dbatools cmdlet can make restoring a database copy</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"exploring-sql-server-with-powershell-and-smo-basics/#rolling-your-own-wheel","title":"rolling your own wheel","text":"<p>Now compare this to the complexity of running your own invocation of the SMO namespace and requires a lot more coding. Since dbatools wraps up a lot of the functionality, I've actually migrated to leveraging this toolkit for these dba related tasks instead of trying to reinvent the wheel.</p>","tags":["sql-server","powershell","smo","tech","dbatools"]},{"location":"fixing-virtual-box-only-showing-32bit-windows-os-options/","title":"Fixing VirtualBox Only Showing 32bit Windows OS Options","text":"<p>Original help was identified from this article Why is VirtualBox only showing 32 bit guest versions on my 64 bit host OS?</p> <p>In browsing through the comments, I saw mention that the root issue is that Hypervisor running interferes with Virtualboxes virtual management, so I disabled Hypervisor service, repaired the install, and rebooted. I also disabled automatic start for Hypervisor. This resolved the issue without requiring the uninstallation of the Hypervisor feature in Windows.</p>","tags":["troubleshooting","virtualization","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/","title":"Ultrawide Monitor - Samsung CF971 - First impressions","text":"<p>As a developer, I've been a big fan of multiple monitors for a long time. I moved to a triple screen setup years ago, and up until recently, had no desire to try anything different.</p> <p>Well... I've finally found the replacement. The holy grail of productivity, immersion, as well as usability. Ultrawides :-)</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#dont-you-miss-the-extra-screen","title":"Don't you miss the extra screen?","text":"<p>No. The width of the screen is plenty for 2 full Visual Studio environments, or 3 text editors side by side. I've always had issues with effectively managing multiple IDE environmen with floating windows as those can tend to get pretty confusing on which parent they belong to. I've found I prefer to keep floating windows in the parent environment, and an ultrawide allows this type of workflow perfectly.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#do-you-need-the-curve","title":"Do you need the curve?","text":"<p>I prefer it.the Samsung CF971 is curved at 1500R, which is one of the most curve displays as of the current time. Unlike TV, where this feature is pretty much useless, with an ultrawide monitor only a few feet from your face, the curve is welcome, and still subtle. I do videography, photography, and some gaming, and for my purposes I've found it to be perfect.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#do-i-need-to-get-a-3440x1440-or-can-i-save-money-and-get-2560x1080","title":"Do I need to get a 3440x1440 or can I save money and get 2560x1080?","text":"<p>Get the 3440x1440 if you are doing anything besides gaming. Then for gaming just toggle to the lower resolution. I way overthought this too, and thought the downscaling would be terrible. It's not. Don't listen to the naysayers. If you aren't a competitive ESports gamer, then it will serve you just fine at the lower resolution, and then you can toggle back up to the high resolution for anytype of productivity work. I recommend Display Fusion as you can setup some quick monitor profiles to toggle without delay.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#gaming-on-it","title":"Gaming on it?","text":"<p>Awesome! I still am running an old AMD 7950, and when I down the settings I'm still enjoying it. It's about time with Vega to upgrade, but until then, I'm still getting some life out of the current card, and still not leveraging the 100hz refresh rate with Freesync.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#any-negatives","title":"Any negatives?","text":"<p>I had 1 stuck pixel that I couldn't fix, so with black background this small dot can be seen (red). However, I'm still happy with the choice.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#for-development","title":"For Development","text":"<p>I plan on moving to this screen in the future for development as well. I included some images of the workspace you gain, so I have no complaints about my time at home working on it.</p>","tags":["gear","ramblings","tech"]},{"location":"ultrawide-monitor-samsung-cf971-first-impressions/#but-im-scared-of-moving-away-from-ips","title":"But I'm scared of moving away from IPS!","text":"<p>Me too. No regrets. The SVA panel beautiful. No complaints whatsoever, and no regrets about leaving behind my IPS display for this.</p> <p>{{&lt; fancybox-gallery     \"gallery1\"     \"Samsung CF971 Gallery\"     \"20170708+1304+2352134.jpg\"     \"20170708+1429+2352140.jpg\"     \"20170708+1445+2352144.jpg\"     \"20170708+1446+2352147.jpg\"     \"2017-07-08_14-52-53.png\"     \"2017-07-08_16-34-48.png\"     \"2017-07-08_14-51-59.png\"     \"2017-07-08_16-04-20.png\"     \"2017-07-08_15-16-31.png\"     \"2017-07-08_16-03-25.png\"     \"2017-07-20_12-45-12.png\"</p> <p>}}</p>","tags":["gear","ramblings","tech"]},{"location":"a-not-so-waterlogged-texan-tuesday/","title":"A Not-So Waterlogged Texan - Tuesday","text":"<p>{{&lt; admonition type=\"warning\" title=\"Updated: 2017-08 1:30pm Wednesday\" &gt;}} Better than expected, we are looking to be back in our house Thursday. Apparently some SUV's are just starting to gain access. Don't want to risk it so giving it a little extra time. That's much better than I expected at beginning of this whole situation!</p> <p>Relief efforts mentioned at bottom are underway at nwvineyard.com so if you are looking to help the Tomball/Northwest Houston area that's one place to help or find out how to help. {{&lt; /admonition &gt;}}</p>","tags":["ramblings","flood"]},{"location":"a-not-so-waterlogged-texan-tuesday/#original-post","title":"Original Post","text":"<p>From my favorite \"No Hype\" weatherman:</p> <p>We are probably about 24 hours away from a general cessation of rainfall, including for most of the eastern half of the Houston area. It's been an absolutely miserable four or five days, but it's almost over - &gt; Space City Weater</p> <p>Almost through it!</p> <p>Message from neighbor reports that everything seems high and dry still after the rains last night, so pretty sure my house, along with a few of the houses high enough on our street are not going to have water enter. If the water keeps going down, I'm hoping to have the family back in the house Friday or Saturday.</p> <p></p> <p>There will definitely be people in our neighborhood that will be dealing with repairs and issues as the video I posted previously shows.</p> <p>Walked over to the church today from our friends house. Figured I'd get some work done, and was doubtful of my ability to concentrate with my kiddos getting stir crazy. The walk was supposed to be 37 mins... an hour later I arrived. Gave me some extra time to pray at least :-)</p> <p></p> <p>Projections for the area are never certain, but seems to point to being clear of anything that would cause water to get into the house.</p> <p></p> <p>For those interested in helping out in the area, my church is participating in relief efforts and starting to gather resources to help those in the community. Randy (the lead pastor at nwvineyard.com) will make sure that any donations made are used to impact the local area as this is a core passion of our small church plant.</p> <p>... ** video no longer available on facebook - sorry **</p>","tags":["ramblings","flood"]},{"location":"a-waterlogged-texan-reporting-in-from-northpoint-tomball/","title":"A Waterlogged Texan Reporting In from Northpoint Tomball","text":"","tags":["ramblings","family","follower-of-Jesus"]},{"location":"a-waterlogged-texan-reporting-in-from-northpoint-tomball/#waterlogged","title":"Waterlogged","text":"<p>What a crazy weekend. Choose to evacuate the family as water was slowly creeping up closer to the house. I was ready today when I made it back to the house to see the water in the house, but to my astonishment it was only half-way up the driveway. Turning down the road 1 house away, many houses did not fare so well, and water looks to have made it in many homes.</p> <p></p> <p>It has been a stressful time, and still not finished, but what a remarkable time to see people coming together. From a neighbor walking my family in a small boat to the front to ease the way for the kids and pregant wife in 4 ft of dirty water to the evacuation boats running all day to help get people out. I saw first responders from a surfer walking a raft in his wetsuit to Kemah volunteer firefighters walking the roads to check on people and confirm everyone wanting evacuation was taken care of.</p> <p>Our area was not as devastated as downtown and other areas that have been on the news, but for us it was still a pretty nerveracking experience.</p> <p>The hospitality and friendship shown by my church was encouraging, and without these friends we'd have been in a tough spot. From driving through some deeper water, to providing warm meals and hospitality (and giving up some rooms/space) for us. As it stands, thanks to my friends I've got my kids tucked in warm and safe and am able to sit back and think about the days' events without being in the rain. Being removed for our home for a week helps put the important things back in perspective for sure!</p> <p></p> <p>If we make it through tonight we should be fine with no water damage, but this storm has definitely given us surprises.</p> <p>Prayers out there for those in worse situations from this storm. It's been a bit numbing to see just the little pieces I've seen in the last 2 days. The financial impact (no flood insurance for many in this storm, including us, as we are outside any normal flood areas) is going to be tough, along with the stress and rebuilding that will be required.</p>","tags":["ramblings","family","follower-of-Jesus"]},{"location":"recovery-is-a-marathon%2C-not-a-sprint-hurricane-harvey/","title":"Recovery is a Marathon, Not a Sprint - Hurricane Harvey","text":"<p>It's important to remember that the road to recovery is not an instant process. Right now we have a ton of volunteers and people working to help those in need. I'm reminding myself, as much as any readers here, the process to recovery for people most impacted is not a single week of effort event. In some cases, entire homes were lost and without flood insurance, the financial devastation can be profound.</p> <p></p> <p>I'm proud to be part of the Vineyard Church association which is sending an equipped response team to the area to send volunteers and various professionals to aid in recovery. This will definitely help increase the impact we have as people have to go back to work and available time is reduced.</p>","tags":["ramblings","flood","follower-of-Jesus"]},{"location":"recovery-is-a-marathon%2C-not-a-sprint-hurricane-harvey/#continued-help","title":"continued help","text":"<p>In the meantime, if you are helping with some giving @ nwvineyard.com/give please consider doing a recoccuring donation for a period of months to continue the help. I'm hoping that the impact the church community has in Houston is going to be a testimony to commitment as believers we have in serving those in suffering and in need.</p>","tags":["ramblings","flood","follower-of-Jesus"]},{"location":"recovery-is-a-marathon%2C-not-a-sprint-hurricane-harvey/#not-be-forgetful","title":"not be forgetful","text":"<p>As a reminder to all my fellow followers-of-Christ, our actions are under scrutinity (just see the barrage of accusations against Joel Olsteen right now). As I was driving to a site on the first day back in my house, I saw a small church with a guy riding a lawnmower and trimming a lawn that already looked good.</p> <p>It made me angry and sad at the same time.</p> <p>Yes, I should not judge. That church may have had a large response team out serving, and I'm probably just seeing part of the picture.</p> <p>However, that same part of the picture is what others would be seeing. Less than 1 mile up the road were houses devasted by the flood, ripping out drywall, furniture, and dealing with the loss of a lot of belongings. I pray that we all remember the hardship that others are going through and while we can enjoy and relish the moments of life we are given, let us strive to not forget those others those can't move on yet. Those who have lost everything, are in shelters, dealing with the emotional fallout of a disaster, and in the process of rebuilding a place they can call home for their family.</p>","tags":["ramblings","flood","follower-of-Jesus"]},{"location":"welcome-to-the-world-little-ella/","title":"Welcome to the World Little Ella","text":"<p>Proud to announce Ella the newest addition to my family joined us today @ 6:19pm, weighing in at 6lbs and 11 ounces. Yes, that's 6lbs of cuteness. :-) I'm incredibly blessed to have this third addition to my family, and look forward to treasuring every moment with her. God sure knows how to give an amazing gift!</p> <p>It was a rough 24 hours resulting in our first cesarean experience. This was a bit intense, but Sarah pulled through as a trooper, and we are now finished with post-operation recovery, and enjoying time with our new bundle of awesome.</p> <p>{{&lt; fancybox-gallery     \"GalleryElla\"     \"Welcoming Ella\"     \"Ella_18.jpg\"     \"Ella_17.jpg\"     \"Ella_16.jpg\"     \"Ella_15.jpg\"     \"Ella_14.jpg\"     \"Ella_12.jpg\"     \"Ella_13.jpg\"     \"Ella_11.jpg\"     \"Ella_10.jpg\"     \"Ella_9.jpg\"     \"Ella_8.jpg\"     \"Ella_7.jpg\"     \"Ella_6.jpg\"     \"Ella_4.jpg\"     \"Ella_5.jpg\"     \"Ella_2.jpg\"     \"Ella_3.jpg\"     \"Ella_1.jpg\"</p> <p>}}</p> <p>Thanks for all the prayers! Now for some sleep... or not ;-)</p> <p>Shout out to Willowbrook Women's Center for their consideration and stellar care during the delivery. I left my camera in the other room, not knowing operating room rules, and wanted to respect that, and not complicate the moment. A nurse asked me where my camera was so I could capture a few of the precious first moments, and walked to the other side of the hospital section to go fetch it for me to make it happen. That's how considerate they were!</p> <p>Thanks to their care I have a healthy baby girl, healthy wife, and some beautiful moments despite the complication of the delivery.</p> <p>{{&lt; youtube vB1EqN-WfkI &gt;}}</p>","tags":["family","photography","follower-of-Jesus","ramblings"]},{"location":"origami-101/","title":"Origami 101","text":"<p>My son and i have begun exploring origami.</p> <p>Few things I've learned....</p> <ul> <li> <p>Patience is key</p> </li> <li> <p>The right paper matters for trying intermediate models with lots of folds or 3d models needing some shape control. I wasted nearly 2 hours on one complex design with 2 hours of work left... And had to stop as the paper was almost splitting from the work I had put into it.</p> </li> <li> <p>Advanced origami is insanely complex, and only in recent years morphed into a realistic portrayal of objects due to advances in mathematics in origami pioneers. (See Robert Lang, Eric Joisel for examples)</p> </li> <li> <p>Advanced origami practitioners typically are wet folding paper to allow shaping flexibility</p> </li> </ul> <p>This is a fun start but definitely have a long way to go! I look forward to trying more soon, as it's been a great way to spend some relaxing time and more so to have a great connection with my son.</p>"},{"location":"delight-in-the-little-moments/","title":"Delight in the Little Moments","text":"<p>This last Easter I had some fun with the kids at their grandparents. Remember the days of spinning until you were nauseous? Was fun to watch the kids enjoying this so I took a few clips, nothing fancy, just a snapshot of enjoying the little moments in life.</p> <p>Thinking I'll try this approach for a while. Small 2 minute videos that aren't refined a lot are easy for me to produce. I typically only do a couple a year because I tend to work on them extensively, but maybe just like blogging, consistency and simplicity can be the key to keeping the momentum going.</p> <p>{{&lt; youtube H6hf6rna6gk &gt;}}</p>","tags":["ramblings","videography","family","follower-of-Jesus"]},{"location":"time-with-family-thanksgiving/","title":"Time With Family - Thanksgiving","text":"<p>I've made it my personal quest as a creative who has a passion for videography to try and capture simple moments during my life with family. There are plenty of epic indie films, and I can't commit this type of time with a day job, family, and other interests. I think this is a good start. Thanks to Robert &amp; Steph for their hospitality and making this a special thanksgiving. Was a wonderful time with family</p>","tags":["family","ramblings"]},{"location":"migration-to-jekyll/","title":"Migration To Jekyll","text":"<p>I've been in the process of migrating my site to it's final home (as far as my inner geek can be satisfied staying with one platform)... Jekyll.</p>","tags":["tech","jekyll"]},{"location":"migration-to-jekyll/#jekyll","title":"Jekyll","text":"<p>Jekyll is a static website generator that takes plain markdown files and runs through through files that are basically templates for the end html content, allowing flexibility in content generation. The result ends up being a static website with beautifully generated typography, search, pagination, and other great features for a blogging engine. You also keep the benefit of writing in our beloved markdown, allowing easy source controlling of your blog.</p> <p>This site at this time basically is a github repo. Upon commit, Netlify provides an amazing free resource for developers to automatically launch a remote build, minify, ensure content is with their CDN and publishes the changes to your site upon successful build. Pretty amazing! They also have more flexibility than Github-Pages in that you can use other Ruby based plugins for Jekyll, while Github limits the plugins available, resulting in less features in Jekyll that are available.</p>","tags":["tech","jekyll"]},{"location":"migration-to-jekyll/#worth-it","title":"Worth It?","text":"<p>This was a pretty exhaustive migration process, primarily because I worked on ensuring all links were correctly remapped, features like tag pages were in place, and all assets were migrated from Cloudinary and other locations. Overall it was a very time consuming affair but considering free hosting that will scale for any load required vs \\$144 at squarespace, I think it's a win. In addition, no MySQL databases to manage, Apache webservers to maintain, PHP editions to troubleshoot.... well that sold me.</p>","tags":["tech","jekyll"]},{"location":"migration-to-jekyll/#using-fuzzy-string-matching-to-fix-urls","title":"Using Fuzzy String Matching To Fix Urls","text":"","tags":["tech","jekyll"]},{"location":"migration-to-jekyll/#resources","title":"resources","text":"<ul> <li>PowerShell: Check List of Urls and Retrieve Status Codes</li> <li>PowerShell: Generate CSV from Sitemap.xml</li> </ul>","tags":["tech","jekyll"]},{"location":"migration-to-jekyll/#matching-urls","title":"matching urls","text":"<p>I noticed a lot of broken url's when migrating my site, so I got a list of url's and wanted to compare the old broken urls against a list of current url's and do a match to find the best resulting match. For instance, with the Jekyll title generating the link, I had issues with a url change like this:</p> <p>I generated a sitemap csv by using ConvertFrom-Sitemap, originally written by Michael Hompus at TechCenter.</p> Original New <code>https://www.sheldonhull.com/blog/syncovery-arq</code> <code>https://www.sheldonhull.com/blog/syncovery-&amp;-arq-syncing-&amp;-backup</code> <p>What I wanted was a way to do a fuzzy match on the url to give me the best guess match, even if a few characters were different... and I did not want to write this from scratch in the time I have.</p> <p>I found a reference to a great library called Communary.PASM and in PowerShell ran the install command: <code>Install-Package Communary.PASM -scope currentuser</code></p> <p>The resulting adhoc script I created:</p> <p>The resulting matches were helpful in saving me a lot of time, finding partial matches when a few characters were off.</p> Original FuzzyMatched /blog/transaction-logging-recovery-101 /blog/transaction-logging-&amp;-recovery-(101) /blog/transaction-logging-recovery-part-2 /blog/transaction-logging-&amp;-recovery-(part-2) /blog/transaction-logging-&amp;-recovery-(part-3) /blog/transaction-logging-recovery-part-3 <p>I then tested out another several algorithms. I had come across references to the Levenshtein algorithm when reading about string matching on Stack Overflow. I added that logic into my script, and watched paint dry while it ran. It wasn't a good fit for my basic string matching. Learning more about string matching sounds interesting though as it seems to be a common occurrence in development, and I'm all for anything that lets me write less regex :-)</p> <p>For my rough purposes the Fuzzy match was the best fit, as most of the title was the same, just typically missing the end, or slight variance in the delimiter.</p> <p>I had other manual cleanup to do, but still, it was an interesting experiment. Gave me an appreciation for a consistent url naming schema, as migration to a new engine can be made very painful by changes in the naming of the posts. After some more digging, I decided to not worry about older post urls and mostly just focused on migrating any comments. I think the most interesting part of this was learning a little about the various string matching algorithm's out there... It's got my inner data geek interested in learning more on this.</p>","tags":["tech","jekyll"]},{"location":"git-cracking/","title":"Git Cracking","text":"<p>{{&lt; admonition type=\"info\" title=\"Resources\" &gt;}} - GitKraken - Source Tree - Posh-Git - Cmder {{&lt; /admonition &gt;}}</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#git-some-pain","title":"Git Some Pain","text":"<p>Having come from a Team Foundation Server background, I found Git to be a bit confusing. The problem is primarily the big difference in a distributed version control system vs non-distributed. In addition to that complexity the terminology is not exactly intuitive. A lot of phrases like <code>PULL</code> have different results depending on what step you are in.</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#heres-your-sign","title":"Here's Your Sign","text":"<p>Here's my version of \"Here's Your Sign\" For Newbie Git Users That Are Coming from TFS Background</p> <p>You Must a TFS'er using Git when...</p> <ul> <li>You commit changes, and refresh the TFS Source Control Server website trying to see your changes... but nothing ... ever... changes.</li> <li>You pull changes to get things locally, but then get confused about why you are submitting a <code>pull request</code> to give someone else changes?</li> <li>You want to use a GUI</li> <li>You use force options often because: 1) You are used to forcing <code>Get Latest</code> to fix esoteric issues 2) Force makes things work better in TFS (no comment)</li> <li>You are googling ways to forcibly reset your respository to one version because you don't know what the heck is out of sync and are tired of merging your own mistakes.</li> <li>You think branching is a big deal</li> <li>You think it's magical that you can download a Git repo onto a phone, edit, commit, and all without a Visual Studio Installation taking up half your lifespan.</li> </ul> <p>I claim I'm innocent of any of those transgressions. And yes, I use command line through Cmder to get pretend some geek cred, then I go back to my GUI. :-) I have more to learn before I become a Git command line pro. I need pictures.</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#the-key-difference-from-tfs","title":"The Key Difference From TFS","text":"<p>The biggest difference to wrap my head around, was that I was working with a DVCS (Distributed Version Control System). This is a whole different approach than TFS, though they have many overlaps. I won't go into the pros/cons list in detail but here's the basics I've pulled (pun intended) from this.</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#pros","title":"Pros","text":"<ul> <li>I can save my work constantly in a local commit before I need to send remotely (almost like if I did shelves for each piece of work, and finally when <code>pushing</code> to server I'd be sending all my work with history/combined history)</li> <li>File Based Workspace. Local Workspaces in TFS have benefit of recognizing additions and other changes, but it's tedious to do. Git makes this much cleaner.</li> <li>Branching! Wow. This is the best. I honestly don't mess around with branching in TFS. It has more overhead from what I've seen, and is not some lightweight process that's constantly used for experimentation. (Comment if you feel differently, I'm not a pro at TFS branching). With Git, I finally realized that instead of sitting on work that was in progress and might break something I could branch, experiment and either merge or discard all very easily. This is probably my favorite thing. I'll be using this a lot more.</li> </ul>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#cons","title":"Cons","text":"<ul> <li>The wording.</li> <li>More complicated merging and branching seem a little more complex with DVCS than non distributed like TFS, but that's just my high level impression. YMMV</li> </ul>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#gitkraken","title":"GitKraken","text":"<p>GitKraken, a Git GUI to solve your learning woes.</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#git-gui-goodness","title":"Git GUI Goodness","text":"<p>I'm a Powershell prompt addict. I prefer command line when possible. However, I think GitKraken helped make this process a bit easier for me. I was using <code>posh-git</code> and Cmder initially, then Vscode with GitLens. However, other than basic commit/pull, I've found myself relying on GitKraken a lot more, as it's just fast, intuitive and easier to understand with my addled brain. I'd rather leave energy for figuring out how to get Query Optimization Through Minification</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#timeline","title":"Timeline","text":"<p>To be honest, their timeline view and the navigation and staging of the changes seemed pretty intuitive to me compared to what I'd seen in other tools. Overall, I found it easier to wrap my head around the concepts of Git with it, and less fear of merging changes from remote as I was able to easily review and accept changes through it's built in merging tool.</p> <p></p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#overall-impression","title":"Overall Impression","text":"<p>Overall impression is positive. I'd say it's a nice solution to help with understanding and getting up and running faster than some other solutions, or using Git via command line along. While that's a worthy goal, being able to easily review changes, amend commits, pull and merge remote changes from multiple sources, and other things, I'm not sure a newbie could do all at any time near what a little effort in GitKraken would provide. So overall, it's a win. I've used it for this blog and am pretty darn happy with it. The cost for professional if using in a work environment with the need for better profile handling, integration with VSTS and other services is a reasonable cost. For those just working with some Github open source repos and Jekyll blogs, they have a free community version, so it's a win!</p>","tags":["cool-tools","tech","git"]},{"location":"git-cracking/#a-free-alternative","title":"A Free Alternative","text":"<p>Source Tree from Atlassian is a pretty solid product as well that I've used. Unfortunatelym I've had stability issues with it lately, and it lacks the most important feature required for all good code tools... a dark theme :-)... on Windows at least as of now. No success getting dark theme traction except on Mac. -1 demerits for this omission! Overall it has promise, but it tends towards so many options it can be daunting. I'd lean towards the implementation by GitKraken being much cleaner, designed for simplicity and flexibility.</p> <p>Disclaimer: I like to review developer software from time to time, and occcasionally recieve a copy to continue using. This does not impact my reviews whatsoever, as I only use the stuff I find helpful that might be worth sharing. Good software makes the world go round!</p>","tags":["cool-tools","tech","git"]},{"location":"offline-net35-install/","title":"SQL .NET Requirements","text":"","tags":["sql-server","tech"]},{"location":"offline-net35-install/#sql-server-install-requirements","title":"SQL Server Install Requirements","text":"<p>SQL Server Installation requirements indicate .NET 3.5, 4.0, or 4.6 depending on the version. This is not including SSMS. At this point you shouldn't use SSMS from any SQL ISO. Just install SQL Management Studio directly.</p> <p>See for more details on this - Improvements with SSMS 2016 - Update SSMS With PS1</p> <p>From a quick review here's what you have regarding .NET requirements for the database engine.</p> SQL Version .NET Required &gt;= SQL 2016 RC1 (SQL 2017 included) .NET 4.6 SQL 2014 .NET 3.5 (manual install required).NET 4.0 (automatic) SQL 2012 .NET 3.5 (manual install required).NET 4.0 (automatic) <p>Specifically noted in SQL 2012-2014 documentation is:</p> <p>.NET 3.5 SP1 is a requirement for SQL Server 2014 when you select Database Engine, Reporting Services, Master Data Services, Data Quality Services, Replication, or SQL Server Management Studio, and it is no longer installed by SQL Server Setup.</p>","tags":["sql-server","tech"]},{"location":"offline-net35-install/#when-net-35-install-just-wont-cooperate","title":"When .NET 3.5 Install Just Won't Cooperate","text":"<p>If you need to install SQL Server that requires .NET 3.5 things can get a little tricky. This is a core feature with windows, so typically it's just a matter of going to Features and enabling, both in Windows 10 and Windows Server.</p> <p>However, if you have a tighter GPO impacting your windows update settings, then you probably need to get this whitelisted. If you are on a time-crunch or unable to get the blocking of .NET 3.5 fixed, then you can also resolve the situation by using a manual offline install of .NET 3.5. Even the setup package Microsoft offers has online functionality and thereby typically fails in those situations.</p>","tags":["sql-server","tech"]},{"location":"offline-net35-install/#offline-approach","title":"Offline Approach","text":"<p>Surprisingly, I had to dig quite a bit to find a solution, as the .NET 3.5 installers I downloaded still attempted online connections, resulting in installation failure.</p> <p>Turns out that to get an offline install correctly working you need a folder from the Windows install image (ISO) located at <code>sources\\sxs</code>.</p> <p>Since I wouldn't want to provide this directly here's the basic steps you take.</p>","tags":["sql-server","tech"]},{"location":"offline-net35-install/#get-netfx3-cab","title":"Get NetFx3 Cab","text":"<ol> <li>Download ISO of Windows 10 (I'm guessing the version won't really matter as you just want the contents in one folder)</li> <li>Mount ISO</li> <li>Navigate to: <code>MountedISO &gt; sources</code> and copy the <code>sxs</code> directory to your location. It should contain <code>microsoft-windows-netfx3-ondemand-package.cab</code>. This is the big difference, as the other methods provide an MSI, not the cab file.</li> </ol>","tags":["sql-server","tech"]},{"location":"offline-net35-install/#create-package","title":"Create Package","text":"<p>Next to create a reusable package</p> <ol> <li> <p>Create a directory: <code>Install35Offline</code></p> </li> <li> <p>Copy SXS directory to this</p> </li> <li> <p>Create 2 files. Gist below to save you some time.</p> <ol> <li>Install35Offline.ps1</li> <li>Install35Offline.bat</li> </ol> </li> </ol> <p>Hopefully this will save you some effort, as it took me a little to figure out how to wrap it all up to make it easy to run.</p> <p>Packing this up in an internal chocolately package would be a helpful way to fix for any developers needing the help of their local dba wizard, and might even earn you some dev karma.</p>","tags":["sql-server","tech"]},{"location":"deleting-a-directory-that-has-a-trailing-space-shouldn-t-be-this-hard/","title":"Deleting a Directory That Has a Trailing Space Shouldn't Be This Hard","text":"<p>It shouldn't be this hard. This is a consumate #windowsmoment</p> <p></p> <p>If you occasionally use something like Robocopy, or other command line tool, it can be possible to create a directory with a trailing slash. For instance</p> <pre><code>robocopy \"C:\\test\\Taco\" \"C:\\Burritos\\are\\delicious \"\n</code></pre> <p>This trailing space would be actually used by Robocopy to initialize a directory that has a trailing space in the name. This can appear as a duplicator in explorer, until you try to rename and notice a trailing slash. Attempts to rename, delete, or perform any activity to manipulate this directory fail as Windows indicates that it can't find the directory as it no longer exists or might have been moved.</p> <p>To resolve this I found details on SO about how to delete in response to the question \"Can't Delete a Folder on Windows 7 With a Trailing Space\".</p> <p>Apparently it's an issue with NFTS handling. To resolve you have to use cmd.exe to <code>rd</code> (remove directory), and change your path to a UNC path referring to your local path.</p> <p>To resolve the error then you'd do:</p> <pre><code>rm \\\\?\\C:\\Burritos\\are\\delicious\n</code></pre> <p>To confirm that PowerShell can't resolve this I did a quick test by running:</p> <pre><code>cd C:\\temp\nmd \".\\ Taco \"\n</code></pre> <pre><code># Fails - No error\nremove-item \"\\\\?\\C:\\temp\\taco \"\n\n# Fails with error: Remove-Item : Cannot find path '\\\\localhost\\c$\\temp\\taco ' because it does not exist.\n$verbosepreference = 'continue'; Remove-Item \"\\\\localhost\\c$\\temp\\taco \"\n\n# SUCCESS: Succeeds to remove it\nGCI C:\\Temp | Where-Object { $_.FullName -match 'taco'} | Remove-Item\n</code></pre> <p>So for me, I wanted to confirm that PowerShell was truly unable to resolve the issue without resorting to cmd.exe for this. Turns out it can, but you need to pass the matched object in, not expect it to match the filepath directly.</p> <p>Now to go eat some tacos....</p>","tags":["tech","powershell"]},{"location":"ntfs-compression-and-sql-server-dont-play-well-together/","title":"NTFS Compression and SQL Server Do Not Play Well Together","text":"<p>Wanted to be proactive and move a database that was in the default path on <code>C:\\</code> to a secondary drive as it was growing pretty heavily.</p> <p>What I didn't realize was the adventure that would ensure.</p>","tags":["tech","sql-server","powershell"]},{"location":"ntfs-compression-and-sql-server-dont-play-well-together/#lesson-1","title":"Lesson 1","text":"<p>Don't move a SQL Server database to a volume that someone has set NTFS Compression on at the drive level.</p>","tags":["tech","sql-server","powershell"]},{"location":"ntfs-compression-and-sql-server-dont-play-well-together/#lesson-2","title":"Lesson 2","text":"<p>Copy the database next time, instead of moving. Would have eased my anxious dba mind since I didn't have a backup. before you judge me.. it was a dev oriented enviroment, not production... disclaimer finished</p>","tags":["tech","sql-server","powershell"]},{"location":"ntfs-compression-and-sql-server-dont-play-well-together/#the-nasty-errors-and-warnings-ensue","title":"The Nasty Errors and Warnings Ensue","text":"<p>First, you'll get an error message if you try to mount the database and it has been compressed. Since I'd never done this before I didn't realize the mess I was getting into. It will tell you that you can't mount the database without marking as read-only as it's a compressed file.</p> <p>Ok... so just go to <code>file explorer &gt; properties &gt; advanced &gt; uncheck compress</code> ... right?</p> <p>Nope...</p> <pre><code>Changing File Attributes 'E:\\DATA\\FancyTacos.mdf' The requested operation could not be completed due to a file system limitation`\n</code></pre> <p>I found that message about as helpful as the favorite .NET error message <code>object reference not found</code> that is of course so easy to immediately fix.</p>","tags":["tech","sql-server","powershell"]},{"location":"ntfs-compression-and-sql-server-dont-play-well-together/#the-fix","title":"The Fix","text":"<ul> <li>Pull up volume properties. Uncheck compress drive OR</li> <li>If you really want this compression, then make sure to uncompress the folders containing SQL Server files and apply.</li> </ul> <p>Since I wasn't able to fix this large of a file by toggling the file (it was 100gb+), I figured to keep it simple and try copying the database back to the original drive, unmark the archive attribute, then copy back to the drive I had removed compression on and see if this worked. While it sounded like a typical \"IT Crowd\" fix (have you tried turning it on and off again) I figured I'd give it a shot.</p> <p>... It worked. Amazingly enough it just worked.</p> <p>Here's a helpful script to get you on your way in case it takes a while. Use at your own risk, and please... always have backups! #DontBlameMeIfYouDidntBackThingsUp #CowsayChangedMyLife</p> <p>and finally to remount the database after copying it back to your drive ...</p>","tags":["tech","sql-server","powershell"]},{"location":"dedupe-google-drive-with-rclone/","title":"Dedupe Google Drive with RClone","text":"","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#an-issue-with-duplicates","title":"An issue with duplicates","text":"<p>I migrated from Amazon Cloud Drive to a paid Google Drive account. To facilate this move, I used a paid service called MultCloud. For me, Comcast prevents unlimited data, so it would have been challenging to manage 1.5TB of video and photography files movement by downloading then reuploading to Google Drive.</p> <p>I ran into issues due to hitting rate limiting with Multcloud. As a result, I had to work through their awkard user interface to relaunch those jobs, which still had failures. I basically was left at the point of not really trusting all my files had successfully transferred.</p> <p>What's worse is that I found that the programmatic access by MultCloud seemed to be creating duplicates in the drive. Apparently Google Drive will allow you have to files side by side with the same name, as it doesn't operate like Windows in this manner, instead each file is considered unique. Same with folders.</p> <p></p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#rclone","title":"RClone","text":"<p>I ran across RClone a while ago, and had passed over it only to arrive back at the documentation regarding Google Drive realizing they have specific functionality for this: dedupe. After working through some initial issues, my situation seems to have improved, and once again Google Drive is usable. In fact, it's time for some house cleaning.</p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#successfully-running","title":"Successfully Running","text":"<p>I suggest you make sure to find the developer api section and create an api access key. If you don't do this and just use Oauth2, you are going to get the dreaded message: <code>Error 403: Rate Limit Exceeded</code> and likely end up spending 30+ mins trying to track down what to do about this. </p> <p>You'll see activity start to show up in the developer console and see how you are doing against your rate limits. </p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#start-simple-and-work-up-from-there","title":"Start Simple and Work Up From There","text":"<p>To avoid big mistakes, and confirm the behavior is doing what you expect, start small. In my script at the bottom, I walked through what I did.</p> <p></p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#other-cool-uses-for-rclone","title":"Other Cool Uses for RClone","text":"","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#merging","title":"Merging","text":"<p>While I think the <code>dedupe</code> command in RClone is specific to Google Drive, you can leverage it's logic for merging folders in other systems, as well as issue remote commands that are server side and don't require download locally before proceeding.</p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#server-side-operations","title":"Server Side Operations","text":"<p>This means, basically I could have saved the money over MultCloud, and instead used Rclone to achieve a copy from Amazon Cloud Drive to Google Drive, all remotely with server side execution, and no local downloads to achieve this. This has some great applications for data migration.</p> <p>For an update list of what support they have for server side operations, take a look at this page: Server Side Operations</p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#aws","title":"AWS","text":"<p>This includes quite a few nifty S3 operations. Even though I'm more experienced with the AWSPowershell functionality, this might offer some great alternatives to syncing to an s3 bucket</p>","tags":["tech","ramblings"]},{"location":"dedupe-google-drive-with-rclone/#mounting-remote-storage-as-local","title":"Mounting Remote Storage As Local","text":"<p>Buried in there was also mention of the ability to mount any of the storage systems as local drives in Windows. See RClount Mount documentation.. This means you could mount an S3 bucket as a local drive with RClone. I'll try and post an update on that after I try it out. It's pretty promising.</p>","tags":["tech","ramblings"]},{"location":"migrating-from-hipchat-to-slack/","title":"Last Minute Migration?","text":"<p>If you are about to perform a last minute migration here's a couple tips as you jump ship from Hipchat and move to Slack. Hipchat is sunsetting I believe on Feb 15th, so I figured I'd share what I do have in case it's helpful, as it won't stay tremendously relevant for long.</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#problem-hipchat-access-will-be-removed-and-you-need-more-time","title":"Problem: Hipchat access will be removed and you need more time","text":"<p>Export the hipchat content to a file and upload to your own s3 bucket. That will ensure you have some time to work through the migration and reverse it and try again if you aren't happy with the results.</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#problem-you-want-to-do-an-initial-import-of-hipchat-content-and-then-update-with-deltas","title":"Problem: You want to do an initial import of Hipchat content and then update with deltas.","text":"<p>Don't even consider it. The slack import can't add delta content for private messages and private rooms. This means you'd get a lot of duplicate rooms being created. It's better to do the migration import in one batch rather than try to incrementally pull in content. Don't go down this route, as I didn't discover this till later in the process resulting in a time-crunch.</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#problem-you-have-hipchat-users-that-have-email-address-that-have-been-migrated-to-a-new-domain-since-they-were-created","title":"Problem: You have hipchat users that have email address that have been migrated to a new domain since they were created.","text":"<p>You've migrated to a new domain, but your Hipchat accounts all have the previous email which you've setup as email aliases. You can't easily change in Hipchat due to the fact it's set a profile level, \"synced\" to the Atlassian account. I had no luck in working on changing this so I instead leveraged the Slack API to bulk update during migration (after all the accounts were created). I mapped the active directory user to the current user by parsing out the email aliases and reversing this. I also created an alternative approach for those that had no matching email alias, and iffy full name matching to use fuzzy matching based soley on last name in the email address.</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#improving-your-migration-experience","title":"Improving Your Migration Experience","text":"","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#rename-your-hipchat-rooms-prior-to-migration-optional","title":"Rename Your Hipchat Rooms Prior to Migration (optional)","text":"<p>The Slack Migration tool is pretty good, but the auto renaming had some rename behavior that didn't align in a clean manner with what my naming convention was going to be. This means to simplify your migration, it's better to rename your Hipchat rooms prior to migration so all your rooms now create slack channels that don't have to be renamed again. Also, if you pull in a delta of content for public rooms, it can automatically match and incrementally add content (this doesn't work for private content).</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#getting-started-with-hipchat-cli","title":"Getting Started with Hipchat CLI","text":"<p>It's painful. Hipchat's going into the great beyond so don't expect support for it.</p> <p>{{&lt; admonition type=\"warning\" title=\"Important\" &gt;}} API Key for personal won't access full list of rooms in the action <code>getRoomList</code> in the CLI. Instead, you'll need to obtain the room list using Add-On token which I found too complex for my one time migration. Instead, you can copy the raw html of the table list, and use a regex script to parse out the room name and number list and use this. You can still perform room rename, just not <code>sendmessage</code> action on the rooms using the API token. {{&lt; /admonition &gt;}}</p> <ol> <li>Install integration from marketplace to the entire account</li> <li>Download the CLI for running locally</li> <li>Create API Key. Important. This is a 40 character personal key, not the key you create as an admin in the administrators section. You need to go to your personal profile, and then create a key while selecting all permissions in the list to ensure full admin privileges.</li> <li>To get the raw HTML easily, simply try this Chrome extension for selecting the table and copying the raw html of the table. CopyTables</li> <li>Open the room listing in Hipchat. Using the extension select <code>Rows</code> as your selection criteria and then select <code>Next Table</code>. Copy the Raw html to an empty doc. Go to the next page (I had 3 pages to go through) and copy each full table contents to append to the raw html in your doc.</li> <li>Once you have obtained all the html rows, then run the following script to parse out the html content into a <code>[pscustomobject[]]</code> collection to work with in your script.</li> </ol> <pre><code>[reflection.assembly]::loadwithpartialname('System.Web')\n$HtmlRaw = Get-Content -Path '.\\TableRowRawHtml.html'\n$Matched = Select-String -InputObject $HtmlRaw -Pattern '((?&lt;=rooms/show/)\\d*(?=\"))(.*?\\n*?.*?)(?&lt;=[&gt;])(.*?(?=&lt;))' -AllMatches | Select-Object -ExpandProperty Matches\n\nWrite-PSFMessage -Level Important -Message \"Total Match Count: $(@($Matched).Count)\"\n\n[pscustomobject[]]$RoomListing = $Matched | ForEach-Object -Process {\n    $m = $_.Groups\n    [pscustomobject]@{\n            RoomId           = $m[1].Value\n            OriginalRoomName = [system.web.httputility]::HtmlDecode($m[3].Value)\n        }\n}\n\nWrite-PSFMessage -Level Important -Message \"Total Rooms Listed: $(@($RoomListing).Count)\"\n</code></pre> <p>Now you'll at least have a listing of room id's and names to work with, even if it took a while to get to it. There are other ways to get the data, such as expanding the <code>column-format=999</code> but this timed out on me and this ended actually being the quickest way to proceed.</p>","tags":["tech","powershell","development"]},{"location":"migrating-from-hipchat-to-slack/#using-cli","title":"Using CLI","text":"<p>To get started, cache your credentials using the fantastic BetterCredentials module. To install you'll need to run <code>Install-Module BetterCredentials -Scope CurrentUser -AllowClobber -Force</code></p> <p>Then set your cached credentials so we don't need to hard code them into scripts. This will cache it in your Windows Credential manager.</p> <pre><code>$cred = @{\n    credential   = ([pscredential]::new('myHipchatEmail' , (\"APITokenHere\" | ConvertTo-SecureString -AsPlainText -Force) ) )\n    type         = 'generic'\n    Persistence  = 'localcomputer'\n    Target       = 'hipchatapi'\n    description  = 'BetterCredentials cached credential for hipchat api'\n}\nBetterCredentials\\Set-Credential @cred\n</code></pre> <p>Initialize the working directory and default parameters for the CLI so you can easily run other commands without having to redo this over and over.</p> <pre><code>#----------------------------------------------------------------------------#\n#                 set location for the java cli environment                  #\n#----------------------------------------------------------------------------#\n$Dir = Join-Path 'C:\\PathToCli' 'atlassian-cli-8.1.0-distribution\\atlassian-cli-8.1.0'\nSet-Location $Dir\n$Url = 'https://TACOS.hipchat.com'\n\n#----------------------------------------------------------------------------#\n#              configure default arguments for calling java cli              #\n#----------------------------------------------------------------------------#\n$JavaCommand = \"java -jar $(Join-Path $dir 'lib/hipchat-cli-8.1.0.jar') --server $url --token $Password --autoWait --quiet\"\n</code></pre> <p>Now you can issue some simple commands to start manipulating the CLI.</p> <pre><code>#----------------------------------------------------------------------------#\n#          Get Entire Room Listing -- Including Archived &amp; Private           #\n#----------------------------------------------------------------------------#\n$Action = '--action getRoomList --includePrivate --includeArchived --outputFormat 1'\n$result = Invoke-Expression -command \"$JavaCommand $Action\"\n$RoomList = $result | ConvertFrom-CSV\n$RoomList | Export-CliXml -Path (Join-Path $ScriptsDir 'CurrentRoomList.xml') -Encoding UTF8 -Force #just so we have a copy saved to review\n</code></pre> <p>I just tweaked this snippet for other types of commands, but this should get you pretty much what you need to run interactive commands via CLI. I've also written up some Slack functions and will likely share those soon as well as I've found them helpful in automatically fixing email addresses, activating &amp; deactivating users, identifying active billed users, and other basic administrative focused actions.</p>","tags":["tech","powershell","development"]},{"location":"sql-server-database-experimentation-assistance-how-to-run-a-capture/","title":"DEA","text":"<p>Very basic look at the setup as I couldn't find much documentation on this when I last tried this out in 2018. Maybe it will help you get started a little more quickly. I've not had a chance to leverage the actual comparisons across a large workload. When I originally wrote up the basics on this last year I found my needs required more customized load testing approaches.</p>","tags":["sql-server","tech","performance-tuning","dbatools"]},{"location":"sql-server-database-experimentation-assistance-how-to-run-a-capture/#adding-the-feature","title":"Adding The Feature","text":"<p>Added the DRCReplay.exe and the controller services by pulling up the feature setup and adding existing features to existing SQL instance installed.</p> <p></p> <p>Pointed the controller directory to a new directory I created</p> <pre><code>[io.directory]::CreateDirectory('X:\\Microsoft SQL Server\\DReplayClient\\WorkingDir')\n[io.directory]::CreateDirectory('X:\\Microsoft SQL Server\\DReplayClient\\ResultDir')\n</code></pre>","tags":["sql-server","tech","performance-tuning","dbatools"]},{"location":"sql-server-database-experimentation-assistance-how-to-run-a-capture/#initializing-test","title":"Initializing Test","text":"<p>Started with backup of the database before executing the activity I wanted to trace.</p> <pre><code>dbatools\\backup-dbadatabase -sqlinstance localhost -database $Dbname -CopyOnly -CompressBackup\n</code></pre> <p>Initialized application application activity, and then recorded in DEA. The result was now in the capture section.</p> <p></p> <p>Restoring after trace was recorded in DEA was simple with the following command from Dbatools</p> <pre><code>restore-dbadatabase -SqlInstance localhost -Path \"&lt;BackupFilePath&gt;\" -DatabaseName SMALL -WithReplace\n</code></pre> <p>After this restore, initiating the replay was achieved by going to the replay tab.</p> <p></p>","tags":["sql-server","tech","performance-tuning","dbatools"]},{"location":"debugging-type-binding-in-powershell/","title":"Debugging Type Binding in PowerShell","text":"<p>Some effort I spent in researching Type Binding in Stack Overflow to help answer a question by Chris Oldwood helped me solidify my understanding of the best way to debug more  complicated scenarios such as this in PowerShell.</p> <p>Why does this PowerShell function's default argument change value based on the use of . or &amp; to invoke a command within it? </p> <p>Spent some digging into this and this is what I've observed.</p> <p>First for clarity I do not believe that you should consider the NullString value the same as null in a basic comparison. Not sure why you need this either, as this is normally something I'd expect from c# development. You should be able to just use <code>$null</code> for most work in PowerShell.</p> <pre><code>if($null -eq [System.Management.Automation.Language.NullString]::Value)\n{\n    write-host \"`$null -eq [System.Management.Automation.Language.NullString]::Value\"\n}\nelse\n{\n    write-host \"`$null -ne [System.Management.Automation.Language.NullString]::Value\"\n}\n</code></pre> <p>Secondly, the issue is not necessarily because of the call operator, ie <code>&amp;</code>. I believe instead you are dealing with underlying parameter binding coercion. Strong data typing is definitely a weak area for PowerShell, as even explicitly declared <code>[int]$val</code> could end up being set to a string type by PowerShell automatically in the next line when writing <code>Write-Host $Val</code>.</p> <p>To identify the underlying behavior, I used the <code>Trace-Command</code> function (Trace Command) .</p> <p>I changed the Use-Dot to just call the function as no write-host was needed to output the string.</p> <pre><code>function Use-Ampersand\n{\n    param(\n        [string]$NullString = [System.Management.Automation.Language.NullString]::Value\n    )\n    Format-Type $NullString\n    &amp;cmd.exe /c exit 0\n}\n</code></pre> <p>The Format-Type I modified to also use what is considered a better practice of <code>$null</code> on the left, again due to type inference.</p> <pre><code>function Format-Type($v= [System.Management.Automation.Language.NullString]::Value)\n{\n\n    if ($null  -eq $v)\n    {\n     '(null)'\n    }\n    else {\n        $v.GetType().FullName\n     }\n}\n</code></pre> <p>To narrow down the issue with the data types, I used the following commands, though this is not where I found insight into the issue. Theyh  when called directly worked the same.</p> <pre><code>Trace-Command -Name TypeConversion -Expression { Format-Type $NullString} -PSHost\nTrace-Command -Name TypeConversion -Expression { Format-Type ([System.Management.Automation.Language.NullString]$NullString) } -PSHost\n</code></pre> <p>However, when I ran the functions using TypeConversion tracing, it showed a difference in the conversions that likely explains some of your observed behavior.</p> <pre><code>Trace-Command -Name TypeConversion  -Expression { Use-Dot} -PSHost\nTrace-Command -Name TypeConversion  -Expression { Use-Ampersand} -PSHost\n</code></pre> <pre><code># USE DOT\nDEBUG: TypeConversion Information: 0 :  Converting \"\" to \"System.String\".\nDEBUG: TypeConversion Information: 0 :      Converting object to string.\nDEBUG: TypeConversion Information: 0 :  Converting \"\" to \"System.Object\". &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nDEBUG: TypeConversion Information: 0 :  Converting \".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.CPL\" to \"System.String\".\nDEBUG: TypeConversion Information: 0 :      Result type is assignable from value to convert's type\n</code></pre> <p><code>OUTPUT: (null)</code></p> <pre><code># Use-Ampersand\nDEBUG: TypeConversion Information: 0 : Converting \"\" to \"System.String\".\nDEBUG: TypeConversion Information: 0 :     Converting object to string.\nDEBUG: TypeConversion Information: 0 : Converting \"\" to \"System.String\". &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nDEBUG: TypeConversion Information: 0 :     Converting null to \"\".        &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nDEBUG: TypeConversion Information: 0 : Converting \".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.CPL\" to \"System.String\".\nDEBUG: TypeConversion Information: 0 :     Result type is assignable from value to convert's type\n</code></pre> <p><code>OUTPUT: System.String</code></p> <p>The noticeable difference is in <code>Use-Ampersand</code> it shows a statement of <code>Converting null to \"\"</code> vs <code>Converting \"\" to \"System.Object\"</code>.</p> <p>In PowerShell, <code>$null &lt;&gt; [string]''</code>. An empty string comparison will pass the null check, resulting in the success of outputting <code>GetType()</code>.</p>","tags":["powershell","deep-dive"]},{"location":"debugging-type-binding-in-powershell/#a-few-thoughts-on-approach-with-powershell","title":"A Few Thoughts On Approach With PowerShell","text":"<p>Why it's doing this, I'm not certain, but before you invest more time researching, let me provide one piece of advice based on learning the hard way.</p> <p>If start dealing with issues due to trying to coerce data types in PowerShell, first consider if PowerShell is the right tool for the job</p> <p>Yes, you can use type extensions. Yes, you can use .NET data types like <code>$List = [System.Collections.Generic.List[string]]::new()</code> and some .NET typed rules can be enforced. However, PowerShell is not designed to be a strongly typed language like C#. Trying to approach it like this will result in a many difficulties. While I'm a huge fan of PowerShell, I've learned to recognize that it's flexibility should be appreciated, and it's limits respected.</p> <p>If I really had issues that required mapping <code>[System.Management.Automation.Language.NullString]::Value</code> so strongly, I'd consider my approach.</p> <p>That said, this was a challenging investigation that I had to take a swing at, while providing my 10 cents afterwards.</p>","tags":["powershell","deep-dive"]},{"location":"debugging-type-binding-in-powershell/#other-resources","title":"Other Resources","text":"<p>After posting my answer, I found another answer that seemed relevant, and also backs up the mentioning of not using <code>[NullString]</code> normally, as its usage in PowerShell is not really what it was designed for.</p> <p>Stackoverflow specific content republished under CC-BY-SA</p>","tags":["powershell","deep-dive"]},{"location":"2019-04-17t102353-0600/","title":"2019-04-17T10:23:53-06:00","text":"<p>Today I learned how to create a microblog / microblogs section in my hugo layout by frakensteining together some styling tweaks to part of jnjosh.com hugo theme. </p>","tags":["til","hugo"]},{"location":"2019-04-18t102331-0600/","title":"2019-04-18T10:23:31-06:00","text":"<p>This infernal keyboard lag is killing me. I'm guessing it's due to some OS corruption as my disk is going bad and bad sector warnings keep increasing. Blah! </p>","tags":["tech","ramblings"]},{"location":"2019-04-19-225700--0500/","title":"2019-04-19 22:57:00 -0500","text":"<p>Working smart: how great performers do less work</p> <p>This book so far is a good read. I like the concept of the \"feedback loop\", and doing small iterative improvements with a targeted narrow focus to improve each day. It's very much in alignment with Agile concepts. It's kinda like delivery of small measurable bits of value for your own improvement. This contrasts our typical promise to ourselves of radical transformation or resolutions that never get realized. Clarifying these concepts is really helpful to help one be proactive instead of reactive about personal growth.</p>","tags":["tech"]},{"location":"2019-04-25-003935--0500/","title":"2019-04-25 00:39:35 -0500","text":"<p>Finally upgraded my graphics card and hardrive. EVGA RTX 2080 and a Samsung Evo 970 NVMe 2TB SSD breathed new life into my PC. Running 100hz 3440x1440 ultrawide on a 10 year old AMD 6950 was just not doing the trick.</p>","tags":["tech"]},{"location":"2019-05-16t111528-0500/","title":"2019-05-16T11:15:28-05:00","text":"<p>Downloading a Visual Studio Code vsix extension and then installing manually in Azure Data Studio works for some extensions! For instance, downloading Simple Alignment from the marketplace, and then running in Azure Data Studio successfully installed the utility.</p> <p>You can also many of your keyboard settings straight from vscode into Azure Data Studio so you don't have to remap all those keys. The beauty of json configuration files </p>","tags":["til","tech"]},{"location":"2019-05-17t155732-0500/","title":"2019-05-17T15:57:32-05:00","text":"<p>Really enjoying my experience with Terraform from the last month. If you have any resources in the cloud you have to deploy and you are having to do them manually, you should take a look. It's very easy to get going with the basics and the time it can save as you build up Terrachops (patent pending) can be tremendous.</p>","tags":["terraform","devops"]},{"location":"2019-05-17t155950-0500/","title":"2019-05-17T15:59:50-05:00","text":"<p>Kids learn so quick. It's amazing how fast my son has picked up #origami. He often has to wait for his slow-poke dad.</p> <p></p>","tags":["dadlife","origami"]},{"location":"2019-05-31t1900420000000/","title":"2019-05-31T19:00:42.000+00:00","text":"<p>I'd read about Terraform 0.12 and thought it was much further out, so moved on with regret from evaluating the massive number of improvements. Just found out it was released, and <code>choco upgrade terraform -y</code> provided me with a delightful <code>0.12</code> upgrade. If you haven't explored it yet, go do it!</p> <p>Things like loops, no longer having to reference any variable with string interpolation, and more promises to make this a big productivity improvement for those enjoying Terraform.</p> <p>If you aren't using any Infrastructure-As-Code approach right now, you'll find it initially a little confusing, but get past that and you'll wonder how you ever lived without it.</p>","tags":["devops","terraform"]},{"location":"leveraging-aws-system-manager-sessions/","title":"Use Case","text":"<p>Leveraging aws system manager sessions can help with aws development, by eliminating the need to RDP for work that can be done via a PowerShell session. In addition, it can help bypass the need to use SSH tunneling, remote Windows management, or RDP hops into the final destination.</p> <p>This leverages IAM Credentials, allowing consistent security management in alignment with other IAM policies, instead of having to manage another security setting like remote management would require, potentially reducing the security explore footprint.</p> <ul> <li>Quickly access an instance that normally would require an additional hop, and then evaluate</li> <li>Restart remote service without having to hop into it (or issue SSM prebuilt command docs)</li> <li>Interact in other ways that are more adhoc in nature, and don't have prebuilt SSM Documents ready to go.</li> </ul>","tags":["tech","aws","devops"]},{"location":"leveraging-aws-system-manager-sessions/#browser","title":"Browser","text":"<p>This is a great option for leveraging AWS Systems Manager web console. When you select start a session you'll be presented with the tagged instances by name that you can quickly select and start a remote session with.</p> <p></p> <p></p> <p>Once you've started the session you'll enter into a remote prompt.</p> <p></p>","tags":["tech","aws","devops"]},{"location":"leveraging-aws-system-manager-sessions/#local-interactive-setup","title":"Local Interactive Setup","text":"<p>I use Cmder for my main terminal, with all other terminals normally running in Visual Studio Code. If you open a Powershell session using the powershell plugin you can write your PowerShell in the editor, and the interactively run it in Visual Studio Code using the predefined <code>F8</code> key.</p>","tags":["tech","aws","devops"]},{"location":"leveraging-aws-system-manager-sessions/#install-on-windows","title":"Install on Windows","text":"<p>Ensure your AWS Credentials are setup, and use the session manager plugin after installation by running:</p>","tags":["tech","aws","devops"]},{"location":"leveraging-aws-system-manager-sessions/#start-session","title":"Start Session","text":"<pre><code>aws ssm start-session --target MyInstanceId\n</code></pre>","tags":["tech","aws","devops"]},{"location":"leveraging-aws-system-manager-sessions/#limitations","title":"Limitations","text":"<p>Refresh rate is slow. Input for large script blocks from Visual Studio Code is also really slow. This means that putting a local function in scope by running <code>F8</code> against it and then wanting to run this function interactively can take a while.</p> <p>The best use case I see is for adhoc administrative or investigative work. If larger scripts are required, then having a script setup to install module or copy from s3 would be a much more performance solution, as it wouldn't require large amounts of console text streaming.</p>","tags":["tech","aws","devops"]},{"location":"bump-nuspec-file-version-with-powershell/","title":"Bump nuspec file version with powershell","text":"","tags":["chocolatey","azuredevops","devops","powershell"]},{"location":"bump-nuspec-file-version-with-powershell/#bump-nuspec-version","title":"Bump Nuspec Version","text":"<p>Bumping the version of the nuspec file requires a little tweaking and I got some help from the slack powershell community to ensure I handled the xml parsing correctly. This was the result. If you are running a chocolatey package build or equivalent nuspec build via an agent and want a way to ensure the latest build updates the build version incrementally this should help.</p> <p>This snippet should help give you a way to bump a nuspec file version programmatically.</p> <p>I modified the logic to support <code>-WhatIf</code> since I'm a fan of being able to run stuff like this without actually breaking things first.</p>","tags":["chocolatey","azuredevops","devops","powershell"]},{"location":"aws-ssm-powershell-script-automation/","title":"AWS SSM PowerShell Script Automation","text":"","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#ssm-overview","title":"SSM Overview","text":"<p>I've found that working with a large number of environments in AWS can provide some interesting challenges for performing various tasks, in a way that scale.</p> <p>When you begin to have dozens to hundreds of servers that you might need to provide a quick fix, the last thing you want to do is RDP into each and perform some type of scripted action.</p> <p>AWS SSM (Systems Manager) provides a tremendous amount of functionality to help manage systems. It can perform tasks from running a script, installing an application, and other mundane administrative oriented tasks, to more complex state management, AMI automation, and other tasks that might go beyond the boundaries of virtual machine management.</p> <p>I'll probably be unpacking a few of these areas over the next few posts, since my world has been heavily focused on SSM usage in the last months, and leveraging it is a must for those working heavily with EC2.</p>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#powershell-execution","title":"PowerShell Execution","text":"<p>For the first simple example, AWS SSM provides documents that wrap up various scripted actions and accept parameters. This can be something like Joining a domain or running a shell script.</p> <p>In my case, I've had the need to change a registry setting, restart a windows service, or set an environment variable across an environment. I additionally wanted to set the target of this run as a tag filter, instead of providing instanceid, since this environment is rebuilt often as part of development.</p> <p>The commands to execute scripts have one flaw that I abhor. I hate escaping strings. This probably comes from my focused effort on mastering dynamic t-sql , at which point I quickly tried to avoid using dynamic sql as much as possible as I realized it was not the end all solution I started to think it was when I just started learning it.</p> <p>With PowerShell and AWS SSM things could get even messier. You'd have to pass in the command and hope all the json syntax and escaping didn't error things out.</p>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#the-solution","title":"The solution","text":"<p>Write PowerShell as natively designed, and then encode this scriptblock for passing as an encoded command. I've found for the majority of my adhoc work this provided a perfect solution to eliminate any concerns on having to escape my code, while still letting me write native PowerShell in my Vscode editor with full linting and syntax checks.</p>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#authenticate","title":"Authenticate","text":"<pre><code>Import-Module AWSPowershell.NetCore, PSFramework #PSFramework is used for better config and logging. I include with any work i do\n$ProfileName = 'taco'\n$region = 'us-west-1'\nInitialize-AWSDefaultConfiguration -ProfileName $ProfileName -region $region\n</code></pre>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#create-your-command","title":"Create Your Command","text":"<p>In this section, I've provided a way to reference an existing function so the remote instance can include this function in the local script execution rather than having to copy and paste it into your command block directly. DRY for the win.</p> <pre><code>#----------------------------------------------------------------------------#\n#                  Include this function in remote command                   #\n#----------------------------------------------------------------------------#\n$FunctionGetAWSTags = Get-Content -Path 'C:\\temp\\Get-AWSTags.ps1' -Raw\n$command = {\n  Get-Service 'codedeployagent' | Restart-Service -Verbose\n}\n</code></pre> <p>Now that you have a script block, you can work on encoding. This encoding will prevent you from needing to concern yourself with escaping quotes, and you were able to write your entire script in normal editor without issues in linting.</p> <pre><code>#----------------------------------------------------------------------------#\n#                   encode command to avoid escape issues                    #\n#----------------------------------------------------------------------------#\n[string]$CommandString = [string]::Concat($FunctionGetAWSTags, \"`n`n\", $Command.ToString())\n$bytes = [System.Text.Encoding]::Unicode.GetBytes($CommandString)\n$encodedCommand = [Convert]::ToBase64String($bytes)\n$decodedCommand = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($encodedCommand));\nWrite-PSFMessage -Level Debug -Message \"Decoded Command: $($DecodedCommand)\"\n</code></pre> <p>In my local script, I'll also include this Wait-SSM command that's a quick way to wait for the results of the SSM job to finish and show status. This is because Send-SSMCommand is actually an asynchronous command and doesn't wait for completion, just the successful sending of the command.</p> <pre><code>function Wait-SSM\n{\n    param(\n        [Amazon.SimpleSystemsManagement.Model.Command]$Result\n    )\n    end\n    {\n        $Status = (Get-SSMCommandInvocation -CommandId $Result.CommandId -Details $true | Select-Object -ExpandProperty CommandPlugins).Status.Value\n        while ($status -ne 'Success')\n        {\n            $Status = (Get-SSMCommandInvocation -CommandId $Result.CommandId -Details $true | Select-Object -ExpandProperty CommandPlugins).Status.Value\n            Start-Sleep -Seconds 5\n        }\n        Get-SSMCommandInvocation -CommandId $Result.CommandId -Details $true | Select-Object InstanceId, Status | Format-Table -Autosize -Wrap\n    }\n}\n</code></pre>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#send-command","title":"Send Command","text":"<p>Finally, we get to the meat  and potatos... or in my case I'd prefer the meat and tacos  of the matter.</p> <p>Sending the command...</p> <pre><code>$Message = (Read-Host \"Enter reason\")\n$sendSSMCommandSplat = @{\n    Comment                                       = $Message\n    DocumentName                                  = 'AWS-RunPowerShellScript'\n    #InstanceIds                                  = $InstanceIds # 50 max limit\n    Target                                        = @{Key=\"tag:env\";Values=@(\"tacoland\")}\n    Parameter                                     = @{'commands' = \"powershell.exe -nologo -noprofile -encodedcommand $encodedCommand\"  }\n    CloudWatchOutputConfig_CloudWatchLogGroupName  = 'ssm/manual/my-command'\n    CloudWatchOutputConfig_CloudWatchOutputEnabled = $true\n}\n$result = Send-SSMCommand  @sendSSMCommandSplat\nWait-SSM -Result $result\n</code></pre> <p>Note that you can also pass in an instance list. To do this, I'd recommend first filtering down based on tags, then also filtering down to available to SSM for running the command to avoid running on instances that are not going to succed, such as instances that are off, or ssm is not running on.</p> <p>To stream results from cloudwatch, try looking at my post: Post on Using Cw for Cloudwatch Log Stream In Terminal</p> <pre><code>cw tail -f --profile=my-profile --region=eu-west-1 'ssm/manual/my-command'\n</code></pre>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#ec2-filters","title":"EC2 Filters","text":"<p>To simplify working with tags, I often use the <code>ConvertTo-Ec2Filter</code> function that was written by David Christian (@dchristian3188) and can be viewed on this blog post EC2 Tags and Filtering.</p> <pre><code>Function ConvertTo-EC2Filter\n{\n    [CmdletBinding()]\n    Param(\n        [Parameter(\n            ValueFromPipeline,\n            ValueFromPipelineByPropertyName)]\n        [HashTable]\n        $Filter\n    )\n    Begin\n    {\n        $ec2Filter = @()\n    }\n    Process\n    {\n        $ec2Filter = Foreach ($key in $Filter.Keys)\n        {\n            @{\n                name   = $key\n                values = $Filter[$key]\n            }\n        }\n    }\n    End\n    {\n        $ec2Filter\n    }\n}\n</code></pre> <pre><code>$searchFor = @{\n    'tag:toppings'   = 'saucesAndMoreSauces'\n    'tag:env'        = 'tacoland'\n}\n\n\n$ssmInstanceinfo        = Get-SSMInstanceInformation\n$ec2Filter              = ConvertTo-EC2Filter -Filter $searchFor\n$Instances              = @(Get-EC2Instance -Filter $ec2Filter).Instances\n[string[]]$InstanceIds  = ($Instances | Where-Object { $_.State.Name -eq 'running' -and $_.InstanceId -in $ssmInstanceinfo.InstanceId } | Select-Object InstanceId -Unique).InstanceId\n</code></pre>","tags":["powershell","devops","aws"]},{"location":"aws-ssm-powershell-script-automation/#wrap-up","title":"wrap-up","text":"<p>Hopefully this will get you going with Send-SSMCommand in a way that helps give you a simple way to issue commands across any number of EC2 instances. For me, it's saved a lot of manual console work to run commands against tagged environments, allowing me to more rapidly apply a fix or chocolatey package, or any number of needs in the context of testing, without all the overhead of doing per instances, or use the dreaded RDP  connection.</p> <p>If you find something unclear or worth more explanation, I'm always up for editing and refactoring this post. </p>","tags":["powershell","devops","aws"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/","title":"Painless Synchronization of Azure Blob Storage with AWS S3","text":"","tags":["tech","aws","devops","azure"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/#synchronization","title":"Synchronization","text":"<p>Moving data between two cloud providers can be painful, and require more provider scripting if doing api calls. For this, you can benefit from a tool that abstracts the calls into a seamless synchronization tool.</p> <p>I've used RClone before when needing to deduplicate several terabytes of data in my own Google Drive, so I figured I'd see if it could help me sync up 25GB of json files from Azure to S3.</p> <p>Very happy to report it worked perfectly, and with only a couple minutes of refamilarizing myself with the tool setup.</p>","tags":["tech","aws","devops","azure"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/#install-rclone","title":"Install RClone","text":"<p>For windows users, it's as easy as leveraging Chocolatey and running</p> <pre><code>choco upgrade rclone -y\n</code></pre>","tags":["tech","aws","devops","azure"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/#setup-providers","title":"Setup Providers","text":"<p>Go through <code>rclone config</code> dialogue and setup your cloud provider. In my case, I setup Azure as a provider to connect to blob storage, and then AWS with s3.</p> <p>{{&lt; admonition type=\"info\" title=\"Cloud to Cloud\" &gt;}} Providers that support cloud to cloud based calls without copying locally are provided in the section for Optional Features where you can view the operations that support calls {{&lt; /admonition &gt;}}</p>","tags":["tech","aws","devops","azure"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/#initialize-sync","title":"Initialize Sync","text":"<pre><code>rclone copy azure:containername s3:bucketname/keyprefix --log-level ERROR --progress --dry-run\n</code></pre>","tags":["tech","aws","devops","azure"]},{"location":"painless-synchronization-of-azure-blob-storage-with-aws-s3/#wrap-up","title":"Wrap-up","text":"<p>Take a look at this if you need a simple way to grab some data from one provider and leverage in another and you might want to save yourself some time on learning provider specific api calls. I've found tools like this, Terraform, and others that help abstract the api calls can be a great resource as you can leverage one syntax to work with two completely different providers and eliminate a lot of effort in coding.</p>","tags":["tech","aws","devops","azure"]},{"location":"2019-09-26t1358580000/","title":"2019-09-26T13:58:58+00:00","text":"<p>You know you probably reserved the wrong hotel when you drive around from the back and think it's a condemned building, only to come around the front and realize that's actually your reservation. Needless to say, that reservation got cancelled.</p>","tags":["travel","family"]},{"location":"2019-09-26t1404400000/","title":"2019-09-26T14:04:40+00:00","text":"<p>I have a new term for the wait my daughter eats food. Instead of please stop wearing food all over your face and eat proper I'm going to label her: \"An experiential tactile food lover\"</p>","tags":["family"]},{"location":"2019-09-26t1410420000/","title":"2019-09-26T14:10:42+00:00","text":"<p>You know you are a dad when you start looking at a minivan as a really cool vehicle and marvel at all the features that have zilch to do with driving. $35-40k for a glorified station wagon with sliding doors would probably make sane humans question reality, while #dadlife looks at sliding doors as less risk for door ding claims from his little rascals.</p> <p>You know you have a researching mentality when you begin to investigate minivan prejudices impact in the market against widerspread adoption. #bunnytrail</p>","tags":["family","dad-life"]},{"location":"getting-started-with-terraform/","title":"Getting Started With Terraform","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated 2020-07\" &gt;}} - Added comments about brand new Terraform users ignoring Terraform Cloud for first time tests. - Added comment about pulling credentials using data source instead of environment variables for AWS as a more advanced option to consider in the future. - Replaced details on creating terraform credential file with the new <code>tf login</code> command {{&lt; /admonition &gt;}}</p> <p>Getting started with using Terraform for infrastructure can be a bit daunting if you've not dived into this stuff before. I put this together as a write up for those looking to get their feet wet and have a better idea of where to go for getting some momentum in starting. There are some assumptions in this, such as basic familiarity with git for source control automation, basic command line usage, and basic cloud familiarity.</p> <p>If time permits, I plan on writing up some more detailed walk through in future posts on Terraform iteration methods, object types, dynamic data inputs, and other things I've explored. However, what I've found is just getting the initial start seems to be a blocker for many people interested in trying it. Hopefully, this will give someone a head start on getting a basic plan going so they can understand how this works a little better and the other more detailed tutorials that abound will make more sense then. Give this post a clap or leave a comment if it helps you or you have any feedback. Cheers! :cheers:</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#purpose-of-this-post","title":"Purpose of This Post","text":"<p>{{&lt; admonition type=\"info\" title=\"Using Terraform Cloud 2020-07\" &gt;}} If you are brand new to Terraform, then consider ignoring the \"backend\" section. This will have all the artifacts that Terraform produces (such as the state file) just sit in your local directory. In retrospect, Terraform Cloud intermixed with getting up and running as a new user might add more complication than required. {{&lt; /admonition &gt;}}</p> <p>In technical documentation, there is a difference between a tutorial and a getting started. The getting started here is going to focus just on getting up and running, not on all the concepts about infrastructure as code. I found that just doing it the first time was the hardest thing. Terminology about modules and re-usability at the beginning of my efforts with Terraform went straight over my head as I couldn't fully wrap my understanding around how it would work. Now that I've gotten a lot more experience with Terraform for various projects, I've got some personal \"best-practices\" that I've found as well as insight from the community.</p> <p>That's for another day </p> <p>Let's just make sure you can get up and running with a basic deployment Terraform deployment from the scratch.</p> <p>I had minimal Cloudformation authoring experience, so this was new stuff to me at the time.</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#what-about-cloudformation","title":"What about Cloudformation?","text":"<p>More knowledgeable people than me have written about this. I'll just say these personal subjective observations:</p> <ol> <li>Terraform is recognized for being a great tool in the industry, it's not some \"indie open source project about to fail\". Hashicorp has some serious vision.</li> <li>Just because you aren't going \"cross provider\" with Azure and AWS doesn't rule out Terraform. You aren't necessarily gaining anything special by \"sticking with native\" AWS CF, like you might think.</li> <li>Terraform's much more succinct, less prone to whitespace/indentation failures.</li> <li>IMO re-usability of Terraform provides itself to a better team collaborative experience.</li> <li>Terraform's preview of changes is more intuitive to me. Less nervous to deploy stuff.</li> <li>I just like HCL (Hashicorps DSL) better than writing YAML docs.</li> <li>If you are writing YAML without any generator... just why!</li> </ol>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#resources","title":"Resources","text":"Links Description Terraform Documentation Reference Terraform Documentation for CLI Terraform Documentation For AWS Terraform AWS Provider Documentation","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#setup","title":"Setup","text":"","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#installation-and-setup","title":"Installation and setup","text":"<p>Install chocolatey via command prompt as administrator</p> <pre><code>@\"%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" &amp;&amp; SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n</code></pre> <p>For macOS <pre><code>brew cask install terraform\n</code></pre></p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#terraform-cloud-setup","title":"Terraform Cloud Setup","text":"<p>{{&lt; admonition type=\"warning\" title=\"Subscription\" &gt;}} This will require a Terraform Cloud account. At the time of this post they have a plan for around 5 users for free with a few features turned off. {{&lt; /admonition &gt;}}</p> <p>Setup your Terraform App Account and make sure to enable 2FA.</p> <p>Once you've been added, create a personal access token at your user settings (this is personal, not team based)</p> <p>If you are using Terraform Cloud, run <code>tf login</code> to generate your local credential file to allow connecting to Terraform Cloud and easily using the registry and remote state workspaces provided.</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#creating-your-first-project","title":"Creating Your First Project","text":"<p>Create <code>main.tf</code>. It will be the first file you create.</p> <p>Create <code>provider.tf</code></p> <p>{{&lt; gist sheldonhull  95c3f9533b2111d7d9fa40ff90a917e3 \"provider.tf\" &gt;}}</p> <p>Create <code>terraform.auto.tfvars</code></p> <p>Note that if you try to create this file with the <code>terraform.tfvars</code> name, it won't work if using Terraform Cloud, as tfvars get generated dynamically from the variables setup in the Cloud workspace.</p> <p>{{&lt; gist sheldonhull  95c3f9533b2111d7d9fa40ff90a917e3 \"terraform.auto.tfvars\" &gt;}}</p> <p>Create <code>variables.tf</code> which is going to house all the input variables we want to declare.</p> <p>{{&lt; gist sheldonhull  95c3f9533b2111d7d9fa40ff90a917e3 \"variables.tf\" &gt;}}</p> <p>Create <code>iam.tf</code> which will provide a nice low risk resource to create that will show you how to use string interpolation for dynamic names in the most simple way, as well as the way to leverage <code>EOT</code> syntax to easily escape mutliline strings. However, if you see yourself doing this constantly, you might reevaluate your approach to ensure you are using objects and properties as much as possible and not just strings.</p> <p>{{&lt; gist sheldonhull  95c3f9533b2111d7d9fa40ff90a917e3 \"iam.tf\" &gt;}}</p> <p>{{&lt; admonition type=\"info\" title=\"HCL Multiline String Syntax\" &gt;}} If you use <code>&lt;&lt;-EOT</code> you get a nice little benefit that's not well documented. The <code>-</code> means it strings buffering whitespace for the following lines. This can allow you to keep your content indented and if you preface the first line with 6 spaces, then all the following lines trim 6 spaces as well to allow you to avoid a bunch of difficult to read string blocks. {{&lt; /admonition &gt;}}</p> <p>You'll likely want to use a workspace with Terraform to organize this work, so instead of using the default, use the command</p> <pre><code>terraform workspace new qa\n</code></pre> <p>Terraform should select this new workspace by default. You can list the current workspaces using <code>terraform workspace list</code> and then select another one later if you wish by running <code>terraform workspace select qa</code>.</p> <p>{{&lt; admonition type=\"warning\" title=\"Terraform Workspace Naming\" &gt;}} Personally, I'd recommend to not drive much of your naming or other configuration based on the workspace name, and instead use variables.</p> <p>Terraform Cloud behavior with trying to use workspace names at the time of this post was not what I expected, so I ended up removing my dependency on workspace names being important for the configuration. See GitHub Issue</p> <p>Instead, I use it as metadata only to organize the workspaces, not try to build configuration based heavily on using workspace name. {{&lt; /admonition &gt;}}</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#deploying-infrastructure","title":"Deploying Infrastructure","text":"<p>Deploying is as simple as running <code>terraform apply</code>. You'll get a preview of the plan before apply, and then you can approve it to actually apply.</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#if-you-connected-this-to-terraform-cloud","title":"If You Connected This to Terraform Cloud","text":"<p>This is assuming you are running via Terraform Cloud. To run locally, you'll want to go to the workspace you created in Terraform Cloud and in the General Settings set to run locally instead of remote.</p> <p>This means you'll be able to run the apply directly on your machine instead of running it from the remote location. Running remote is great, but for this to work you need to edit your Terraform remote cloud workspace and add the AWS access keys, as the job is actually running in the remote context and not using your local machine credentials.</p> <p>{{&lt; admonition type=\"info\" title=\"Terraform Cloud Credentials\" &gt;}} My preferred solution for this is to setup another Terraform workspace to create the credentials and then call this datasource to provide me with access instead of having to configure environment variables per workspace. This is a more advanced operation and not required on your first go-round, but keep it in mind as you scale up to managing many workspaces later on. {{&lt; /admonition &gt;}}</p> <p>Connecting your git repository to your Terraform workspace can also be done for automatically planning on commit. This forces changes to come through your git commits instead of being able to run locally, which can be great for ensuring source control truly is the equivalent of your release when working with a team.</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#tearing-down-infrastructure","title":"Tearing Down Infrastructure","text":"<p>To tear down the infrastructure we just deployed, you can run: <code>terraform destroy</code> and approve the resulting preview it gives you.</p> <p>If you are using Terraform Cloud, in order to destroy a remote workspace (by queuing the destroy then destroying the workspace fully), you'll need to ensure the environment variable is set in the remote workspace for <code>CONFIRM_DESTROY = 1</code></p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-started-with-terraform/#wrap-up","title":"Wrap up","text":"<p>Terraform documentation is pretty solid on all the provider resources, so you can normally copy and paste (or use vscode extension mentioned). Another great way to learn is to look at github and the various Terraform modules that have been published. You can see how they structure their code a bit better and learn from that as well.</p> <p>If you are using Visual Studio Code, also download the Hashicorp Terraform extension for extra support.</p> <p>Good luck! If any steps were unclear or confusing please put in a comment and I'll do my best to improve this for an initial on-boarding experience for a new Terraform user.</p>","tags":["tech","infrastructure-as-code","devops","terraform"]},{"location":"getting-starting-with-python-for-data-exploration/","title":"Getting Started With Python For Data Exploration","text":"","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#intro","title":"Intro","text":"<p>To get started with running python queries with SQL Server is actually pretty easy. I waited far too long to tackle this, thinking it was much harder than it turned out to be. Hopefully, this can be a good starting point for you as well.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#why-would-i-want-to-do-this","title":"Why would I want to do this?","text":"<p>When you use great tools like dbatools with PowerShell, you come to think that PowerShell is a first class citizen for working with SQL Server. This is true in regards to administration and general usage. However, data exploration is a different beast.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#powershell-net-are-not-designed-for-adhoc-data-exploration","title":"PowerShell &amp; .NET Are Not Designed for Adhoc Data Exploration","text":"<p>First, can they? Yes. But just because there is a library someone created to give you some of these features, are the languages themselves first class data exploration languages, especially for adhoc and exploratory work. I think most would agree: no.</p> <p>To be very clear, I know there are possibilities, such as the newer Azure Data Studio powershell notebook feature, and likely other modules that can help with exploration. What I'm trying to be clear about is that those are all exceptions to the rule, whereas exploration in Python is a first class citizen from the start. (coming from a dedicated PowerShell addict )</p> <p>Traditionally, I've used Azure Data Studio or SQL Management Studio and run individually queries in a large sql script to explore and manipulate the data. However, once you start wanting to pivot, aggregate, sum, and do various other experimental actions, T-SQL can be over complicated to do this for that adhoc work. Just because you can do unpivot doesn't mean it's a smarter use of time than using a pivot table in Excel for adhoc work (as much as it pains me to say).</p> <p>{{&lt; admonition type=\"info\" title=\"Azure Data Studio\" &gt;}}</p> <p>Azure data studio is making great progress with SQL Server based notebooks, which I highly recommend checking out. It's still in its infancy. So while it's pretty useful for basics, if you want to stick with a scripted approach to explore, python will be superior. Just recently PowerShell based code cells are available too, I believe.</p> <p>{{&lt; /admonition &gt;}}</p> <p>However, even better than playing around with data in an Excel sheet, what if you wanted to stay in your programmatic script and start slicing and dicing the data coming back, molding into \"tidy data\" for what you want to analyze?</p> <p>PowerShell and .NET don't have the intuitive and experimental data exploration capabilities that have existed for a long time in the Python libraries. They can easily manipulate, group, mold and visualize the data with minimal effort. This is the forte of Python's libraries, and has much more maturity than doing some homegrown solution work around.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>I'm sticking with Visual Studio Code, so install the python extension <code>ms-python.python</code> and then create a new notebook directly from the command pallet. Once you open this up you can start writing some markdown notes and then running queries and visualizing the results in one place.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#python-plotly-library","title":"Python Plotly Library","text":"<p>The standard libraries are pretty good. I wanted something that also created graphs that I'd consider pleasing to the eye. The default <code>matplotlib</code> wasn't really what I was looking for, so I used <code>plotly</code>. This fantastic library has an express edition that gets you up and running in minutes playing with data, and can even convert your visualizations into a nice web app dashboard with <code>Dash</code> (also a plotly product).</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#pandas","title":"Pandas","text":"<p>In addition to working with <code>plotly</code> I leveraged <code>pandas</code>. I think from my initial working with it, that this library will be a staple for my adhoc data work for a long time. It basically provides a nice easy way to work with <code>Dataframes</code>, which is kinda of like working with a .NET <code>datatable</code>, albeit, many extensibility, exporting, and other features built into it. I found it intuitive and pretty easy to work with.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#installing-pyodbc","title":"Installing pyodbc","text":"<p>{{&lt; admonition type=\"info\" title=\"platform\" &gt;}}</p> <p>This guide is written for someone using a Windows based development environment. The setup requirements for running in Linux/Mac will be different for drivers</p> <p>For running in AWS Lambda in python runtime, there are more complex issues requiring the driver to be compiled and uploaded to a Lambda layer for use. Windows dev machine doesn't have this requirement and should just work after running ODBC driver install and pyodbc. </p> <p>{{&lt; /admonition &gt;}}</p> <ol> <li>Link for SQL Server ODBC Drivers</li> <li><code>pip3 install pyodbc</code></li> </ol>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#generate-connection-file","title":"Generate Connection File","text":"<p>If you are constantly changing the SQL Server instance you are connecting to, then you may just want provide variables for your script. In my test case, I wanted to create a default connection file that I could use in several scripts to avoid having to set in each module file.</p> <p>I found some great examples online and cobbled this together from several.</p> <pre><code>import configparser\nimport keyring\nimport xlsxwriter\n\nservice_account = \"DomainOfTheTaco\\\\TacoEater\"\nsql_login_name = \"TACO-HIGH-PRIEST\"\nsql_instance = \"TACO-SERVER-01\"\n\ndriver_name = ''\ndriver_names = [x for x in pyodbc.drivers() if x.endswith(' for SQL Server')]\nprint(\"The following ODBC drivers were identified on your system\")\nprint(driver_names)\n\nif driver_names:\n    driver_name = driver_names[0]\nif driver_name:\n    config = configparser.ConfigParser()\n    config['SQL_SERVER'] = {\n                            'DRIVER': driver_name,\n                            'SERVER': sql_instance,\n                            'DATABASE': \"master\",\n                            'Trusted_Connection': 'yes',\n                            \"APP\": \"PythonTamer\",\n                            \"UID\": sql_login_name,\n                            \"PWD\": keyring.get_password(service_account,service_account)\n    }\n    with open('db_connect.ini', 'w') as configfile:\n        config.write(configfile)\nelse:\n    print('(No suitable driver found. Cannot connect.)')\n</code></pre> <p>Since I had 2 versions of ODBC drivers available, this is what came up:</p> <pre><code>The following ODBC drivers were identified on your system\n['ODBC Driver 13 for SQL Server', 'ODBC Driver 17 for SQL Server']\n</code></pre> <p>This generated a connection file I could use in other py files I wanted. The file generated in my user directory <code>$ENV:USERPROFILE</code>.</p> <pre><code>[SQL_SERVER]\ndriver = ODBC Driver 17 for SQL Server\nserver = TACO-SERVER-01\ndatabase = master\ntrusted_connection = yes\napp = PythonTamer\nuid = TACO-HIGH-PRIEST\npwd = redactedtacos\n</code></pre> <p>This could then be used in python files like below</p> <pre><code>import configparser\nimport pyodbc\nimport pandas\n\nconfig = configparser.ConfigParser()\nconfig.read('db_connect.ini')\nconstr = 'DRIVER={{{drv}}};SERVER={srv};DATABASE={db};Trusted_Connection={tc};APP={app};UID={uid};PWD={pwd}'\\\n              .format(drv=config['SQL_SERVER']['DRIVER'],\n                      srv=config['SQL_SERVER']['SERVER'],\n                      db=config['SQL_SERVER']['DATABASE'],\n                      tc=config['SQL_SERVER']['Trusted_Connection'],\n                      app=config['SQL_SERVER']['APP'],\n                      uid=config['SQL_SERVER']['UID'],\n                      pwd=config['SQL_SERVER']['PWD']\n                      )\n\ncnxn = pyodbc.connect(constr)\n</code></pre> <p>This connection object then would allow running queries similar to executing <code>ADO.NET</code> queries.</p> <pre><code>out_xlsx = \"C://temp//python-sql-export.xlsx\"\ncnxn = pyodbc.connect(constr)\nresult = pd.read_sql(\"\"\"\nselect\n    name\nfrom\n    sys.databases\n\"\"\",cnxn)\nprint(result)\n\nprint(f\"Exporting to {out_xlsx}\")\ndata_frame = pd.DataFrame(result)\ndata_frame.to_excel(out_xlsx, index = None, header=True)\nprint(\"finished query\")\n</code></pre> <p>To sample in your notebook a few random entries, try this</p> <pre><code>data_frame.sample(n=20,random_state=1)\n</code></pre> <p>You should have an excel workbook exported after this. This was even more succinct than using <code>ImportExcel</code> PowerShell module which is my normal go to.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#group-results-into-ranges","title":"Group Results Into Ranges","text":"<pre><code>import plotly.express as px\n\n# How many results are being returned?\nprint(len(data_frame.value))\n\n# These are the buckets I want to return results in\n# Ie, how many of my results fit into the 0-250 range, and how many 500. These are not fixed ranges.\n# Histograms do this type of work as well, I'm showing the grouping overriden at specific ranges with a dataframe in Pandas.\n\nranges = [0,250,500,1000,2000,3000,4000,5000,6000,7000,10000]\ngroupby = data_frame.groupby(pd.cut(data_frame.value, ranges)).count()\nprint(groupby)\n\n# Plotly express visualization\nfig = px.histogram(data_frame, x=\"time\", y=\"value\",title='MyFancyTitle')\nfig.show()\n</code></pre> <pre><code>               time  metric  value\nvalue\n(0, 250]       1652    1652   1652\n(250, 500]       22      22     22\n(500, 1000]      10      10     10\n(1000, 2000]      9       9      9\n(2000, 3000]      2       2      2\n(3000, 4000]      1       1      1\n(4000, 5000]      5       5      5\n(5000, 6000]      9       9      9\n(6000, 7000]      2       2      2\n(7000, 10000]     1       1      1\n</code></pre> <p>This is pretty cool, as while you can do this with windowing functions in SQL Server, for adhoc work, I was able to do this in 2 lines. This could save a lot of time and debugging over more complex T-SQL procs.</p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#scatterchart","title":"Scatterchart","text":"<p>Never having really used scattercharts, I'm seeing in data exploration they can be a tremendously valuable tool.</p> <pre><code>print(len(data_frame.value))\nfig = px.scatter(data_frame, x=\"metric\", y=\"value\",title='MyFancyTitle',size=\"value\",render_mode=\"auto\",marginal_y=\"rug\",marginal_x=\"histogram\")\nfig.show()\n</code></pre>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#quick-example","title":"Quick Example","text":"<pre><code>pip3 install pydataset #optional for more than iris data\n</code></pre> <pre><code>from pydataset import data\ntitanic = data('titanic')\nprint(titanic.sample(n=20,random_state=1))\n</code></pre> <p>This results in output like the image below</p> <p></p> <p>To generate a more advanced visualization, you can run something to visualize with <code>plotly</code>.</p> <pre><code>import pandas\nimport plotly.express as px\nfrom pydataset import data\n\ndf = data('iris')\ndf.sample(n=10,random_state=1)\n\nprint(len(df))\nfig = px.scatter(df, x=\"Petal.Width\", y=\"Sepal.Width\",title='Iris Data Example',render_mode=\"auto\",marginal_y=\"rug\",marginal_x=\"histogram\")\nfig.show()\n</code></pre> <p></p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"getting-starting-with-python-for-data-exploration/#wrap-up","title":"Wrap Up","text":"<p>Lots of great options to visualize data with Python  . I'll definitely be spending more time learning how to manipulate and query datasets with Python, and eventually give Dash by Plotly a try as well. I've enjoyed Grafana, but it's really meant for time series monitoring, not for the flexible data visualization options that can be done so easily in Python.</p> <p>Hopefully, this gave you a good start on some of the benefits and ease of use of Python if you, like me, are coming from a PowerShell and SQL Server heavy background, and have wanted to know more about Python. </p>","tags":["devops","sql-server","python","tech","data-visualization","dbatools"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/","title":"Improving the Quality of Your Automation Logging with Cowsay","text":"","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#automation-taxes-your-sanity","title":"Automation Taxes Your Sanity","text":"<p>You have to glue together systems with your amazing duct taped scripts.</p> <p>You see failure after failure.</p> <p>You want help predicting the success of your next run, so I'm going to provide you with an advanced artificially intelligent way to do this through the power of <code>npm install</code>.</p>","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#npm","title":"NPM","text":"<pre><code>npm install cowsay -g\nnpm install lucky -g\nnpm install catme -g\n</code></pre> <pre><code> _____________________________________\n&lt; Will my run succeed this time? Hmmm &gt;\n        \\    ,-^-.\n         \\   !oYo!\n          \\ /./=\\.\\______\n               ##        )\\/\\\n                ||-----w||\n                ||      ||\n\n               Cowth Vader\n</code></pre> <pre><code> ________________________________________\n&lt; Will my run succeed this time? No way! &gt;\n ----------------------------------------\n   \\\n    \\\n     \\\n        __ \\ / __\n       /  \\ | /  \\\n           \\|/\n       _.---v---.,_\n      /            \\  /\\__/\\\n     /              \\ \\_  _/\n     |__ @           |_/ /\n      _/                /\n      \\       \\__,     /\n   ~~~~\\~~~~~~~~~~~~~~`~~~\n</code></pre> <p>Now include the header in your script</p> <pre><code>\"Will my run succeed this time? $( lucky --eightball)\" | cowsay -r\n</code></pre> <p>Or spice up your console with a friendly cat using <code>catme</code></p> <pre><code> /\\     /\\\n{  `---'  }\n{  O   O  }\n~~&gt;  V  &lt;~~\n \\  \\|/  /\n  `-----'__\n  /     \\  `^\\_\n {       }\\ |\\_\\_   W\n |  \\_/  |/ /  \\_\\_( )\n  \\__/  /(_E     \\__/\n    (  /\n     MM\n</code></pre> <pre><code>  /\\ ___ /\\\n (  o   o  )\n  \\  &gt;#&lt;  /\n  /       \\\n /         \\       ^\n|           |     //\n \\         /    //\n  ///  ///   --\n</code></pre> <p>There's a few PowerShell related one's, but I honestly just use other packages for this this.</p>","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#python","title":"Python","text":"<p>I just looked and found out there's a few great python equivalents so you could easily run some great stuff. They've got cowsay, a benedict cumberbatch like name generator, and more. I think I fell in love with Python a little bit more today.</p>","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#level-up","title":"Level Up","text":"<p>Level it up by installing <code>lolcat</code> and if running Cmder you'll enjoy the Skittlitizing of your console output.</p> <p>PowerShell version is: <code>Install-Module lolcat -Scope CurrentUser</code></p> <pre><code>\"Will my run succeed this time? $( lucky --eightball)\" | cowsay -r | lolcat\n</code></pre> <p>The resulting majesty:</p> <p></p>","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#linux-powerup","title":"Linux Powerup","text":"<p>If you want to really wow your friends... just jump into bash inside windows and run from your cli. This is a level so far beyond the normal windows command line experience it might just make you uninstall windows and live purely on a Linux terminal for your remaining days.</p> <p>This looks like a good background while waiting to start presentations </p> <pre><code># first time if repos out of date\nsudo apt update\nsudo apt install cmatrix\n</code></pre> <p></p>","tags":["cli","cool-tools","devops","tech"]},{"location":"improving-the-quality-of-your-automation-logging-with-cowsay/#resources","title":"Resources","text":"<ul> <li>Benerator CumberPy</li> <li>Dadjokes-cli</li> <li>Cownet</li> </ul> <p>{{&lt; admonition type=\"warning\" title=\"disclaimer\" &gt;}}</p> <p> There are a couple ascii layouts that are a little inappropriate, so if doing a live demo or something more visible don't use random mode if you don't want something showing up that might embarass you </p> <p>{{&lt; /admonition &gt;}}</p>","tags":["cli","cool-tools","devops","tech"]},{"location":"go-r1-day-26/","title":"Go R1 Day 26","text":"","tags":["microblog","golang","100DaysOfCode","development","tech"]},{"location":"go-r1-day-26/#progress","title":"Progress","text":"<ul> <li>Evaluated gorm usage best practices with Slack Gopher community.</li> <li>Obtained a great example to get me started on go routine and channels usage with multi-database queries.</li> </ul>","tags":["microblog","golang","100DaysOfCode","development","tech"]},{"location":"running-sql-server-in-docker/","title":"Running SQL Server in Docker","text":"<p>{{&lt; admonition type=\"info\" title=\"Updated 2020-05-05\" &gt;}}</p> <p>I've had lots of challenges in getting docker for sql-server working because I've wanted to ensure for my dev use case that there was no need for virtual volume management and copying files into and out of this. Instead, I've wanted to bind to local windows paths and have it drop all the mdf/ldf right there, so even on container destruction everything is good to go.</p> <p>After working through the changes in SQL 2019 that require running as non-root, I've gotten it work again. No install of sql-server needed. Easy disposable development instance through docker! I'll update my docker compose content when I can, but in the meantime, this should get you running even more quickly with SQL Server 2019.</p> <pre><code>docker run `\n    --name SQL19 `\n    -p 1433:1433 `\n    -e \"ACCEPT_EULA=Y\" `\n    -e \"MSSQL_SA_PASSWORD=ThisIsNotARealPassword@!1}\" `\n    -v C:\\mssql\\SQL19:/sql `\n    -d mcr.microsoft.com/mssql/server:2019-latest\n\ndocker run `\n    --name SQL19WithSpaces `\n    -p 1434:1433 `\n    -e \"ACCEPT_EULA=Y\" `\n    -e \"MSSQL_SA_PASSWORD=ThisIsNotARealPassword@!1}\" `\n    -v C:\\mssql\\SQL19WithSpaces:/sql `\n    -d mcr.microsoft.com/mssql/server:2019-latest\n</code></pre> <p>{{&lt; /admonition &gt;}}</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#why-use-docker-for-mssql","title":"Why Use Docker for MSSQL","text":"<p>Say you have an onboarding and need to get a new developer up to speed. Have you gone through the setup process for SQL Server on Windows? It's a bit tedious and even with running scripts to install (or in my case I wrapped up with Chocolatey), it's still a lot of things you need, including possibly a reboot if missing some .net dependencies.</p> <p>The normal solution is to leverage more localdb for quickly deploying and isolating the database.</p> <p>This has its own set of challenges that personally I like to avoid. Localdb is more isolated, but it's not truly like running a local SQL Server standard edition, and debugging permissions, sharing, and other things can with its own set of steps to work through. I've seen it common for many devs to just avoid these issues and run a local SQL Server installation to just simplify and work with SQL Server as it's expected to be.</p> <p>I'd explored Docker SQL Server containers in the past, but one big issue for adoption to me was the issues I had mounting the local Windows folders to the Linux based container. \u2753 Why is this important? Ease of usage for a developer. If I proposed this would save effort to many developers working with SQL Server, I'm pretty sure telling them that they'd have to copy a backup or database file in via CLI to a virtual mounted drive that they can't easily manage would be a deal-breaker. I basically wanted to ensure if they spun up SQL Server in a container, the database backups, mdf and ldf files, and any other created files would be able to persist outside of that instance, ensuring easy development workflow.</p> <p>I'm happy to say I finally have it working to my satisfaction, resolving most of those concerns.</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#scenarios-this-is-good-for","title":"Scenarios This is Good For","text":"<p>If you've desired to do the following, then using Docker might end up saving you some effort.</p> <ul> <li>simplify the setup of a new SQL Server instance</li> <li>be able to reset your SQL Server instance to allow testing some setup in isolation</li> <li>be able to switch SQL Server editions to match a new requirement</li> <li>be able to upgrade or patch to a later version of SQL Server with minimal effort</li> </ul> <p>This is not for production. There's more effort and debate that goes on to using SQL Server in containers, Kubernetes (even more complex!), and other containerization approaches that are far outside the scope of what I'm looking at. This is first and foremost focused on ensuring a development workflow that reduces complexity for a developer and increases the ease at which SQL Server testing can be implemented by making the SQL Server instance easily disposed of and recreated.</p> <p>If this also means I spread some \ud83d\udc27 Linux magic to some Windows stack developers... then count it as a sneaky win. \ud83d\ude0f</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#sql-server-on-linux-or-windows-containers","title":"SQL Server on Linux or Windows containers","text":"<p>The following example is done with SQL Server on Linux. As I'm already comfortable with SQL Server on Windows, I wanted to be try this on Linux based container. I also wanted to continue using Linux based containers for tooling, and not have to switch back to Windows containers for the sole purpose of running SQL Server. At the time I began this testing, I found it was exclusive. You either ran Linux or Windows-based containers. This is changing with the advent of new features in Docker that are there to allow side by side Windows + Linux based containers.</p> <p>Release notes indicate:</p> <p>Experimental feature: LCOW containers can now be run next to Windows containers (on Windows RS3 build 16299 and later). Use --platform=linux in Windows container mode to run Linux Containers On Windows. Note that LCOW is experimental; it requires the daemon experimental option. Docker Community Edition 18.03.0-ce-win59 2018-03-26</p> <p>The main difference in your local development process will be Windows Authentication vs SQL Authentication. Use SQL Authentication with Linux based SQL Server</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#docker-compose-example","title":"Docker Compose Example","text":"<p>The following is the result of a lot of trial and error over a year. Thanks to Shawn Melton \ud83d\udc4f also for providing me with support doing my troubleshooting as part of the SQL Server community in Slack. You can find Shawn's example I used as a starting point for my \ud83e\uddeatesting in this gist.</p> <p>A few helpful tips:</p> <ol> <li>Remove <code>-d</code> for detached and you can see the SQL Server console output in the console.</li> <li>See the persisted databases (system and user!) in the artifacts directory after docker-compose begins running.</li> </ol> <p>{{&lt; gist sheldonhull  a70a3a731b329b67f47a331c64c72ab5 &gt;}}</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#improving-code-tests","title":"Improving Code Tests","text":"<p>Another reason I'm really excited about is the ability to better support testing through tools like Pester. Yes, it's a geek thing, but I love a solid written test \ud83e\uddeathat maintains my code and helps me ensure quality with changes. Better TDD is a goal of mine for sure.</p> <p>\ud83d\udd28 This supports implementation of better tests by providing the potential for spinning up a local SQL Instance, restoring a test database, and running a sequence of actions against it with pass or fail without the additional infrastructure requirements to have this done on another server. Making your tests that are not full integration testing as minimally dependent on external factors is a fantastic step to saving you a lot of work.</p> <p>A simple pester (PowerShell) might frame the start of a test like this:</p> <pre><code>Before All {\n    docker-compose up -d\n    Import-Module Dbatools\n    # Wait Until dbatools confirms connectivity through something like test-dbaconnection, then proceed with tests\n    # Test-DbaConnection ....\n    # Restore-DbaDatabase ...\n}\n\nAfter All {\n    docker-compose down --volume\n}\n\nDescribe \"DescribeName\" {\n    Context \"ContextName\" {\n        It \"ItName\" {\n            Assertion\n        }\n    }\n}\n</code></pre>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"running-sql-server-in-docker/#wrap-up","title":"Wrap Up","text":"<p>Hope this helps someone. I spent at least a year coming back over time to this hoping to actually get it working in a way that felt like a first-class citizen and reduced complexity for development work.</p> <p>I'm pretty happy with the results. \ud83d\ude01</p>","tags":["devops","docker","sql-server","tech","dbatools"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/","title":"A Smattering of Thoughts About Applying Site Reliability Engineering principles","text":"","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#whats-this-about","title":"What's This about","text":"<p>I figured I'd go ahead and take this article which I've gutted several times and share some thoughts, even if it's not an authority on the topic. \ud83d\ude00</p> <p>In the last year, I've been interested in exploring the DevOps philosophy as it applies to operations as well as software development. I originally started various drafts on these concepts back before April 2019, but never got around to finishing it until now.</p> <p>This isn't a very focused post, more a smattering of thoughts, so let's get to it! \ud83d\udcaa</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#devops-is-a-challenge","title":"DevOps Is A Challenge","text":"<p>Having come from a development background, applying DevOps to the operational side of the fence is an interesting challenge.</p> <p>There are so many things that can be unique to the evolving way a team you are part of is growing and learning, that it can be difficult sometimes to know what is \"best practice\" and was is \"essential practice\" in a DevOps culture.</p> <ul> <li>What does it mean to plan and scale your workflow in a DevOps culture?</li> <li>How do operational stability and the pressure for new feature delivery in a competitive market meet in a healthy tension?</li> <li>How do you do \"everything-as-code\" in practice?</li> <li>Lots more!</li> </ul> <p>There are various ways to implement DevOps, and core principles to DevOps. I've found that one that really resonated with me as I've looked for how others have done this.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#site-reliability-engineering","title":"Site Reliability Engineering","text":"<p>Google's Site Reliability Engineering (SRE) material provides a solid guide on the implementation of SRE teams. They consider SRE and DevOps in a similar relationship to how Agile views Scrum. Scrum is an implementation of Agile tenants. Site Reliability Engineering is an implementation of a DevOps culture, with guidelines on how to approach this in a systematic way.</p> <p>If you think of DevOps like an interface in a programming language, class SRE implements DevOps. Google SRE</p> <p>What I like about the material, is that a lot of the fixes I've considered to improvements in workflow and planning have been thoughtfully answered in their guide, since it's a specific implementation rather than a broader philosophy with less specific steps.</p> <p>Regardless of where you are in your journey, a lot of the principles have much to offer. Even smaller organizations can benefit from many of these concepts, with various adjustments being made to account for the capabilities of the business.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#recommended-sre-reading","title":"Recommended SRE Reading","text":"<p>Recommended reading if this interests you:</p> Link Deploys: It\u2019s Not Actually About Fridays \u2013 charity.wtf DevOps Topologies Do you have an SRE team yet? How to start and assess your journey Google - Site Reliability Engineering Love (and Alerting) in the Time of Cholera (and Observability) \u2013 charity.wtf","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#where-i-started","title":"Where I Started","text":"<p>At the beginning of my tech career, I worked at a place that the word \"spec\" or \"requirement\" was considered unreasonable. Acceptance criteria would have been looked down upon, as something too formal and wasteful.</p> <p>While moving towards to implementation of any new project, I was expected to gather the requirements, build the solution, ensure quality and testing, and deploy to production. That \"cradle to grave\" approach done correctly can promote the DevOps principles, such as ensuring rapid feedback from the end-users and ownership from creation through to production.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#a-different-take","title":"A Different Take","text":"<p>I've been in a somewhat interesting blend of roles that gives me some insight into this. As a developer, I always look at things from an \ud83e\udd16 automation &amp; coding perspective. It's the exception, rather than the norm for me to do it manually without any type of way to reproduce via code.</p> <p>I've also been part of a team that did some automation for various tasks in a variety of ways, yet often resolved issues via manual interactions due to the time constraints and pressures of inflowing work. Building out integration tests, code unit tests, or any other automated testing was a nice idea in theory, but allocating time to slow down and refactor for automated testing on code, deployments, and other tasks were often deemed too costly or time prohibitive to pursue, in addition to requiring a totally different skillset to focus on.</p> <p>For example, you can't write code based tests against a deployment, unless you have made the time and effort to learn to code in that language and work through the pain of discovery in writing integration oriented tests. It's not a simple feat \ud83e\uddb6 to just pick up a language and start writing tests in it if you've never done this before.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#reactive","title":"Reactive","text":"<p>Reactive work also challenges DevOps focused teams that do operational work.</p> <ul> <li>Difficult to categorize emergency from something that could be done in a few weeks</li> <li>Difficult to deliver on a set of fully completed tasks in a sprint (if you even do a sprint)</li> <li>High interrupt ratio for request-driven work instead of able to put into a sprint with planning. (This is common in a DevOps dedicated team topology that is in some of the Anti-Types mentioned in DevOps Anti-Type topologies)</li> <li>Sprint items in progress tend to stay there for more than a few days due to the constant interruptions or unplanned work items that get put on their plate.</li> <li>Unplanned work items are constant, going above the 25% buffer normally put into a sprint team.</li> <li>Continued feature delivery pressure without the ability to invest in resiliency of the system.</li> </ul> <p>Google has a lot more detail on the principles of \"on-call\" rotation work compared to project-oriented work. Life of An On-Call Engineer. Of particular relevance is mention of capping the time that Site Reliability Engineers spend on purely operational work to 50% to ensure the other time is spent building solutions to impact the automation and service reliability in a proactive, rather than reactive manner.</p> <p>In addition, the challenges of operational reactive work and getting in the zone on solving project work with code can limit the ability to address the toil of continual fixes. Google's SRE Handbook also addresses this in mentioning that you should definitely not mix operational work and project work on the same person at the same time. Instead whoever is on call for that reactive work should focus fully on that, and not try to do project work at the same time. Trying to do both results in frustration and fragmentation in their effort.</p> <p>This is refreshing, as I known I've felt the pressure of needing to deliver a project, yet feeling that pressure of reactive work with operational issues taking precedence.</p> <p>It's important to recognize the importance of that operational work. It helps ensure a team is delivering what is most important to a business, a happy customer base! At the same time, always living in the reactive world can easily cause the project related work and reliability improvements that will support those efforts to suffer if a intentional plan to handle this is not made.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#reducing-toil","title":"Reducing Toil","text":"<p>I've always been focused on building resilient systems, sometimes to my own detriment velocity wise. Balancing the momentum of delivery features and improving reliability is always a tough issue to tackle. Automation isn't free. It requires effort and time to do correctly. This investment can help scaling up what a team can handle, but requires slower velocity initially to do it right.</p> <p>How do you balance automating and coding solutions to manual fixes, when you often can't know the future changes in priority?</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#it-can-be-applied-to-immediate-needs","title":"It can be applied to immediate needs","text":"<p>A leader I've worked with mentioned one great way is to ensure whatever solution you build is immediately applicable to solving work in progress. By this, he meant work that could immediately benefit a team, rather than building things for future possibilities that haven't yet come into play. This aligns with a LEAN mentality and there is a lot to like about it.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#it-solves-an-immediate-pain","title":"It solves an immediate pain","text":"<p>Another way I've begun looking at this is solving the key pain-points that are mentioned by others. I tend to look at many improvements, and it can be difficult to choose what area to tackle at times, as I can see value in many of them.</p> <p>However, solving paint points that are directly impacting someone enough that they are asking for help should take precedence in many cases (not all).</p> <p>Why? This pain-point solved gains you an immediate win.</p> <p>If you are solving an issue that others are ok with, or don't recognize as an issue, then you have an uphill battle for adoption compared to solving something immediately impacting their work. This doesn't negate addressing problems that others have not yet recognized, it is just something that's become clear can help in moving forward with improvements with less resistance.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#last-priority-solving-problems-no-one-yet-knows-they-need-solved","title":"Last Priority - Solving Problems No One Yet Knows They Need Solved","text":"<p>There is an important caveat to all of this. There is a realm of problems when you have expertise in an area that you will identify that others don't see. When assessed against goals of a business, these can be critical.</p> <p>In my experience, the only way for these identified issues to be clearly prioritized is having a strong ability to communicate the problem and design some clear objectives to place this into the team backlog to address.</p> <p>Verbally communicating, while important, won't have any staying power compared to a clearly defined objective in a backlog and advocated for in a presentation that helps raise concern for the same issue you care about.</p> <p>They key is that your signal-to-noise-ratio is good, and when you raise the concern about the issue others don't have to work hard to understand why it's a problem.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#how-to-balance-product-vs-reliability-improvements","title":"How to balance product vs reliability improvements","text":"<p>SREs balance the juggling of product velocity and reliability improvements in a unique way... the usage of SLO and the error budget concept.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#slo-error-budgets","title":"SLO &amp; Error Budgets","text":"<p>The concepts of having an error budget really struct home for me. Clear Service Level Objectives (SLO) and error budgets to work with helps ensure both the development velocity and stability desired by operations is handled correctly.</p> <p>Error budgets provide the key ingredient to balancing new feature delivery, while still ensuring happy customers with a stable system.</p> <p>One of the best sections I've read on this was: Embracing Risk.</p> <p>Error budgets are discussed, and internal SLI (Server Level Indicators). These are integral to ensuring a balance of engineering effort in balance with new feature delivery. The goal of 100% reliability, while sounding great, is inherently unrealistic.</p> <p>Product development performance is largely evaluated on product velocity, which creates an incentive to push new code as quickly as possible. Meanwhile, SRE performance is (unsurprisingly) evaluated based upon the reliability of a service, which implies an incentive to push back against a high rate of change. Information asymmetry between the two teams further amplifies this inherent tension.</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"a-smattering-of-thoughts-about-applying-site-reliability-engineering-principles/#wrap-up","title":"Wrap Up","text":"<p>I've been mulling over this for a while, and find a lot of the concepts very helpful as I've been challenged with both operational and development focus. As always, these are personal thoughts and don't reflect the opinion of my employer. I hope it gives you some food for thought. If you have some feedback, just post up a comment and let's dialogue!</p>","tags":["agile","working-smart","site-reliability-engineering","tech","devops"]},{"location":"using-pester-to-improve-operational-tasks/","title":"Using Pester to Improve Operational Tasks","text":"","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#taking-the-time-to-test","title":"Taking the Time To Test","text":"<p>Requirements in a constant state of change set you up for failure.</p> <p>Failure to work through requirements before starting can also increase the risk of failure.</p> <p>Planning with defined acceptance criteria of work you are committing to is not about filling in more documentation and work that no one cares about. Properly defining the acceptance criteria for yourself is about the exploratory process that defines and limits the scope of the work to deliver the minimum viable product. This allows continued momentum and delivery of value to the business.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#doesnt-this-create-more-busy-work-for-me","title":"Doesn't this create more busy work for me?","text":"<p>Without effective acceptance criteria, you are setting yourself up for more work, and thus likely to deliver value to the business as quickly. From my perspective, if you cannot truly define the acceptance criteria for work, then it's likely the effort will result in less value, be difficult to deliver, or end up with scope creep.</p> <p>This is a critical thing to communicate to any teams struggling with reactive work. Without spending time ensuring proactive planning and defining the requirement and acceptance criteria, more time is often spent reworking and solving problems that might have been better handled with a little planning and forethought.</p> <p>A classic example of unclear acceptance criteria and the wasteful impact of work is from a youtube clip here.</p> <p>{{&lt; youtube BTTdHW8Z668 &gt;}}</p> <p>How many times have you started work on a project and found yourself in a similar situation?</p> <p>\"There are many reasons why software projects go wrong. A very common reason is that different people in the organization or on the team have very different understandings of how the software should behave, and what problems it\u2019s trying to solve. Ignorance is the single greatest impediment to throughput\" Teams that deliberately seek to discover what they are ignorant about before development starts are more productive because there is less rework. The most effective way to do this is through conversation and collaboration between key stakeholders... Dan North</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#small-steps-in-testing","title":"Small Steps In Testing","text":"<p>Now that I did my high-level philosophizing ... let's do something more low level and fun.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#an-example-of-adding-tests-to-devops-oriented-tasks","title":"An example of adding tests to DevOps oriented tasks","text":"<p>If we want to apply the \"software engineer solving operational problems\" approach to a very simple task, we could take the example of deploying a logging agent.</p> <p>A software engineer expects a healthy culture to include tests for any code they write. This ensures better quality, more rapid velocity in changes, and other benefits that TDD advocates can tell you in other places.</p> <p>For our example, let's say we are deploying a monitoring agent on some servers.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#gherkin","title":"Gherkin","text":"<p>Gherkin is the syntax used to describe the tests.</p> <p>{{&lt; admonition type=\"info\" title=\"Pester Syntax\" &gt;}} You can write Pester tests with PowerShell in a different format, but I'm going to use Gherkin here because of its sentences and far less of a confusing DSL for our example. {{&lt; /admonition &gt;}}</p> <p>My understanding would be to compare Cucumber \"spoken word\", and Gherkin as the \"language\" chosen to communicate. When you write the feature files to work with Cucumber, you write them in Gherkin. Gherkin is supported for automated testing in many scripting languages. In a Windows environment, for example, you can use it to check many things like services running, connectivity with a server, installation of an app, and more using invocation via Pester.</p> <p>Most examples I've found on Cucumber are focused very much on user testing, like website clicks, saving things, and other very narrow development focused actions.</p> <p>What I've failed to see as much of is the discussion on the value of using this approach with teams implementing \"Infrastructure as Code\", operations teams, and other non-application specific roles.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#make-your-checklist-automated","title":"Make Your Checklist Automated","text":"<p>In my example, let's start small and say you just have PowerShell, and some servers.</p> <p>What I've discovered is that to actual validate DevOps oriented work is completed, you typically go through the equivalent of what a Cucumber test would have. This \"checklist\" of validations is often manually performed, lacking consistency and the ability to scale or repeat with minimal effort.</p> <p>Consider an alternative approach to helping solve this issue, and expanding your ability to automate the tedious testing and validation of changes made.</p> <pre><code>Scenario\nGiven (environment state)\nWhen (something is done)\nThen (expected result)\n</code></pre>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#using-pester-to-write-an-ops-test","title":"Using Pester To Write An Ops Test","text":"<p>Instead of using manual checks to validate a series of steps, let's see what a Cucumber oriented test might define as the acceptance criteria to confirm successfully finished, and how this might work with automating the tests.</p> <p>This is a simple MSI with some command-line arguments that might be required. You want to validate the chocolatey package you deploy with correctly parsed the arguments passed in, and then successfully registered with the logging services.</p> <p>An example feature file with Gherkin might look like this:</p> <pre><code>@class\nFeature: I can generate configuration files for FancyLoggingVendor collection dynamically\n\n    Background: The test environment\n        Given the test environment is setup\n\n    Scenario: Source json is configured to collect windows event logs\n        Given FancyLoggingVendor json object is initialized\n        When the package parameters request a Windows Event\n        Then the created json file contents match\n            \"\"\"\n{\n\"api.version\":\"v1\",\n\"sources\":\\[\n{\n\"sourceType\":\"LocalWindowsEventLog\",\n\"name\":\"GherkinTest\",\n\"renderMessages\":true,\n\"cutoffRelativeTime\":\"-1h\",\n\"hostname\":\"gherkintest\",\n\"logNames\":\\[\n\"Security\",\n\"Application\"\n\\]\n}\n\\]\n}\n            \"\"\"\n</code></pre> <p>Another one that shows Scenario Outlines. This adds more functionality to looping through various variations of test cases.</p> <pre><code>Feature: I can install FancyLoggingVendor on my machine\n\n    Background: The local package is updated and the functions loaded\n        Given the test environment is setup\n\n    Scenario Outline: As a local user, I can install FancyLoggingVendor and configure capture my desired target\n        Given this package is not installed\n        When the package source is a local nupkg file\n        And a &lt;Description&gt; is provided with the values: &lt;Paths&gt;\n        Then I can install from this source\n        And the sources json returns sources of: &lt;ExpectedSourcesCount&gt;\n        And the application should show up in installed programs\n        And the service should show up\n        And the service should be running\n        Examples: Source Variations\n            | Description              | Paths                              | ExpectedSourcesCount |\n            | Single_File              | C:\\\\temp\\\\taco.log                 | 1                    |\n            | Single_File_with_Filters | C:\\\\temp\\\\_.log                    | 1                    |\n            | SingleFolder             | C:\\\\temp\\\\foobar                   | 1                    |\n            | MultipleFolders          | C:\\\\temp\\\\foobar,C:\\\\temp\\\\foobar2 | 2                    |\n            | SingleFolder_with_filter | C:\\\\temp\\\\foobar\\\\_.log            | 1                    |\n</code></pre> <p>This provides us with a way to validate and test something like a chocolatey package installation or custom install script.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#leveraging-automation-to-scale-your-test","title":"Leveraging Automation To Scale your Test","text":"<p>So all that writing...what benefit? Is it going to save time?</p> <p>With PowerShell, I'd write something similar to match a called step to a code block here.</p> <p>{{&lt; admonition type=\"warning\" title=\"Warning\" &gt;}}</p> <p>Case sensitive name as of 2019-04-16 for keywords. Therefore to match steps, use <code>Before</code> not <code>before</code> or it won't match.</p> <p>{{&lt; /admonition &gt;}}</p> <pre><code># Uses PSFramework as greatly simplifies any variable scoping or general configuration work, making this type of test easily reusable\n#this is incomplete, just to give some practical examples of how I might use to repeat a test like this.\n\nBefore Each Scenario {\n# other prep work\n}\n\nGiven 'local package is updated and the functions loaded' {\n# package up the\n}\nGiven 'the test environment is setup' {\n# do setup stuff here or other stuff that might be needed\n}\nGiven 'this package is not installed' {\n{ choco uninstall (Get-PSFConfigValue 'gherkin.packagename') } | Should -Not -Throw\n}\nWhen 'the package source is a local nupkg file' {\nSet-PSFConfig 'gherkin.packagesource' -Value (Get-ChildItem -Path (Get-PSFConfigValue 'gherkin.packagefolder') -Filter *.nupkg).FullName\n}\n\nThen 'I can install from this source' {\n\n$ChocoArguments = @()\n$ChocoArguments += 'upgrade'\n$ChocoArguments += Get-PSFConfigValue 'gherkin.packagename'\n$ChocoArguments += '--source \"{0}\"' -f (Get-PSFConfigValue 'gherkin.packagefolder')\n$ChocoArguments += (Get-PSFConfigValue 'gherkin.params')\n$ChocoArguments += '--verbose'\n{ Start-Process choco.exe -ArgumentList $ChocoArguments -NoNewWindow -PassThru | Wait-Process } | Should -Not -Throw\n}\n\nAnd 'the sources json returns sources of: &lt;ExpectedSourcesCount&gt;' {\nparam($ExpectedSourcesCount)\nWrite-PSFmessage -Level Important -FunctionName 'FancyLoggingVendor.Install.steps.ps1' -Message \"ExpectedSourcesCount -- $ExpectedSourcesCount\"\n$DefaultJsonFile = 'PathToJsonHere'\nTest-Path $DefaultJsonFile -PathType Leaf | Should -Be $True\n@(Get-Content $DefaultJsonFile -Raw | ConvertFrom-Json | Select-Object Sources).Count | Should -Be $ExpectedSourcesCount\n}\nAnd 'the application should show up in installed programs' {\n\n# helper function could be loaded and used to parse registry info for installed app showing up, or you could code it directly\n\nGet-InstalledApp 'MyAgent' | Should -Be $true\n}\nAnd 'the service should show up' {\n@(Get-Service 'MyAgent*').Count | Should -Be 0\n}\nAnd 'the service should be running' {\n@(Get-Service 'MyAgent*' | Where-Object Status -eq 'Running').Count | Should -Be 0\n}\n</code></pre> <p>Now that I wrote this, I could simply run:</p> <pre><code>Invoke-Gherkin -Path InstallFancyLoggingVendor.steps.ps1\n</code></pre> <p>This would run all the steps from a feature file and ensure I'm able to repeat these tests after any change to confirm they work.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#other-great-use-cases","title":"Other Great Use Cases","text":"<p>I've leveraged this to validate SQL Server configuration changes on a new AWS RDS Deployment, validate build steps completed successfully, tested file paths existing, and more. I really like how you can have this all integrated in a nice UI by uploading the nunit tests in Azure DevOps pipelines too.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#start-small","title":"Start Small","text":"<p>Take a look at the simple Pester syntax examples or the gherkin examples I gave and use that to do anything you keep having to check more than a few times. You'll find your efforts rewarded by having more consistent testing and probably save quite a bit of effort as well.</p>","tags":["tech","devops","testing"]},{"location":"using-pester-to-improve-operational-tasks/#helped","title":"Helped?","text":"<p>If you found these concepts helpful and would like an example of using Pester to test SQL Server login authentication, user group assignment, and more, let me know. I've done SQL Server pester tests using traditional Pester syntax that would validate maintenance solution deployment, login mapping, and more. If this was valuable, I could do a write-up of this.</p> <p>If you also would like more fundamentals, I'd be willing to do a basic Pester write-up for an operational focused task that is ground zero too, just depends if you the reader find this helpful.</p> <p>Comments always appreciated if this helped you! Let's me know it's actually helping someone out and always great to connect with others. \ud83c\udf7b</p>","tags":["tech","devops","testing"]},{"location":"asciidoc-distributed-docs-as-code/","title":"Asciidoc Distributed Docs as Code","text":"","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#the-problem","title":"The Problem","text":"<ul> <li>I want to keep my code and my documentation in the same place.</li> <li>I want to separate the presentation layer from my documentation content.</li> <li>I want to be flexible to publish documentation to a variety of endpoints and formats as processes evolve, without my content being impacted.</li> <li>I want to write a solid runbook for things that can't be fully automated, but still include scripts and other code in their native format.</li> </ul> <p>Documentation is such an important part of a developer's life. I think we often take it for granted, and it's an afterthought in many projects. However, as I consider my work, I know that I'm not reinventing the wheel very often \ud83d\ude00. Most of what I do is built on the back of others' work. When I use tooling, I'm reading the documentation and using it as my basis to get work done. When I use my notes and blog posts as a reference, I'm using my informal version of knowledge gathering.</p> <p>INVEST in documenting your work as you go, for the person behind you. You don't find time to do it, you make time to do it while you work, as a first class citizen of your work, not an after-thought. Think of all the times you've had to dig for answers and save someone else that experience.</p> <p>You code and document not as much for yourself, but for the person that comes behind you.</p>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#asciidoctor","title":"Asciidoctor","text":"<p>I've found a happy solution in the Asciidoctor documentation format over markdown. You can go google this for more expanded understanding, but I've decided that other than for basic notes and blog posts which are very simplistic, I now choose Asciidoctor.</p> <p>Why use Asciidoc format over markdown comes down to the needs of technical documentation.</p> <p>Here are some key reasons why I've found Asciidoc format to be worth learning:</p> <ul> <li>I can reference code files with a simple <code>include::file[]</code> statement, while markdown would require me to embed my code directly as a code block.</li> <li>I can generate a table from a csv file, further helping me automate a refresh of the underlying data that is rendered to a table display</li> <li>I can create tables much more cleanly and with control than in markdown, even allowing nested tables for complicated process documentation.</li> <li>Automatic admonition callouts without extensions using simple statements like <code>IMPORTANT: foo</code></li> </ul>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#presentation","title":"Presentation","text":"<p>Since the common documentation system used where I am at is Confluence, I decided to leverage the incredible confluence-publisher project that made this entire process a breeze. Check the repo and the linked documentation out here: Confluence Publisher</p> <p>In the future, if I didn't use confluence, I'd explore rendering as a static website through Hugo (that's what this site is generated from) or revisit Antora and maybe merge my content into the format required by Atora programmatically.</p>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#use-docker","title":"Use Docker","text":"<p>Since Asciidoc is written in Ruby, use docker and you won't have to deal with dependency nightmares, especially on Windows.</p> <pre><code>$RepoDirectoryName = 'taco-ops-docs'\necho \"\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\ud83c\udf2e\"\necho \"Running confluence publisher \ud83c\udf2e\"\necho \"\ud83d\udcc3 Publishing $RepoDirectoryName repo contents\"\n\ndocker run --rm -v $BUILD_SOURCESDIRECTORY/$RepoDirectoryName/docs:/var/asciidoc-root-folder -e ROOT_CONFLUENCE_URL=$ROOT_CONFLUENCE_URL \\\n-e SKIP_SSL_VERIFICATION=false \\\n-e USERNAME=$USERNAME \\\n-e PASSWORD=$PASSWORD \\\n-e SPACE_KEY=$SPACE_KEY \\\n-e ANCESTOR_ID=$ANCESTOR_ID \\\n-e PUBLISHING_STRATEGY=$PUBLISHING_STRATEGY \\\nconfluencepublisher/confluence-publisher:0.0.0-SNAPSHOT\necho \"\ud83d\udcc3 Publishing $RepoDirectoryName repo contents finished\"\n</code></pre> <p>Yes... I know. I get bored reading log messages when debugging so my new year premise was to add some emoji for variety. Don't judge. \ud83d\ude01</p>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#distributed-docs-structure","title":"Distributed Docs Structure","text":"<p>So the above approach is fantastic for a single repo. I wanted to take it to a different level by solving this problem for distributed documentation. By distributed I meant that instead of containing all the documentation in a single \"wiki\" style repo, I wanted to grab documentation from the repositories I choose and render it. This would allow the documentation related to being contained in the repository it is related to.</p> <p>For instance, what if I wanted to render the documentation in the following structure:</p> <pre><code>** General Documentation**\ntaco-ops-runbook\n---&gt; building-tacos\n--------&gt; topic.adoc\n---&gt; eating-tacos\n--------&gt; topic.adoc\n---&gt; taco-policies\n--------&gt; topic.adoc\n---&gt; taco-as-code\n--------&gt; topic.adoc\n\n** Repo Oriented Documentation**\ngithub-repos\n---&gt; taco-migration\n--------&gt; category-1\n------------&gt; topic.adoc\n------------&gt; topic.adoc\n--------&gt; category-2\n------------&gt; topic.adoc\n------------&gt; topic.adoc\n---&gt; taco-monitoring\n--------&gt; category-1\n------------&gt; topic.adoc\n------------&gt; topic.adoc\n--------&gt; category-2\n------------&gt; topic.adoc\n------------&gt; topic.adoc\n</code></pre> <p>The only current solution found was Antora. Antora is very promising and great for more disciplined documentation approaches by software development teams. The limitation I faced was complexity and rigidity in structure. For Antora to generate a beautiful documentation site, you have to ensure the documentation is structured in a much more complex format. For example, the docs might be under <code>docs/modules/ROOT/pages/doc.adoc</code> and have a <code>nav.adoc</code> file as well. While this promises a solid solution, retrofitting or expecting adoption might be tricky if your team has never even done markdown.</p>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#azure-devops-pipeline","title":"Azure DevOps Pipeline","text":"<p>I ended using an Azure DevOps pipeline (YAML of course \ud83e\udd18) that provides a nice easy way to get this done.</p> <p>First, for proper linking, you should follow the directions Azure DevOps gives on the creation of a Github Service Connection which uses OAUTH. This will ensure your setup isn't brittle and using your access token.</p> <p>{{&lt; gist sheldonhull  \"053cb176d5c2847a4e323f01207acb82\" &gt;}}</p>","tags":["devops","docs-as-code","tech"]},{"location":"asciidoc-distributed-docs-as-code/#things-to-know","title":"Things to Know","text":"<ul> <li>Ensure you use the format shown here for documentation to render in confluence correctly. You need to have the names match in the doc/folder for it to know to render the child pages.</li> </ul> <pre><code>** Repo Oriented Documentation**\ntaco-ops-repo\nREADME.adoc  -- optional, but I always include for landing page, and point to the docs folder using link:./docs/myrepo.adoc\n---&gt; [docs]\n------&gt; [resources]  -- optional, but keeps the scripts organized and consistent, or any images\n------&gt; process.adoc\n------&gt; another-process.adoc\n---&gt; taco-ops-repo.adoc\n</code></pre> <ul> <li>Include your scripts by using <code>include::./resources/_myscript.ps1[]</code>. You may have to test that relative path issue if doing multiple repos.</li> <li>Ensure your non-asciidoc contents are prefaced with an underscore in the title name. I don't like this, but it's a requirement from confluence-publisher. This ensures it won't try to render as a page.</li> <li>Anything in the target directory (ancestor) gets purged in the process. I recommend a dedicated confluence space you create just for this to minimize risk and disable manual edits.</li> </ul> <p>{{&lt; admonition type=\"info\" title=\"Docker Commands in Microsoft-Hosted Agent\" &gt;}} I didn't expect docker commands to work in Azure DevOps agent, thinking nested virtualization would not work and all. However, it works beautifully. Consider using Azure DevOps yaml pipelines for running your docker commands and you take one step towards better build processes. {{&lt; /admonition &gt;}}</p>","tags":["devops","docs-as-code","tech"]},{"location":"quickly-create-github-release-via-cli/","title":"Quickly Create Github Release via Cli","text":"","tags":["git","devops","tech","automation","golang"]},{"location":"quickly-create-github-release-via-cli/#intro","title":"Intro","text":"<p>I've been trying to improve modularization with Terraform. Using Terraform Cloud, you get a private module repository. The modules are linked to tags in git.</p> <p>I've wanted additionally to specifically create releases for the versions in addition to the tags, to make tracking of usable versions a little cleaner.</p> <p>There are several ways to do this, including using the GitHub API, npm modules, and more. I wanted a simple CLI tool to do this and ran across this great Go utility that worked seamlessly for me.</p> <p>I've used the Hub cli but the create release syntax never really worked for me.</p>","tags":["git","devops","tech","automation","golang"]},{"location":"quickly-create-github-release-via-cli/#github-release","title":"github-release","text":"<p>Github-release is a simple golang cli that worked great for me. Note that this is a fork. This fork is more up to date than the original.</p> <p>With go installed just run this to get it installed and available in <code>PATH</code>.</p> <pre><code>go get github.com/itchio/gothub\n</code></pre> <p>To simplify GitHub access, ensure you set an environment variable for your user called GITHUB_TOKEN.</p> <p>With PowerShell you can do it quickly like this (you might need to close and reopen vscode/terminal for this to be recognized)</p> <pre><code>    [System.Environment]::SetEnvironmentVariable('GITHUB_TOKEN','tokenhere','User')\n</code></pre>","tags":["git","devops","tech","automation","golang"]},{"location":"quickly-create-github-release-via-cli/#usage","title":"Usage","text":"<p>To use this, you can chain together some steps and see how it can save you time on creating a GitHub release.</p> <p>{{&lt; gist sheldonhull  \"53055bbff368a4ebe4e0794076a56c37\" &gt;}}</p> <p>This helped me get moving faster \u26a1 on using Github releases without the tedious work to create. If this helped you or have any feedback, drop a comment below and let me know! The comments are powered by Utterances which will open a Github issue to discuss further. \ud83d\udc4d</p>","tags":["git","devops","tech","automation","golang"]},{"location":"improved-windows-terminal-setup/","title":"Improved Windows Terminal Setup","text":"<p>I've long been a Cmder/ConEmu user for Windows as it's provided a much-needed improvement of the standard Windows terminal.</p> <p>I've started trying to use the newer Windows Terminal project to benefit from the improved performance and support, and found getting it up and running with any customizations was a little time consuming and confusing. This wasn't something I'd hand off to someone who wasn't looking for experimentation.</p> <p>So here it goes! Rather than hunting all around to get a nice start on some default prompts, I've linked to some gists that can help you get up and running quickly with a few extra perks.</p>","tags":["cool-tools","development","git","powershell","tech"]},{"location":"improved-windows-terminal-setup/#getting-started","title":"Getting Started","text":"<p>This will help you get the terminal installed, along with downloading some settings I've already pre-setup with keybindings and more.</p> <p>To customize your own keybindings, you can go to the profiles.json documentation.</p> <p>{{&lt; gist sheldonhull  \"93d8060e6f86e0c46535ef6699d6e0c8\" \"install-settings.ps1\" &gt;}}</p>","tags":["cool-tools","development","git","powershell","tech"]},{"location":"improved-windows-terminal-setup/#improve-your-experience","title":"Improve Your Experience","text":"<p>After install, you can run the next command to help you get a better font setup with full support for ligatures and more.</p> <p>{{&lt; gist sheldonhull  \"93d8060e6f86e0c46535ef6699d6e0c8\" \"install-font.ps1\" &gt;}}</p> <p>And after this, if you don't have a nice PowerShell prompt experience, this will help give you a great start. This contains a few things, including starship. This is really useful as it has a library of prompt enhancements baked in. For example, if you are have an AWS profile active, it will display that for reference. It can display an active terraform workspace, git branch info, python virtual environment and more. Definitely a nice quick productivity booster with no real configuration needed to get going.</p> <p>{{&lt; gist sheldonhull  \"93d8060e6f86e0c46535ef6699d6e0c8\" \"configure-profile-prompt.ps1\" &gt;}}</p> <p>If you are wondering why I didn't leave the pretty awesome <code>\"useAcrylic\": true</code> on for my main pwsh session, it's because I found the background contrast reduction made it hard to read some darker colors on the prompt.</p> <p>Be sure to try out the retro pwsh theme for some nice eye candy.</p>","tags":["cool-tools","development","git","powershell","tech"]},{"location":"improved-windows-terminal-setup/#the-result","title":"The result","text":"<p>PowerShell Protip: Note the suggested completion based on prior commands in the pwsh prompt. This is some great prerelease work on a better PSReadline experience with Powershell.</p> <p></p> <p></p>","tags":["cool-tools","development","git","powershell","tech"]},{"location":"2020-04-30t0526320000/","title":"2020-04-30T05:26:32+00:00","text":"<p>Got some use out of the new beta plugins for netlify.</p> <p>I added a link checker task and it ran over 2000 internal tests to ensure all internal resources such as images, internal links, and even css assests all correctly were found. \ud83e\uddf9</p> <p>That's a great idea for static sites. Incremental maintenance always is better than trying to catch up after years of no upkeep.</p>","tags":["tech"]},{"location":"2020-05-05t1733000000/","title":"2020-05-05T17:33:00+00:00","text":"<p>If you want to simplify keeping up with github releases instead of using email, try the GitHub slack app. Simple create a releases channel and then run these two steps:</p> <ol> <li><code>/github subscribe terraform-providers/terraform-provider-terraform</code></li> <li><code>/github unsubscribe terraform-providers/terraform-provider-terraform issues, deployments, statuses, public, commits,pulls</code></li> </ol> <p>This should result in a final message saying:</p> <pre><code>This channel will get notifications from terraform-providers/terraform-provider-terraform for:\nreleases\n</code></pre> <p>Why not use <code>/github subscribe terraform-providers/terraform-provider-terraform releases</code>? I've found that at least on mobile this fails to filter properly. For me, it ignored many filters. By doing it in two steps it worked every-time without issues.</p> <p>This means you'll now get a nice formatted slack notification of just a release, without all the extra noise of pull requests, issues, commits and more. Easy way to keep up to date with projects if you want to keep it in slack. The other way is to use the github notifications section, but personally I like the slack approach.</p>","tags":["tech","slack"]},{"location":"2020-05-05t2012010000/","title":"2020-05-05T20:12:01+00:00","text":"<p>Finally got atomic algolia to work in CICD for my hugo blog. I've tried tackling this in many ways over time, but this finally just worked with plug and play ease thanks to just adding a line to the hugo build <code>netlify.toml</code> file.</p> <p>If you want to try this out, assuming you've already got an algolia index, json file generated and all... then just:</p> <ol> <li>Setup env variables in netlify build</li> <li>Add the following line to your netlify production build script</li> </ol> <pre><code>echo \"Starting atomic-algolia update\"\nnpm run algolia\n</code></pre> <p>You should get the following output from your netlify build if everything went right. No hits to algolia if you didn't change your indexes! \ud83c\udf89</p> <pre><code>3:13:47 PM: &gt; sheldonhull.hugo@1.0.0 algolia /opt/build/repo\n3:13:47 PM: &gt; atomic-algolia\n3:13:47 PM: [Algolia] Adding 0 hits to sheldonhull.com\n3:13:47 PM: [Algolia] Updating 0 hits to sheldonhull.com\n3:13:47 PM: [Algolia] Removing 0 hits from sheldonhull.com\n3:13:47 PM: [Algolia] 156 hits unchanged in sheldonhull.com\n</code></pre>","tags":["tech","blogging"]},{"location":"2020-05-07t0419310000/","title":"2020-05-07T04:19:31+00:00","text":"<p>GitHub Codespaces was announced today. I've been a believer in this for a while now, and have been on the early access testing of online.visualstudio.com. Apparently it's renamed to Visual Studio Codespaces now, and is the same tech. Will be a while before this trickles out to be mainstream, but as an early adopter of \"developer workspaces as code\" it's an exciting time.</p> <p>The fact that I can now spin up with a couple text files a full development test environment, with SQL Server in Docker, a configured development environment with linting and more, and then rebuild this in a few minutes on any machine is pretty incredible.</p>","tags":["til","tech"]},{"location":"2020-05-07t2254340000/","title":"2020-05-07T22:54:34+00:00","text":"<p>Kept getting an error <code>Incorrect syntax near the keyword 'read'</code> when running the some updated PowerShell 7 in lambda with the <code>dotnet3.1</code> sdk. Was troubleshooting loading types thinking I was having path issues.</p> <p>Turns out one of these things is not like the other. \ud83e\udd23</p> <pre><code>set nocount on\nset transaction isolation read uncommitted\n</code></pre> <pre><code>set nocount on\nset transaction isolation level read uncommitted\n</code></pre> <p>I think this explains why \"error during \"read\". Maybe I should have run in my Azure Data Studio session before trying serverless \ud83d\ude00</p>","tags":["tech","sql-server","aws","serverless"]},{"location":"2020-05-13t0336280000/","title":"2020-05-13T03:36:28+00:00","text":"<p>Jedi: Fallen Order has the most linear gameplay I've done in a while. View map, get object, run for 15 mins to get back to beginning of map. Rinse and repeat. Feels like I'm back to Skyrim and endless walking. I think Dying Light ruined me with it's parkour and movement dynamics that made everything feel so immersive.</p>","tags":["gaming"]},{"location":"2020-05-15t1609220000/","title":"2020-05-15T16:09:22+00:00","text":"<p>\u2714 Animated a fuzzy tickle creature.</p> <p>\u2714 Rescue a bird stuck in a vent</p> <p>\u2714 Begin DevOps yaking shaving session</p>","tags":["tech","work-from-home"]},{"location":"2020-05-17t1632480000/","title":"2020-05-17T16:32:48+00:00","text":"<p>Digging the guitar tone in this. Need to break out my Line6 Helix now. \ud83c\udfb8</p> <p>{{&lt; youtube PanHXyARGlM &gt;}}</p>","tags":["music"]},{"location":"2020-05-19t1727310000/","title":"2020-05-19T17:27:31+00:00","text":"<p>Migrating from Windows 10 to macOS for professional use this week. So far the most painful things have been:</p> <ul> <li>Pretty rough experience getting displaylink to work, definitely not as plug-and-play as Windows.</li> <li>Shortcuts! I'm a serious shortcut fanatic. The difference in control/command/alt usage on on a mac is painful to my muscle memory and coding flow. Trying to not remap to windows based keys, but it's been hard.</li> <li>Window management. I've used divvy and tried some other's, but seriously, the lack of window snapping and aero peak is pretty glaring as a new user.</li> </ul> <p>Other than that, so far have most things up and running, and forced myself to do it through Ansible to learn something new. I now know I could setup my macbook 90% of the way in minutes with Ansible, so that's a win!</p>","tags":["tech","ansible","macOS","apple"]},{"location":"2020-05-29t2216020000/","title":"2020-05-29T22:16:02+00:00","text":"<p>Iterm2 is just beautiful. What a fantastic tool with so many usability features.</p>","tags":["tech","macOS"]},{"location":"2020-06-02t1753510000/","title":"2020-06-02T17:53:51+00:00","text":"<p>Quickly install Git Town using bash in Ubuntu container with the following:</p> <p>{{&lt; gist sheldonhull  \"adeb27394527dbd9a27408c1ba79718d\" \"install-git-town.sh\" &gt;}}</p>","tags":["til","tech","shell","linux"]},{"location":"2020-06-02t1922510000/","title":"2020-06-02T19:22:51+00:00","text":"<p>The GitHub logo is an octopus with ears... not a cat.</p> <p>Mind blown. \ud83e\udd2f</p>","tags":["til","tech","github"]},{"location":"2020-06-04t2214240000/","title":"2020-06-04T22:14:24+00:00","text":"<p>What a cool website... Music For Programming</p>","tags":["tech"]},{"location":"2020-06-04t2215210000/","title":"2020-06-04T22:15:21+00:00","text":"<ul> <li>add dev container configuration for Go project in Visual Studio code</li> <li>Added docker compose file</li> <li>Add SQL Server to compose as another service.</li> <li><code>docker-compose up -d</code></li> <li>Attach to running container in VSCode</li> <li>Magic \u26a1</li> </ul> <p>Now I can open Visual Studio Code and run a go project against SQL Server, and tear it all down as I want. Gonna definitely blog on this, as it's super cool to see this work so well.</p> <p>While Docker has a steep learning curve, being able to have a full clean dev test environment with SQL Server in a local container and another Ubuntu container to run Go projects in absolutely amazing, not to mention working \"inside it\" with Remote Workspaces extension from Microsoft.</p>","tags":["til","tech","golang","docker","sql-server"]},{"location":"2020-06-05t2006360000/","title":"2020-06-05T20:06:36+00:00","text":"<p><code>brew install micro</code> resulted in my finding what I think will be my new go to cli editor. Very impressed so far. I've got too many things to learn to be fluent in vim, and this cross platform cli tool might be my new best friend for more command line-fu. Micro</p>","tags":["til","tech","cool-tools"]},{"location":"2020-06-10t2243370000/","title":"2020-06-10T22:43:37+00:00","text":"<p>Set an environment variable <code>export DOCKER_BUILDKIT=1</code> to enable a much cleaner Dockerfile output when you don't need verbose debug level information.  Reference: Buildkit</p>","tags":["til","tech","docker"]},{"location":"2020-06-17t2232320000/","title":"2020-06-17T22:32:32+00:00","text":"<p>Here's a cool way to visualize runspaces and how they work asynchronously.</p> <pre><code>$IndentLevel = 2\nGet-Random; @(1..100) | ForEach-Object -Parallel {\n    $i = $_\n    Start-Sleep -Milliseconds (Get-Random -Minimum 100 -Maximum 1500)\n    \"{0,$($i * $using:IndentLevel)}\" -f $i | Write-Host\n}\n</code></pre> <p>The random delay can help show how some tasks finish out of order. For example, running that might result in this in the console:</p> <pre><code>       4\n 1\n     3\n   2\n             7\n         5\n                 9\n           6\n                  10\n               8\n</code></pre>","tags":["tech","powershell","snippets"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/","title":"My Experience Switching To A Macbook Pro From Windows","text":"<p>{{&lt; admonition type=\"Info\" title=\"update 2021-05-19\" open=\"true\"&gt;}}</p> <p>updated with fresh thoughts</p> <p>Regarding stability issues, I'd say those initial stability issues haven't continued, so I'm very happy with the overall stability. The only thing that really gave me pain was DisplayLink drivers, which seem to always be a pain on Windows or MacOS.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#my-background","title":"My background","text":"<p>My background has been very strongly focused on the .NET world, with a focused start on SQL Server, later branching into learning PowerShell. This world was heavily focused on Windows.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#why-switch","title":"Why switch","text":"<p>Having focused on Windows for so long, using a Macbook for development always intrigued me, but was actually not the driving factor. Poor specs and performance issues with the currently issued laptop kept impacting my work, causing impact to my efforts. As my work right now is heavily focused in DevOps oriented areas, I found that the reliance on tooling that only worked in Windows over time has greatly reduced.</p> <p>While I'm very excited for WSL2 with Windows, what I kept feeling like I was doing was battling my system instead of accomplishing the work I needed to get done. As someone who has been an obsessive tweaker of whatever OS I'm on, I've actually found my desire for full customization diminishing as the development work becomes more interesting. In this case, being able to work with more linux tooling without as much conflict was appealing.</p> <p>Additionally, the windows specs for the workbooks I had available were definitely subpar to what a highend macbook could offer.</p> <p>So I took the plunge, was approved thanks to a supportive manager, and now am working exclusively on a 16\" Macbook Pro I9.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#setup","title":"Setup","text":"<p>Learned some Ansible on the fly and configured everything pretty much from the start with Ansible. Overall, that made it a much better developer onboarding experience than trying to write my initial chocolatey package bootstrap script.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#painpoints","title":"Painpoints","text":"<p>:(fas fa-external-link-alt): DisplayLink installation was painful for docking usage. Gotta be careful during updates.</p> <p>:(fas fa-bullseye): No aeropeek</p> <p>:(fas fa-window-restore): No automatic window snapping without third party apps</p> <p>:(fas fa-keyboard): Text editing shortcuts</p> <p>:(fas fa-home): Absolutely despise the changed home/end and selection behavior. Trying to remap this is painful. I've limited my remapping to try and adjust to the way it works natively, but it's difficult. VSCode remapping for certain actions is also tricky, but possible.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#wins","title":"Wins","text":"<p>:(fas fa-search): So far I've found Finder to be really good and much more intuitive than Explorer in Windows.</p> <p>:(fas fa-folder): The consistency of app placement makes finding things to run quick, whereas in Windows, finding the app can be a beast at times. Think about this from a new Windows developer perspective. Go to Program Files (x86), or Program Files, but some go into AppData, and then settings in ProgramData, but sometimes that program prefers to put some of the configuration data in the Program directory and sometimes in ProgramData.... unless they want to put it in AppData.... and then Local/Roaming etc. It gets really really confusing quick. That's why tools like Everything are so helpful!</p> <p>:(fab fa-docker): Docker startup is \ud83d\ude80 FAST. I'm talking about an update for Docker, and install restart and in a few seconds it's back up and running. I've been so used to it being minutes on Windows. Mounted drive performance isn't great, but neither is it on Windows.</p> <p>:(fas fa-beer): I love Chocolatey &amp; Scoop on Windows, but I'm seeing some advantages to a central repository containing all the packages, instead of each package being the responsibility of maintainers to keep up to date. This makes the contribution phase much more difficult in Chocolatey. I've written very complex Choco packages for my company, but haven't yet setup a community one, whereas Brew, Scoop, and others have a central repository that you can easily submit a push request to with improvements or new additions without having to manage the autorefresh of the packages. A minor, but important distinction to me, as the ease of contributing must be there for more people to engage in it. {{&lt; typeit  &gt;}}Brew is stellar.{{&lt; /typeit &gt;}}</p> <p>:(fab fa-windows): Not having Windows as my OS has helped me go cold turkey as much as possible on running more workloads in Docker. Visual Studio Docker workspaces are absolutely incredible, and paired with a good tool like <code>InvokeBuild</code> with PowerShell, you have a stellar setup than can easily bootstrap itself on a new machine with ease.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#quirks","title":"Quirks","text":"<p>:(fas fa-mouse): As of this time, mouse cursor not auto-hiding in full screen video without work arounds on certain sites.</p> <p>:(fas fa-long-arrow-alt-right): Experimenting with some apps resulted in 2 full system crashes in first 3 weeks of using, so stability while good wasn't as stellar as I was hoping. Just like Windows, it all depends on what you are running.</p> <p>:(fas fa-long-arrow-alt-right): Massive lag on bluetooth and even Logitech Unifying receiver based mouse and keyboard, enough to make them unusable. Others seem to have had similar issues when I searched.</p> <p>:(fas fa-long-arrow-alt-right): Need to buy a powered hub to expand ports, as only USB-C resulting in all my peripherals not working.</p> <p>:(fas fa-long-arrow-alt-right): Docks don't provide enough power for a macbook pro at times. Gone are the days of a slick dock that my laptop locks into. Get used to running cables.</p> <p>:(fas fa-long-arrow-alt-right): Had to google \"how to remove bonks\" to get the annoying keyboard sound effect from driving my insane. This required editing: <code>/DefaultKeyBinding.dict</code>. Seriously, this was just silly.</p> <p>:(fas fa-long-arrow-alt-right): I find the power button the mac annoying. Tends to have a mind of it's own when I'm thinking \"just start darn you!\"</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#development-experience","title":"Development Experience","text":"<p>I feel kinda strange as I've not needed to dive into system internals like I felt I needed with Windows.</p> <p>At somepoint, I needed to touch the registry, work through service quirks, .NET library conflicts and more.</p> <p>Overall, it feels like things are just easier and I fight less with development tools and libraries.</p> <p>Perhaps the most hands on portion is just making sure binaries get to <code>PATH</code> as there is no real \"global variable\" like you have in Windows. Instead, this is normally managed in the user's <code>.bashrc</code> file, or in my case I use <code>.profile</code> and a powershell file to load my own overrides.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#powershell-shells","title":"PowerShell &amp; Shells","text":"<p>I put work into using <code>zsh</code> as my default, with a bunch of customizations. I still came back to PowerShell \u2764\ufe0f.</p> <p>PowerShell is a fantastic default shell for macOS, and allowed me to immediately get value from the terminal without having to use a specific syntax to that shell (if it isn't POSIX compliant like Fish)</p> <p>With PSReadline, the editor experience and autocomplete on my history is fantastic and I have no complaints.</p> <p>I'm trying to keep an open mind and not \"hate on bash/fish/zsh\", as they have a long history. I can see if someone has a background in Linux only, that PowerShell is too much of a paradigm change to adopt without a desire to explore it.</p> <p>For those with experience in more object oriented tooling, it will be more natural in my opinion than the quirks of bash scripts.<sup>1</sup></p> <p>With <code>ConvertFrom-Json</code> being such as magical tool, I've been able to mix and match native tooling with PowerShell automagic cmdlets and get some great productive results.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#shell-customization","title":"Shell Customization","text":"<p>With my dotfiles managed by chezmoi, my terminal in any environment looks great as I'm using starship</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#apps","title":"Apps","text":"<p>The ecosystem for nice dev tools feels better. I bought Alfred Microblog Post and Dash and find them really useful.</p>","tags":["devops","tech","development","apple","macOS"]},{"location":"my-experience-switching-to-a-macbook-pro-from-windows/#verdict","title":"Verdict","text":"<p>So far, the experience has been one I'd repeat. For me, I've actually accomplished more, and gotten aware from my desk more with the improved quality of the mobile experience and touchpad.</p> <p>Would I do it again? Currently yes.</p> <ol> <li> <p>After digging through the mess of bash if comparison blocks, I found the PowerShell ternary operator very readable. <code>(Test-Path $File -PathType Leaf) ? (Write-Host \"\ud83d\ude00 Do something with positive match and process\") : (Write-Host \"\ud83d\ude22 Sad panda\")</code> Since all the bash tools are pretty much accessible from your <code>pwsh</code> prompt and scripts simply by being called, you gain all the perks of a clear readable scripting language, while still using native tooling. It's not better than bash, just different. For those experienced in bash and not really desiring to try something new, no problem! For those looking to try something new, I think it's a great way to write a cross platform script, and a good default shell.\u00a0\u21a9</p> </li> </ol>","tags":["devops","tech","development","apple","macOS"]},{"location":"azure-data-studio-sql-notebook-for-diagnostic-queries/","title":"Azure Data Studio SQL Notebook for Diagnostic Queries","text":"","tags":["dbatools","powershell","cool-tools","azure-data-studio","sql-server"]},{"location":"azure-data-studio-sql-notebook-for-diagnostic-queries/#diagnostic-queries","title":"Diagnostic Queries","text":"<p>Glenn Berry has long been known for producing the definitive diagnostic query set for various SQL Server versions. Between his amazing work and my favorite Brent Ozar First Responder Kit, you are pretty much set.</p> <p>One of the things that can be painful though about running diagnostic queries is that it's a lot of small individual queries that you want to run and ideally save the results for review.</p> <p>You can do this with dbatools and running queries individually, which is actually what I did a while back for a special support tool that dynamically split those queries into files and exported to xml for later import and review.</p>","tags":["dbatools","powershell","cool-tools","azure-data-studio","sql-server"]},{"location":"azure-data-studio-sql-notebook-for-diagnostic-queries/#azure-data-studio","title":"Azure Data Studio","text":"<p>I'm a big fan of Azure Data Studio and as I'm not primarily focused right now on SQL Server administration, the feature-set perfectly fits my needs for running queries, doing some basic server administration, and overall just having a lighter weight solution to SSMS. Since I migrated to macOS, this provides me a nice cross-platform tool that I can use on Windows or macOS.</p> <p>A great feature that has been continually improving is the Azure Data Studio notebooks. Not only can you run T-SQL notebooks now, but also PowerShell and python using whatever kernel you desire.</p> <p>As part of this, you get the benefits of a nice intuitive structure to ad-hoc queries you might want to provide to someone with details on what it means and more. Additionally, the results are cached as part of the JSON so if you save the file and come back later you can review all the results that were pulled (and as a plus they render in GitHub viewer too).</p>","tags":["dbatools","powershell","cool-tools","azure-data-studio","sql-server"]},{"location":"azure-data-studio-sql-notebook-for-diagnostic-queries/#diagnostic-queries-azure-data-studio-dbatools","title":"Diagnostic Queries + Azure Data Studio + dbatools = \ud83c\udf89","text":"<p>To merge the power of all 3 technologies, you can use dbatools to export the diagnostic queries for a targeted SQL server version as an Azure Data Studio Notebook. Pretty freaking cool.</p> <p>To get started on this just make sure you have the latest dbatools: <code>Install-Module dbatools -confirm:$false</code></p> <p>Then generate a new Azure Data Studio Notebook like this:</p> <pre><code># This will create the notebook in whatever location you currently are in\n$Version = 2017\nNew-DbaDiagnosticAdsNotebook -Path \"DiagnosticNotebook${Version}.ipynb\" -TargetVersion $Version\n</code></pre> <p>Open up this new notebook and enjoy the result! To make reading easier, you can issue the command to \"collapse all cells\" and the queries will be minimized allowing you to read through all the query options.</p> <p>Note that even the description of the queries is provided in the notebook, providing insight on the purpose of the query.</p> <p>{{&lt; admonition type=\"warning\" title=\"Warning\" &gt;}}</p> <p>As always, make sure you are careful before just running all queries by default against a production server. Some queries take heavy resources and might not be appropriate to run in the middle of a production workflow.</p> <p>{{&lt; /admonition &gt;}}</p> <p></p>","tags":["dbatools","powershell","cool-tools","azure-data-studio","sql-server"]},{"location":"2020-06-24t1603490000/","title":"2020-06-24T16:03:49+00:00","text":"<p>After using Windows versions of launchers that tried to emulate Alfred, I finally took the plunge and bought Alfred's Powerpack. The buyer remorse of paying &gt; $60 on a \"shortcut\" app hasn't really set in as I'm seeing such a wealth of great features that I think the gains as a developer and shortcut aficionado will be well worth the cost in my workflow. So far, highly recommend. The best part for me is the easy plugging in of bash/pwsh scripts to execute on demand for quick productivity tasks without having to navigate and open it up in the terminal.</p> <p></p>","tags":["tech","macOS","alfred"]},{"location":"2020-06-29t1829030000/","title":"2020-06-29T18:29:03+00:00","text":"<p>Visual Studio Code has a pretty great way to browse through themes by just selecting installed themes and using arrow keys to preview without apply. However, browsing those themes isn't quite so good, as you need to install to see the changes.</p> <p>Ran across Vscode Themes which provides a really nice experience if you feel like changing things up on your editor.</p>","tags":["til","tech","visual-studio-code"]},{"location":"2020-07-06t12-00/","title":"2020-07-06T12:00:00-05:00","text":"<p>Windows users, nice little win for making the great git-town tool even more accessible. \ud83c\udf89</p> <ul> <li>install scoop: <code>iwr -useb get.scoop.sh | iex</code></li> <li><code>scoop install git-town</code></li> </ul> <p>This is one of my favorite tools for git workflow. If you use GitHub flow to keep a simple workflow, it's a life saver.</p> <p>For example, on a branch and need to start a new bit of work to keep your commits atomic? <code>switch to master &gt; stash pending work &gt; pull latest with rebase &gt; create new branch &gt; push branch</code> to remote OR <code>git town hack feat/tacos</code>. Need to squash commits and ship to master? <code>git town ship</code> What about prune all those remote branches that have been merged? <code>git town prune-branches</code> This is one of my favorite git productivity tools (and it's written in Go \ud83d\udc4d  so cross platform and fast)</p>","tags":["tech","development","git","golang"]},{"location":"getting-started-with-stream-analytics/","title":"Getting Started with Stream Analytics","text":"","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#resources","title":"Resources","text":"Resources If you want a schema reference for the json Application Insights produces // Azure Application Insights Data Model // Microsoft Docs If you want to visualize last 90 days of App Insight Data with Grafana // Monitor Azure services and applications using Grafana // Microsoft Docs","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#the-scenario","title":"The Scenario","text":"<p>Application insights is integrated into your application and is sending the results to Azure. In my case, it was blob storage. This can compromise your entire insights history.</p> <p>Application Insights has some nice options to visualize data, Grafana included among them. However, the data retention as of this time is still set to 90 days. This means historical reporting is limited, and you'll need to utilize <code>Continuous Export</code> in the Application Insights settings to stream out the content into blob storage to</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#the-process","title":"The process","text":"<ol> <li>Install Visual Studio Azure Plugin</li> <li>Initialize a new Stream Analytics project in Visual Studio</li> <li>Import some test data</li> <li>(Optional) If using SQL Server as storage for stream analytics then design the schema.</li> <li>Write your stream analytics sql, aka asql.</li> <li>Debug and confirm you are happy with this.</li> <li>Submit job to Azure (stream from now, or stream and backfill)</li> <li>Configure Grafana or PowerBI to connect to your data and make management happy with pretty graphs.</li> </ol>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#install-visual-studio-azure-plugin","title":"Install Visual Studio Azure Plugin","text":"<p>I don't think this would have been a feasible learning process without having run this through Visual Studio, as the web portal doesn't provide such a smooth experience. Highly recommend using Visual Studio for this part.</p> <p>Learning the ropes through the web interface can be helpful, but if you are exploring the data parsing you need a way to debug and test the results without waiting minutes to simply have a job start. In addition, you need a way to see the parsed results from test data to ensure you are happy with the results.</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#new-stream-analytics-project","title":"New Stream Analytics Project","text":"","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#setup-test-data","title":"Setup test data","text":"<p>Grab some blob exports from your Azure storage and sample a few of the earliest and the latest of your json, placing into a single json file. Put this in your solution folder called inputs through Windows Explorer. After you've done this, right click on the input file contained in your project and select <code>Add Local Input</code>. This local input is what you'll use to debug and test without having to wait for the cloud job. You'll be able to preview the content in Visual Studio just like when you run SQL Queries and review the results in the grid.</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#design-sql-schema","title":"Design SQL Schema","text":"<p>Unique constraints create an index. If you use a unique constraint, you need to be aware of the following info to avoid errors.</p> <p>When you configure Azure SQL database as output to a Stream Analytics job, it bulk inserts records into the destination table. In general, Azure stream analytics guarantees at least once delivery to the output sink, one can still achieve exactly-once delivery to SQL output when SQL table has a unique constraint defined. Once unique key constraints are set up on the SQL table, and there are duplicate records being inserted into SQL table, Azure Stream Analytics removes the duplicate record. Common issues in Stream Analytics and steps to troubleshoot  Using the warning above, create any unique constraints with the following syntax to avoid issues.</p> <pre><code>create table dbo.Example (\n...\n,constraint uq_TableName_internal_id_dimension_name\n          unique ( internal_id, dimension_name ) with (IGNORE_DUP_KEY  = on)\n</code></pre>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#stream-analytics-query","title":"Stream Analytics Query","text":"<p>warning \"Design Considerations\" Pay attention to the limits and also to the fact you aren't writing pure T-SQL in the <code>asaql</code> file. It's a much more limited analytics syntax that requires you to simplify some things you might do in TSQL. It does not support all TSQL features. Stream Analytics Query Language Reference</p> <p>Take a look at the query examples on how to use <code>cross apply</code> and <code>into</code> to quickly create Sql Server tables.</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#backfilling-historical-data","title":"Backfilling Historical Data","text":"<p>When you start the job, the default start job date can be changed. Use custom date and then provide it the oldest data of your data. For me this correctly initialized the historical import, resulting in a long running job that populated all the historical data from 2017 and on.</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#configure-grafana-or-powerbi","title":"Configure Grafana or PowerBI","text":"<p>Initially I started with Power BI. However, I found out that Grafana 5.1 &gt; has data source plugins for Azure and Application insights, along with dashboard to get you started. I've written on Grafana and InfluxDB in the past and am huge fan of Grafana. I'd highly suggest you explore that, as it's free, while publishing to a workspace with PowerBI can require a subscription, that might not be included in your current MSDN or Office 365 membership. YMMV.</p>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#filter-syntax","title":"Filter Syntax","text":"<p>Filter Syntax Reference</p> <p>I had to search to find details on the filtering but ended up finding the right syntax for doing partial match searches in the Filter Syntax Reference linked above. This also provides direct links to their ApiExplorer which allows testing and constructing api queries to confirm your syntax.</p> <p>If you had a custom metric you were grouping by that was <code>customEvent\\name</code> then the filter to match something like a save action could be:</p> <pre><code>startswith(customEvent/name, 'Save')\n</code></pre> <p>This would match the custom metrics you had saved that might provide more granularity that you'd normally have to specify like:</p> <pre><code>customEvent/Name eq 'Save(Customer)'\ncustomEvent/Name eq 'Save(Me)'\ncustomEvent/Name eq 'Save(Time)'\ncustomEvent/Name eq 'Save(Tacos)'\n</code></pre>","tags":["tech","development","azure"]},{"location":"getting-started-with-stream-analytics/#wrap-up","title":"Wrap-up","text":"<p>I only did this one project so unfortunately I don't have exhaustive notes this. However, some of the filter syntax and links were helpful to get me jump started on this and hopefully they'll be useful to anyone trying to get up and running like I had too.</p>","tags":["tech","development","azure"]},{"location":"2020-07-13T14-00-00/","title":"2020-07-13T14:00:00-05:00","text":"<p>PowerShell has some functionality that can help in pre-processing data by grouping and aggregating. If you are using <code>ImportExcel</code> this might be useful to pre-aggregate the results prior to an Excel workbook. If you are working with PowerShell and needing to do some quick measurement of objects, maybe this will be useful.</p> <p>{{&lt; gist sheldonhull  \"af7e3355953a2f6533c813d9ca220a7d\" &gt;}}</p>","tags":["tech","development","powershell"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/","title":"Compiling a Custom Provider and Including for Terraform Cloud","text":"","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#assumptions","title":"Assumptions","text":"<ul> <li>You are familiar with the basics of setting up <code>Go</code> and can run basic Go commands like <code>go build</code> and <code>go install</code> and don't need much guidance on that specific part.</li> <li>You have a good familiarity with Terraform and the concept of providers.</li> <li>You need to include a custom provider which isn't included in the current registry (or perhaps you've geeked out and modified one yourself \ud83d\ude01).</li> <li>You want to run things in Terraform Enterprise \u2601.</li> </ul>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#terraform-cloud","title":"Terraform Cloud","text":"<p>For Terraform Cloud, bundling is not allowed.</p> <p>Instead, the \"legacy\" way of running this is to include the plugin directly in the directory that Terraform will be invoked on with <code>terraform.d/plugins/linux_amd64</code> as the path containing the provider. See discussion: Using Community Providers With Terraform Cloud.</p> <p>Part of my current walk-through (primarily using terraform-bundle) is relevant only for Terraform Enterprise, not Terraform Cloud. I missed the ending documentation section on the custom bundle requiring installation and not being supported in Terraform Cloud.</p> <p>For the directions below, disregard the bundle aspect for Terraform Cloud, and instead focus on building the custom provider and including in the project directory as shown.</p> <p>If you are willing to explore Atlantis, I bet something can be done with custom providers in there.</p> <p>After following the custom provider build steps below, create a <code>.terraformignore</code> file in your project directory and put in the config below.</p> <pre><code>.terraform\n.git\n.gtm\n*.tfstate\n</code></pre> <p>With a backend like below, I was actually able to get terraform cloud to run the custom provider and return the plan.</p> <pre><code>terraform {\n  backend \"remote\" {\n    hostname     = \"app.terraform.io\"\n    organization = \"myorg\"\n\n    workspaces {\n      name = \"terraform-artifactory\"\n    }\n  }\n}\n</code></pre> <p>If you get an error the first time you run this, see the troubleshooting section at the end.</p>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#custom-providers-bundling","title":"Custom Providers Bundling","text":"<p>As of the time of this post, to include a custom provider with Terraform Enterprise, you need to create a custom terraform bundle bundle to package up the terraform package and any desired custom plugins.</p> <p>This terraform bundle includes the terraform program, as well as any range of other providers that you want to include for running in the remote terraform workspace.</p> <p>Before you go down this route, you should make sure that the Terraform Registry doesn't already include the provider you need.</p>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#source-of-truth","title":"Source Of Truth","text":"<p>For the most up to date directions, you can go through these directions:</p> <ul> <li>Setting Up Development Environment</li> <li>Using Terraform Bundle</li> <li>Installing Custom Software</li> </ul>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#compiling-the-custom-provider","title":"Compiling the custom provider","text":"<p>In this example, I'm working with a provider for Jfrog Artifactory, which IMO has an atrocious management experience on the web. By compiling this custom provider, my goal was to provide a clean user, repository, and group management experience.</p> <p>You need to target the platform for Go in the build step, as the Terraform Enterprise environment expects <code>Linux</code> and <code>amd64</code> as the target.</p> <pre><code>git clone https://github.com/atlassian/terraform-provider-artifactory.git\ngit install .\n\n# I use pwsh even on macOS \ud83d\ude01\n#$ENV:GOOS='linux'\n#$ENV:GOARCH='amd64'\n#go build\n\n#See troubleshooting section below. More robust than simple go build. This simplifies things and will generate all binaries for you\n\ngoreleaser build --snapshot\n</code></pre>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#get-stuff-setup","title":"Get Stuff Setup","text":"<pre><code>git clone https://github.com/hashicorp/terraform.git\n</code></pre> <p>To checkout a specific tagged version (recommended):</p> <pre><code>git checkout tags/&lt;tag_name&gt;\n</code></pre> <p>Quick little #devhack... Use Git Graph in Visual Studio Code to make working with busy repositories much easier. Yes, I'm no Vim magician. Sometimes a little visual help is much better than trying to do it all in cli. #heresy</p> <p></p> <p>Next, you'll want to install and validate your install worked. <code>go install</code> ensures that dependencies are downloaded, so once again the magic of <code>Go</code> wins the day. If you flip to a new tagged version, make sure to rerun the install so you have the correct version of the tooling available.</p> <pre><code>go install .\ngo install ./tools/terraform-bundle\n</code></pre> <p>For some reason, I had issues with the path picking it up in my current session, so for expediency, I just ran the next steps with the fully qualified path: <code>/Users/sheldonhull/go/bin/terraform-bundle</code> instead of <code>terraform-bundle</code> directly.</p> <p>Grab an example of the configuration <code>hcl</code> file for the terraform-bundler from the link mentioned above. Then you can create this in the project directory or qualify it to a subdirectory if you want to save various bundle configuration files.</p> <pre><code>mkdir mybundles\nNew-Item -Path ./mybundles/terraform-bundle.hcl -ItemType File\n</code></pre> <p>Here is a trimmed down example config with what worked for me. See the bottom troubleshooting section for more details on why I did not adhere to the documented config from the README.</p> <pre><code>terraform {\n  version = \"0.12.28\"\n}\nproviders {\n artifactory = [\"0.0.0\"]\n}\n</code></pre> <p>We need to include this plugin in a specific location for the bundle tool to do it's magic.</p> <p>Also ensure you follow the naming convention for a provider.</p> <p>To be recognized as a valid plugin, the file must have a name of the form <code>terraform-provider-&lt;NAME&gt;</code></p> <p>This is where PowerShell shines, and it's easy to make this path without issue using <code>Join-Path</code> in a way that also is fully cross-platform with macOS, Linux, or Windows (pick your poison)</p> <pre><code>try\n{\n    $version = terraform-bundle --version *&gt;&amp;1\n    if ($version -notmatch '\\d{1}[.]\\d{2}[.]\\d{1,2}') { throw \"failed to run terraform bundle: $($_.Exception.Message)\" }\n}\ncatch\n{\n    Write-Host \"Adding go bin/path to path so terraform-bundle can be resolved\"\n    $ENV:PATH += \"${ENV:HOME}/go/bin/:$PATH\"\n}\n\n\n$SOURCEHOST     ='github.com'  # any arbitrary value allowed per docs\n$SOURCENAMESPACE='atlassian'    # any arbitrary value allowed per docs\n$NAME           ='artifactory'\n$OS             ='linux'\n$ARCH           ='amd64'\n$VERSION        = '0.0.0'\n$PluginPath     = Join-Path plugins $SOURCEHOST $SOURCENAMESPACE $NAME $VERSION \"${OS}_${ARCH}\"\n$null           = New-Item -Path $PluginPath -ItemType Directory -Force\nRemove-Item -LiteralPath ./plugins -Recurse -Confirm:$false\nNew-Item plugins -ItemType directory -Force -ErrorAction silentlycontinue\nCopy-Item ${ENV:HOME}/git/github/terraform-provider-artifactory/dist/terraform-provider-artifactory_linux_amd64/terraform-provider-artifactory -Destination (Join-Path plugins \"terraform-provider-artifactory\") -Force\nterraform-bundle package -os=linux -arch=amd64 --plugin-dir ./plugins ./jfrog-bundle.hcl\n</code></pre> <p>Now to bundle this up</p> <pre><code>terraform-bundle package -os=linux -arch=amd64 jfrog-bundle.hcl\n</code></pre>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#troubleshooting","title":"Troubleshooting","text":"","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#problems-parsing-the-bundle-configuration-file","title":"Problems Parsing the bundle configuration file","text":"<p>I ran into some issues with it parsing the configuration file as soon as I added the custom plugin. It reported <code>unknown type for string *ast.ObjectType</code>.</p> <p>Here's what I looked at:</p> <p>In the project, there is a <code>tools/terraform-bundle/config.go</code> that is responsible for parsing the hcl file.</p> <p>First, the configuration looks correct in taking a string slice for the versions, and the source is a normal string.</p> <pre><code>type TerraformConfig struct {\n    Version discovery.VersionStr `hcl:\"version\"`\n}\n\ntype ProviderConfig struct {\n    Versions []string `hcl:\"versions\"`\n    Source   string   `hcl:\"source\"`\n}\n</code></pre> <p>This seems to mean the configuration syntax of meets with the schema required by the configuration code.</p> <pre><code>terraform {\n    version = \"0.12.28\"\n}\nproviders {\n    artifactory = {\n        versions = [\"0.1\"]\n        source = \"example.org/myorg/artifactory\"\n    }\n}\n</code></pre> <p>It looks like the configuration syntax from the example is a bit different from what is being successfully parsed. Instead of using the fully designated schema, I adjusted it to <code>artifactory = [\"0.0.0\"]</code> and it succeeded in parsing the configuration.</p> <p>The help <code>terraform-bundle package --help</code> also provides an example indicating to just use the simple syntax and let it look for the provider in the default directory of <code>./plugins</code>.</p>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#failed-to-resolve-artifactory-provider-01-no-provider-exists-with-the-given-name","title":"Failed to resolve artifactory provider 0.1: no provider exists with the given name","text":"<p>This next piece was a bit trickier to figure out. Once I enabled <code>$ENV:TF_LOG = 'TRACE'</code> I found some output showing an issue with the version of the provider.</p> <pre><code>2020/07/14 16:12:51 [WARN] found legacy provider \"terraform-provider-artifactory\"\nplugin: artifactory (0.0.0)\n- Resolving \"artifactory\" provider (0.1)...\n- Checking for provider plugin on https://releases.hashicorp.com...\n</code></pre> <p>I went back to the provider project and installed goreleaser using: <code>brew install goreleaser/tap/goreleaser</code> which provided me the same tool to build the various packages for this provider. Build the provider by running <code>goreleaser build --snapshot</code>.</p> <p>After reviewing the help in more detail, the following CLI content conflicts with the main README.md, so I had to experiment with various output methods and finally... success! \ud83c\udf89</p> <p>The message did provide a warning: <code>found legacy provider \"terraform-provider-artifactory-v2.0.0\"</code>.</p> <p>I tested and found it matched the local provider with <code>0.0.0</code> by running <code>terraform providers</code> and seeing the output:</p> <pre><code>2020/07/14 16:49:52 [TRACE] Meta.Backend: backend *remote.Remote supports operations\n.\n\u2514\u2500\u2500 provider.artifactory\n</code></pre> <p>However, what to bundle correctly required simplifying the output to no nested directories.</p> <p></p> <p>The output of the bundle was successful with</p> <pre><code>Fetching Terraform 0.12.28 core package...\n2020/07/14 16:54:34 [TRACE] HTTP client HEAD request to https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip\n2020/07/14 16:54:35 [TRACE] HTTP client GET request to https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip\nFetching 3rd party plugins in directory: ./plugins\n2020/07/14 16:54:37 [DEBUG] checking for provider in \"./plugins\"\n2020/07/14 16:54:37 [WARN] found legacy provider \"terraform-provider-artifactory\"\nplugin: artifactory (0.0.0)\n- Resolving \"artifactory\" provider (0.0.0)...\nCreating terraform_0.12.28-bundle2020071421_linux_amd64.zip ...\nAll done!\n</code></pre>","tags":["tech","development","golang","terraform"]},{"location":"compiling-a-custom-provider-and-including-for-terraform-cloud/#terraform-cloud-fails-with-terraformtfstate-detected","title":"Terraform Cloud Fails with terraform.tfstate detected","text":"<p>Since the local plugins seem to generate some tfstate for mapping the local plugin directory, I ensure you have a <code>.terraformignore</code> file in the root of your directory per the notes I provided at the beginning.</p> <pre><code>Terraform Enterprise detected a terraform.tfstate file in your working\ndirectory: &lt;VCS-REPO&gt;/terraform.tfstate\n</code></pre> <p>Once I added the <code>.terraformignore</code> the apparent conflict with uploading a local tfstate on the plugins was resolved and the plan succeeded.</p>","tags":["tech","development","golang","terraform"]},{"location":"how-to-iterate-through-a-list-of-objects-with-terraforms-for-each-function/","title":"How to Iterate Through A List of Objects with Terraform's for_each function","text":"","tags":["tech","development","terraform"]},{"location":"how-to-iterate-through-a-list-of-objects-with-terraforms-for-each-function/#what-i-want-to-do","title":"What I want to do","text":"<pre><code># create file local.users.yml\nuser:\n  - name: foobar1\n    email: foobar1@foobar.com\n  - name: foobar2\n    email: foobar2@foobar.com\n  - name: foobar3\n    email: foobar3@foobar.com\n</code></pre> <pre><code>locals {\n  users_file         = \"local.users.yml\"\n  users_file_content = fileexists(local.users_file) ? file(local.users_file) : \"NoSettingsFileFound: true\"\n  users_config       = yamldecode(local.users_file_content)\n}\n</code></pre> <p>What I want to work:</p> <pre><code>resource \"something\" {\nfor_each local.users_config\n\nname = each.key # or even each.value.name\nemail = each.value.email\n}\n</code></pre>","tags":["tech","development","terraform"]},{"location":"how-to-iterate-through-a-list-of-objects-with-terraforms-for-each-function/#what-ive-had-to-do","title":"What I've had to do","text":"<p>Now to iterate through this collection, I've had challenges, as the only way I've gotten this to work would be to ensure there was a designated key in the <code>yaml</code> structure. This provides a map object with a key/value format, instead of a collection of normal objects.</p> <p>This would result in a yaml format like:</p> <pre><code>user:\n  - 'foobar1':\n      name: foobar1\n      email: foobar1@foobar.com\n  - 'foobar2':\n       name: foobar2\n       email: foobar2@foobar.com\n  - 'foobar3':\n       name: foobar3\n       email: foobar3@foobar.com\n</code></pre> <p>This provides the \"key\" for each entry, allowing Terraform's engine to correctly identify the unique entry. This is important, as without a unique key to determine the resource a plan couldn't run in a deterministic manner by comparing correctly the previously created resource against the prospective plan.</p>","tags":["tech","development","terraform"]},{"location":"how-to-iterate-through-a-list-of-objects-with-terraforms-for-each-function/#another-way-using-expressions","title":"Another Way Using Expressions","text":"<p>Iterating through a map has been the main way I've handled this, I finally ironed out how to use expressions with Terraform to allow an object list to be the source of a <code>for_each</code> operation. This makes feeding Terraform plans from <code>yaml</code> or other input much easier to work with.</p> <p>Most of the examples I've seen confused the issue by focusing on very complex flattening or other steps. From this stack overflow answer, I experimented and finally got my expression to work with only a single line.</p> <pre><code>resource \"foobar\" \"this\" {\n    for_each = {for user in local.users_config.users: user.name =&gt; user}\n    name     = each.key\n    email    = each.value.email\n}\n</code></pre> <p>This results in a simple yaml object list being correctly turned into something Terraform can work with, as it defines the unique key in the expression.</p>","tags":["tech","development","terraform"]},{"location":"assume-a-role-with-aws-powershell-tools/","title":"Assume a role with AWS PowerShell Tools","text":"","tags":["tech","development","aws","powershell"]},{"location":"assume-a-role-with-aws-powershell-tools/#assume-a-role","title":"Assume A Role","text":"<p>I've had some issues in the past working with <code>AWS.Tools</code> PowerShell SDK and correctly assuming credentials.</p> <p>By default, most of the time it was easier to use a dedicated IAM credential setup for the purpose.</p> <p>However, as I've wanted to run some scripts across multiple accounts, the need to simplify by assuming a role has been more important.</p> <p>It's also a better practice than having to manage multiple key rotations in all accounts.</p> <p>First, as I've had the need to work with more tooling, I'm not using the SDK encrypted <code>json</code> file.</p> <p>Instead, I'm leveraging the <code>~/.aws/credentials</code> profile in the standard <code>ini</code> format to ensure my tooling (docker included) can pull credentials correctly.</p> <p>Configure your file in the standard format.</p> <p>Setup a <code>[default]</code> profile in your credentials manually or through <code>Initialize-AWSDefaultConfiguration -ProfileName 'my-source-profile-name' -Region 'us-east-1' -ProfileLocation ~/.aws/credentials</code>.</p> <p>If you don't set this, you'll need to modify the examples provided to include the source <code>profilename</code>.</p> <p>{{&lt; gist sheldonhull  \"e73dc7689be62dc7e8946d4ab948728b\" \"aws-cred-example\" &gt;}}</p> <p>Next, ensure you provide the correct Account Number for the role you are trying to assume, while the MFA number is going to come from the \"home\" account you setup. For the <code>Invoke-Generate</code>, I use a handy little generator from <code>Install-Module NameIt -Scope LocalUser -Confirm:$false</code>.</p> <p>{{&lt; gist sheldonhull  \"e73dc7689be62dc7e8946d4ab948728b\" \"aws-sts-assume-role-example.ps1\" &gt;}}</p> <p>Bonus: Use Visual Studio Code Snippets and drop this in your snippet file to quickly configure your credentials in a script with minimal fuss. \ud83c\udf89</p> <p>{{&lt; gist sheldonhull  \"e73dc7689be62dc7e8946d4ab948728b\" \"vscode-snippet.json\" &gt;}}</p> <p>I think the key area I've missed in the past was providing the mfa and token in my call, or setting up this correctly in the configuration file.</p>","tags":["tech","development","aws","powershell"]},{"location":"assume-a-role-with-aws-powershell-tools/#temporary-credentials","title":"Temporary Credentials","text":"<p>In the case of needing to generate a temporary credential, say for an environment variable based run outside of the SDK tooling, this might also provide something useful.</p> <p>It's one example of further reducing risk vectors by only providing a time-limited credential to a tool you might be using (can limit to a smaller time-frame).</p> <p>{{&lt; gist sheldonhull  \"e73dc7689be62dc7e8946d4ab948728b\" \"generate-temporary-credentials.ps1\" &gt;}}</p>","tags":["tech","development","aws","powershell"]},{"location":"assume-a-role-with-aws-powershell-tools/#aws-vault","title":"AWS-Vault","text":"<p>Soon to come, using aws-vault to improve the security of your AWS sdk credentials further by simplifying role assumption and temporary sessions.</p> <p>I've not ironed out exactly how to deal with some issues with using this great session tool when jumping between various tools such as PowerShell, python, docker, and more, so for now, I'm not able to provide all the insight. Hopefully, I'll add more detail to leveraging this once I get things ironed out.</p> <p>Leave a comment if this helped you out or if anything was confusing so I can make sure to improve a quick start like this for others. \ud83c\udf2e</p>","tags":["tech","development","aws","powershell"]},{"location":"hello-world/","title":"Go R1 Day 1","text":"","tags":["tech","development","golang","100DaysOfCode"]},{"location":"hello-world/#day-1-of-100","title":"Day 1 of 100","text":"","tags":["tech","development","golang","100DaysOfCode"]},{"location":"hello-world/#progress","title":"progress","text":"<ul> <li>Cloned <code>learning-go-with-tests</code> to ensure a nice structured start, even though I've already done <code>hello-world</code></li> <li>Setup fresh <code>gotools</code> updates - Ran golangci-lint through docker to ensure improved linting options ready for further tests</li> <li>Fixed default debug template in vscode to use workspacefolder instead of file directory. Strange that it defaulted to the wrong path.</li> </ul>","tags":["tech","development","golang","100DaysOfCode"]},{"location":"hello-world/#links","title":"links","text":"<ul> <li>learn-go-with-tests</li> <li>golangci-lint</li> </ul>","tags":["tech","development","golang","100DaysOfCode"]},{"location":"setting-default-open-with-on-macos/","title":"setting default open with on macOS","text":"<p>It should be easy to pick a default program to open a file. On macOS, I was surprised at how poor the design was. Seriously, how is this intuitive? <code>Open With</code> &gt; <code>Set this as default</code>. Apparently this only set it for an individual file. This means, every different <code>csv</code> file required me to do this again.</p> <p>Instead, I had to <code>Get Info</code> &gt; <code>Unlock settings</code> and then choose the default <code>Open With</code> setting, and further select <code>Use this application to open all documents like this.</code></p> <p>I enjoy most of my development experience with macOS.</p> <p>Don't try and tell me that it is the pinnacle of usability though, some of this stuff is just quirky and over complicated. In what world, should my default behavior be set on a specific file and not the the file type?</p>","tags":["tech","development","macOS"]},{"location":"go-R1-D002/","title":"Go R1 Day 2","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D002/#day-2-of-100","title":"Day 2 of 100","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D002/#progress","title":"progress","text":"<ul> <li>Focused on logistics, setup for remaining tests</li> <li>Forked Learn Go With Tests</li> <li>Setup Visual Studio Codespaces (Docker Workspace) for isolating this work</li> <li>Codespaces default container + Added pwsh as default prompt</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D002/#links","title":"links","text":"<ul> <li>Learn Go With Tests</li> <li>Gitbook.io Learn Go With Tests</li> <li>Forked: Learn Go With Tests</li> <li>Commit: Add devcontainer config</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"grave-accent/","title":"grave accent","text":"<p>TIL: What I've been calling the backtick \ud83d\udc49 ```` \ud83d\udc48 for years, is technically the grave accent.</p>","tags":["tech","development","microblog","til"]},{"location":"simple-conditional-flag-in-terraform/","title":"simple conditional flag in terraform","text":"<p>Sometimes, you just need a very simple flag for enabled or disabled, or perhaps just a resource to deploy if <code>var.stage == \"qa\"</code>. This works well for a single resource as well as collections if you provide the splat syntax.</p> <pre><code>resource \"aws_ssm_association\" \"something_i_need_in_testing_only\" {\n   count = var.stage == \"qa\" ? 1 : 0\n   name = var.name\n}\n</code></pre>","tags":["tech","development","microblog","terraform"]},{"location":"random-microsoft-teams-quirks-01/","title":"random-microsoft-teams-quirks-01","text":"<ul> <li>Using <code>cmd+=</code> results in zooming to 120%,145%,170%</li> <li>Using <code>cmd+-</code> results in zooming out to 85%, 70%, 60%</li> </ul>","tags":["tech","development","microblog","teams"]},{"location":"go-R1-D003/","title":"Go R1 Day 3","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D003/#day-3-of-100","title":"Day 3 of 100","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D003/#progress","title":"progress","text":"<ul> <li>Learned about GOROOT, GOPATH and how to configure</li> <li>Ran into problems with Visual Studio code reporting:</li> </ul> <pre><code>Failed to find the \"go\" binary in either GOROOT() or PATH(/usr/bin:/bin:/usr/sbin:/sbin. Check PATH, or Install Go and reload the window.\n</code></pre> <ul> <li>After attempting solution with various profile files, I tried setting the setting: <code>\"go.goroot\": \"/usr/local/opt/go/libexec/\",</code> in <code>settings.json</code> and this resolved the issue.</li> <li>After it recognized this, I ran the <code>Go: Current GOPATH</code> from the command palette and it found it.</li> <li>Finally, after this it reporting back some feedback showing it was recognizing the latest version I was running.</li> <li>Initialized a new serverless framework project for <code>aws-go-mod</code> template using the following command: <code>serverless create --template aws-go-mod --path ./sqlserver</code> and the initial project layout was created.</li> <li>I'm sure this will need to be improved as I go along, but since macOS failed on the go path setup, this resolved my problems for now.</li> </ul> <pre><code># GO: Make tools work in console sessions\n$ENV:GOPATH = \"$ENV:HOME$($ENV:USERPROFILE)/go\"\n\nif ($PSVersionTable.OS -match 'Darwin') {\n    $ENV:GOROOT = \"/usr/local/opt/go/libexec\"\n    $ENV:PATH += \"$ENV:PATH:$(go env GOPATH)/bin\"\n    $ENV:GOBIN = \"$(go env GOPATH)/bin\"\n}\n</code></pre>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D003/#links","title":"links","text":"<ul> <li>serverless aws-go-mod</li> <li>serverless aws-go-mod github</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D004/","title":"Go R1 Day 4","text":"","tags":["tech","development","golang","100DaysOfCode","microblog","api"]},{"location":"go-R1-D004/#day-4-of-100","title":"Day 4 of 100","text":"","tags":["tech","development","golang","100DaysOfCode","microblog","api"]},{"location":"go-R1-D004/#progress","title":"progress","text":"<ul> <li>In the last week some more Pluralsight Go (Nigel's courses are always stellar)</li> <li>I've done some odd work as had time in between last update, but nothing structured. Might continue with <code>learn-go-with-tests</code> but also came across exercism and might tackle that soon as well.</li> <li>Setup new project in Visual Studio Codespaces</li> <li>Used Serverless Framework to generate <code>aws-go</code> template project</li> <li>Imported <code>aws-go-sdk</code> resources for AWS SSM Secrets Manager</li> <li>Grabbed SSM Secrets manager output and used to create a POST request for authorization token</li> <li>Used DumpRequest to pretty print output for debugging purpose</li> <li>Hung my head in shame at my copy pasta project \ud83d\ude00</li> <li>Realized half of the battle is just getting my lambda debugging effort running locally. Not exactly the easiest starter project.</li> <li>Need to master different request types. PowerShell makes this easy with Invoke-RestMethod and object construction. I found that the <code>Post</code> method wasn't appropriate as I wanted to control custom headers. This required me to use a different approach, which I ended up doing with <code>http.Client{}</code>.</li> <li>Not sure in a Go Playground sample to test and echo a post request. Thinking if no service that does this might try creating a Go routine to accept the request locally and echo for the purpose of a demo. Any suggestions welcome.</li> <li>Identified i really need better understanding of the godoc libraries as would help a lot. Vscode intellisense even in codespaces isn't performing great, so I'm not getting that help that would really expedite discovery as a beginner. I might give my EAP Goland install a try and see if it helps.</li> <li>Successfully included <code>zap</code> logging library</li> <li>Successfully submitted request for authorization token and got response back into a struct, something that as a newbie I found a major win \ud83c\udf89 using <code>json.Unmarshal([]byte(body), ar)</code> with <code>ar</code> referring to a pointer another win for me as new to pointers) being passed in like below:</li> </ul> <pre><code>// AuthResponse the authorization object returned by taco service when doing fancy things\ntype AuthResponse struct {\n    MagicalToken string `json:\"magical_token\"`\n    ExpiresIn    int    `json:\"expires_in\"`\n}\n\nfunc main() {\n    var ar AuthResponse\n    getToken(&amp;ar) // pointer for the win \u26a1\ufe0f\n}\nfunc getToken(ar *AuthResponse) {\n    json.Unmarshal([]byte(body), ar) // \u2753 using bytes for json is new to me. gotta learn more\n    sugar.Infow(\"Logging library with zap used to generate structured logs\",\n    \"authResponse\", ar,  // trailing quotes is the norm, also a new pattern for me\n)\n}\n</code></pre>","tags":["tech","development","golang","100DaysOfCode","microblog","api"]},{"location":"go-R1-D004/#links","title":"links","text":"<ul> <li>http client.do</li> <li>DumpRequest</li> <li>serverless-framework</li> <li>httpbin.org</li> <li>zap</li> <li>json-to-go</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog","api"]},{"location":"go-R1-D005/","title":"Go R1 Day 5","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D005/#day-5-of-100","title":"Day 5 of 100","text":"","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D005/#progress","title":"progress","text":"<ul> <li>I created my first unit test for go</li> <li>It's a bit interesting coming from a background with PowerShell and Pester as my primary unit test framework. For instance, in Pester you'd declare the anything, but autodiscovery works with <code>*.tests.ps1</code>, being the normal convention.</li> <li>There is no pointer value providing the test package, it's just other PowerShell calling PowerShell.</li> <li>I'm biased I know, but the first test condition being like below seems clunky. I was hoping for something that was more like Pester with <code>test.Equals(got, want,\"Error message\")</code> as the syntax is more inline to what I'd expect. I haven't dived in further so this is just a thought, hoping this is just the newbie 101 test case example and there are more succinct comparison and test methods available.</li> </ul> <pre><code>package main\n\nimport \"testing\"\n\nfunc TestHello(t *testing.T) {\n    got := Hello()\n    want := \"Hello, world\"\n    if got != want {\n        t.Errorf(\"got %q want %q\", got, want)\n    }\n}\n</code></pre> <ul> <li>Update: 2020-08-24 2020-08-24</li> <li>I'll stick with the default package while I'm learning. However, there is a package called Testify that is worth exploring if I find I still want assertions later on.</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"go-R1-D005/#links","title":"links","text":"<ul> <li>learn-go-with-tests</li> <li>golang-writing-unit-tests</li> <li>testify</li> </ul>","tags":["tech","development","golang","100DaysOfCode","microblog"]},{"location":"incredible-setup-experience-with-gitpod/","title":"Incredible setup experience with gitpod","text":"<p>What a polished setup experience from gitpod. Super impressed as I've never seen an interactive terraform setup asking for user input.</p> <p>This basically generated an entire setup for GitPod, and persisted the Terraform plan for the entire stack it created in a directory for management and adjustments as desired.</p> <p>I'm seriously impressed.</p> <p>Check this out at: Install on AWS</p> <pre><code># Setup Gitpod with self-setup docker installer\n# https://www.gitpod.io/docs/self-hosted/latest/install/install-on-aws-script/\n# Set-Location ./terraform/gitpod\n\nImport-Module aws.tools.common, aws.tools.SecurityToken\n\nSet-AWSCredential -ProfileName 'MyProfileName' -Scope Global\n\n$ENV:AWS_ACCESS_KEY_ID = $cred.GetCredentials().AccessKey\n$ENV:AWS_SECRET_ACCESS_KEY = $cred.GetCredentials().SecretKey\n$ENV:AWS_DEFAULT_REGION = 'eu-west-1'\n\n# can't use STS temporary credentials to create iam resources, so use normal iam credentials\ndocker run --rm -it -e AWS_ACCESS_KEY_ID=$ENV:AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY=$ENV:AWS_SECRET_ACCESS_KEY -e AWS_DEFAULT_REGION=$ENV:AWS_DEFAULT_REGION `\n-v ${PWD}/awsinstall:/workspace eu.gcr.io/gitpod-io/self-hosted/installer:latest aws\n</code></pre>","tags":["tech","development","microblog","docker","kubernetes","terraform"]},{"location":"go-r1-day-6/","title":"Go R1 Day 6","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-6/#day-6-of-100","title":"Day 6 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-6/#progress","title":"progress","text":"<ul> <li>Learned a little on bubble sort algorithm</li> <li>Edited aws-go lambda tests</li> <li>Used debugger a bit more to look at error return values on failure</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"rebuild-dash-index-due-to-corrupted-index/","title":"Rebuild Dash Index Due to Corrupted Index","text":"<p>I use Dash for improved doc access. Terraform updated recently to <code>0.13.x</code> and I began having odd issues with AWS provider results coming through. If you need to rollback, just go to the preferences and pick an older docset, in my case <code>0.13.0</code> worked correctly. Make sure to remove the problematic version (the uninstall refers to just the most recent, not any additional versions you selected under the dropdown)</p> <p>If the index doesn't rebuild, you can close the app, manually remove the index, and it will rebuild on open. I'm pretty sure you don't need to do this if you use the uninstall option in the dialogue.</p> <p>On macOS 10.15, you can find the index at <code>~/Library/Application Support/Dash/Data/manIndex.dsidx</code> and delete this. Reopen Dash and it will rebuild the index.</p>","tags":["tech","development","microblog","aws","terraform","cool-tools"]},{"location":"go-r1-day-7/","title":"Go R1 Day 7","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-7/#day-7-of-100","title":"Day 7 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-7/#progress","title":"progress","text":"<ul> <li>Successfully deployed lambda to AWS</li> <li>Figured out how to return value cleanly from AWS Lambda vs the log output.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/","title":"Reflections on Being a New Gopher With A Dotnet Background","text":"","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#disclaimer","title":"Disclaimer","text":"<p>Newbie Gopher. Much of what I observe is likely to be half right. I'll probably look back at the end of the year and shake my head, but gotta start the journey somewhere, right? \ud83d\ude01</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#my-background","title":"My Background","text":"<p>I've learned my development skills primarily in the dotnet world. Coming from SQL Server performance, schema, and development, I transitioned into learning PowerShell and some C#. I found for the most part the \"DevOps\" nature of what I was doing wasn't a good fit for focusing on C# and so have done the majority of my development in PowerShell.</p> <p>For those unfamilar with PowerShell, I highly recommend it as a fantastic shell experience and as a great ecosystem for automation oriented programming.</p> <p>With everything as an \"object\" in PowerShell, not only does the manipulation of those objects tend to be pretty flexible, but built in methods, properties, and deep diving into object collections in objection collections provide powerful control of working with things like AWS.Tool.SDK and Windows.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#why-i-started-with-go","title":"Why I Started With Go","text":"<p>I wanted to focus on Python, as I think it's a beautiful language and I'd probably be more productive quickly in it. However, as I started my Python journey I discovered that I was basically substituting PowerShell (my goto) with a language that in many ways was very similar.</p> <p>Python provides a \"glue\" language that can do many things, in many varying domains with a vast ecosystem of user built modules.</p> <p>So why Go?</p> <p>Primarily, I wanted 2 main things from my next step:</p> <ul> <li>Expand my horizons with a language that approached things differently than what I'm used to, forcing me to solve problems in a new way, not just a simple syntax shift.</li> <li>Try something that was more strictly typed and opinionated to help align my next development to be less \"adhoc\" but more robust.</li> </ul> <p>Some additional perks would be: choosing a language that made me more competitive in the market (so choosing low adoption languages), immersed me more in Linux while still supporting cross platform work, and additionally supported quick adhoc tooling to be able to deliver small portable tooling to help support a DevOps culture.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#is-it-better-than-x","title":"Is it better than x","text":"<p>No. It's not \"better\" than PowerShell, C#, Python, or even Bash.</p> <p>That's an incomplete way to consider this, just like saying your hammer is better than a screwdriver or replaces the need for a table saw. \ud83d\ude01</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#how-its-helping-me-think-differently","title":"How It's Helping Me Think Differently","text":"<p>What I find myself observing is it is accomplishing one of my primary objectives of thinking differently.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#interfaces","title":"Interfaces","text":"<p>Interfaces are something I've rarely used, as I'm not doing much C#, but when I had used this it seemed to be extra work for the the sack of good design and abstraction of implementation. Since, most of my development was automation oriented, I had little case for regularly using them.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#bytes","title":"Bytes","text":"<p>Bytes are something I've rarely had to work with, as it's abstracted in PowerShell. However, in Go, bytes seem to be a universal medium.</p> <p>Let's say I need to run a web request for gathering some results in json and manipulating then. With PowerShell a simple rest call with <code>ConvertFrom-Json</code> is all that is required. With Go, you have to convert the bytes and use <code>json.Unmarshal</code> as one way to take the response and place the response into a predefined struct (or empty struct is possible as well I believe)</p> <p>Want to deal with images, files, web requests, and more... bytes. Endless streams of bytes.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#dotnet-types","title":"Dotnet Types","text":"<p>Say, I want to work with a semver version number. This is provided in the standard System library in C#, and also accessible via a type accelerator in PowerShell.</p> <pre><code>$ver = [System.Version]::new()\n[System.Version]::TryParse($ParseMe,[ref]$ver) # Returned bool success/fail\nWrite-Host $ParseMe\n</code></pre> <p>In Go, this wouldn't be part of the standard library (at least from what I've found). Instead, you'd get the same essential functionality by finding a package like: go-version and using it.</p> <p>Instead of a class object being returned, such as shown about with PowerShell, you'd have structs returned.</p> <p>Where <code>[pscustomobject]@{}| Get-Member</code> would report back the .NET type of <code>System.Management.Automation.PSCustomObject</code>, running reflection against something go.</p> <p>From go-version</p> <pre><code>// Version represents a single version.\ntype Version struct {\n    metadata string\n    pre      string\n    segments []int64\n    si       int\n    original string\n}\n</code></pre> <p>This would be basically the new Type that I'd expect to see in Go.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#last-thoughts","title":"Last Thoughts","text":"<p>I'm really enjoying the experience of looking at things from a new perspective and doing my best to keep an open mind to learn Go without prejudice to force it to my way of thinking.</p> <p>So far it has been interesting and probably harder initially because my paradigm has come from the dotnet ecosystem. Removing dotnet from the equation, it's interesting to see how much I take for granted.</p> <p>I think if I had been immersed in Python from the beginning, transitioning to Go would be more intuitive to me. When I was juggling C# and PowerShell, I found it very intuitive as most of the basics were just syntax differences, until you added in more complex things like Linq/Delegates, abstract classes, interfaces, and other things that don't have quite the place in the PowerShell world.</p> <p>The generalization of \"everything is an object\" in Windows vs in Linux \"everything is a file/text\" rings true in the approach I think I've observed underlying the way the languages function.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#give-something-new-a-shot","title":"Give Something New A Shot","text":"<p>If you are from a Linux background, I'd suggest you give PowerShell a shot (its cross platform now) and try this experience of learning a new paradigm. Using something like <code>InvokeBuild</code> will give you a much richer experience than <code>Make</code> files. If from Windows, then Python or Go would be a great choice to help challenge you.</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"reflections-on-being-a-new-gopher-with-a-dotnet-background/#give-yourself-time-to-be-terrible-at-it","title":"Give Yourself Time To Be Terrible At It","text":"<p>I read recently from @Duffney:</p> <p>My biggest challenge with using the 20 hours rule to learn new things is becoming comfortable being \"unproductive\"</p> <p>I think that was a very freeing concept, as I've wrestled with this. Being pretty experienced in PowerShell, being ok writing non-idiomatic Go code with rough project structure, limited tests, and more has been mentally inhibiting.</p> <p>It's always better to get momentum on something and once competence is growing, challenge yourself to excel with more idomatic, tested, and well designed solutions. Build and ship some stuff so you can even get to the place where you wince at your old work in the first place \ud83d\ude06</p>","tags":["tech","development","golang","dotnet","100DaysOfCode"]},{"location":"go-r1-day-8/","title":"Go R1 Day 8","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-8/#day-8-of-100","title":"Day 8 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-8/#progress","title":"progress","text":"<ul> <li>Worked through Algorithms in Go: <code>Determine if a number is in a list</code></li> <li>Passed the tests without needing to find the solution, so that's a win.</li> <li>Had to remember to grab the second value from the range operator like <code>for _, i := range items</code> instead of using <code>foreach($i in $items)</code> like PowerShell would do. It's more similar to using hashtables with <code>GetEnumerator()</code>.</li> <li>Used codespaces with Docker and also experimented with WSL2 and Visual Studio Code mounting the directory from inside WSL2.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-8/#links","title":"links","text":"<ul> <li>Algorithms With Go</li> <li>num_in_list.go</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"terraform%27s-elegance-in-abstraction/","title":"Terraform's Elegance in Abstraction","text":"<p>Migrated a forked copy of a module over to a new module with similar schema. There were some additional properties that were removed. In rerunning the plan I was expecting to see some issues with resources being broken down and rebuilt. Instead, Terraform elegantly handled the module change.</p> <p>I imagine this has to do with the resource name mapping being the same, but regardless it's another great example of how agile Terraform can be.</p>","tags":["tech","development","microblog","terraform"]},{"location":"go-r1-day-10/","title":"Go R1 Day 10","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-10/#day-10-of-100","title":"Day 10 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-10/#progress","title":"progress","text":"<ul> <li>Experimented with CLI tool using go-prompt</li> <li>Customized initial options</li> <li>OS independent call to get user home directory.</li> <li>Iterated through a directory listing</li> <li>Used path join to initialize path for directory search.</li> <li>One challenge in working with structs being returned was figuring out how to print the values of the struct. Initially, I only had pointers to the values coming back. This made sense, though, as I watched a tutorial this weekend on slices, and better understand that a slice is actually a small data structure being described by: pointer to the location in memory, length, and the capacity of the slice. Without this tutorial, I think seeing the pointer addresses coming through would have been pretty confusing.</li> <li>In reading StackOverflow, I realized it's a \"slice of interfaces\".</li> <li>Worked with apex logger and moved some of the log output to debug level logging.</li> <li>Final result</li> </ul> <p>{{&lt; asciinema id=\"uAGRQLD2Tuj3NVgDePMGqVnT1\" &gt;}}</p>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-10/#links","title":"links","text":"<ul> <li>apex-log</li> <li>prettyslice</li> <li>go-prompt</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-10/#source","title":"source","text":"<p>{{&lt; gist sheldonhull  \"709b7cf02c40863c3c845de9b4fb6d5a\" &gt;}}</p>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-9/","title":"Go R1 Day 9","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-9/#day-9-of-100","title":"Day 9 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-9/#progress","title":"progress","text":"<ul> <li>Watched some Pluralsight and other material on testing with Go</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"delta-improves-the-git-diff-experience/","title":"Delta Improves the Git Diff Experience","text":"<p>Checkout delta for a much-improved git diff experience. I typically use VSCode or a GUI based editor because I find the diff view pretty messy by default.</p> <p>This new diff view is a perfect example of a simple CLI tool that improves a development workflow by just fixing something I didn't know could easily be fixed. \ud83d\ude00</p> <p></p>","tags":["tech","development","microblog","git","cool-tools"]},{"location":"improve-your-cloudwatch-debugging-experience-with-cw/","title":"Improve Your Cloudwatch Debugging Experience With Cw","text":"<p>A quick fix to improve your debugging of remote commands in AWS is to install cw.</p> <p>With a quick install, you can run a command like: <code>cw tail -f --profile=qa --region=eu-west-1 ssm/custom-automation-docs/my-custom-doc</code>. This will give you a real-time stream of what's running.</p> <p>You can also use the AWS Visual Studio Code extension, but I prefer having a terminal open streaming this as I don't have to go in and refresh any further tools to see what's happening. I tend to always start with a single instance/resource for debugging so this is a great way to remove the barrier to visibility a bit more.</p>","tags":["tech","development","microblog","cool-tools","aws","golang"]},{"location":"deadcells/","title":"Deadcells","text":"<p>Beat Deadcells with 3 cells active. Uninstalled. There is no way I'd find any pleasure in life trying to do more. This game is an endless pit of \"git gud\".</p> <p>Now to go do something productive \ud83d\ude04</p>","tags":["gaming","ramblings"]},{"location":"go-r1-day-11/","title":"Go R1 Day 11","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-11/#day-11-of-100","title":"Day 11 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-11/#progress","title":"progress","text":"<ul> <li>Pluralsight Go material and some reading.</li> <li>Start Learning Go Web Development course by Jon Calhoun.</li> <li>Created repo and logged initial progress</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-11/#link","title":"Link","text":"<ul> <li>New Repo: web-development-with-go to log my public progress</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-12/","title":"Go R1 Day 12","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-12/#day-12-of-100","title":"Day 12 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-12/#progress","title":"progress","text":"<ul> <li>Worked on Algolia index project to do atomic updates on search index on my blog.</li> <li>Worked with json, structs, ranges, and more.</li> <li>Saw success with the first value in my output now correctly parsing out the title from the front matter.</li> <li>Implemented zerolog.</li> <li>Used front library to parse yaml front matter into map.</li> <li>Accessed map to get title into json.</li> </ul> <p>Hoping that eventually I can build out a Go app for sharing that's the equivalent of \"atomic alogia\" allowing diff updates. I haven't found anything like that for hugo so far.</p>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-12/#links","title":"links","text":"<ul> <li>gernest/front</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"keep-the-snippet-simple/","title":"Keep the Snippet Simple","text":"<p>I took a quick step back when too many parentheses started showing up. If you question the complexity of your quick snippet, you are probably right that there is a much simpler way to do things.</p> <p>I wanted to get a trimmed message of the results of <code>git status -s</code>. As I worked on this snippet, I realized it was becoming way overcomplicated. \ud83d\ude06</p> <pre><code>$(((git status -s) -join ',') -split '')[0..20] -join ''\n</code></pre> <p>I knew my experimentation was going down the wrong road, so I took a quick step back to see what someone else did. Sure enough, Stack Overflow provided me a snippet.</p> <pre><code>$(((git status -s) -join ','))[0..20] -join ''     # returns the string '12345'\n</code></pre> <p>Moral of the story... there's always someone smarter on Stack Overflow. \ud83d\ude06</p>","tags":["tech","development","microblog"]},{"location":"ways-to-improve-codespaces-local-docker-experience/","title":"Ways to Improve Codespaces Local Docker Experience","text":"<p>I've been enjoying Codespaces local development workflow with Docker containers.</p> <p>I'm using macOS and on Docker experimental release. Here are some ideas to get started on improving the development experience.</p> <ul> <li>Clone the repository in the virtual volume (supported by the extension) to eliminate the binding between host and container. This would entail working exclusively inside the container.</li> <li>Increased Docker allowed ram to 8GB from the default of 2GB.</li> </ul> <p>Any other ideas? Add a comment (powered by GitHub issues, so it's just a GitHub issue in the backend)</p>","tags":["tech","development","microblog","docker","codespaces","visual-studio-code"]},{"location":"set-theory-basics-in-the-eyes-of-10-year-old/","title":"Set Theory Basics in the Eyes of 10 Year Old","text":"<p>My morning. Explaining set and intersect theory basics to my 10 year old with Minecraft gamer tags. Trying to justify the need to know this, the best I could come up with was his future need to build a shark attack report accurately.</p> <p>Kids are the best. Tech is fun. What job would have me spin up with <code>docker-compose up -d</code> my MSSQL container, write a quick SQL example with <code>INTERSECT</code>, <code>UNION</code> and all to demonstrate this magic.</p> <p>Followed it up with a half-hearted lie that my day is comprised of <code>cmatrix</code> \ud83d\ude02  which he didn't believe for more than a couple seconds.</p> <p>{{&lt; asciinema id=\"DnQ0MCgZekv11MggByfjqRNNT\" &gt;}}</p>","tags":["tech","development","microblog","dadlife"]},{"location":"go-r1-day-13/","title":"Go R1 Day 13","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-13/#day-13-of-100","title":"Day 13 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-13/#progress","title":"progress","text":"<ul> <li>Worked with type asserts in my efforts to generate json collection from the parsed front matter.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-13/#links","title":"links","text":"<ul> <li>Handling Type Assertion Failure</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"quick-start-to-using-influxdb-on-macos/","title":"Quick Start to Using Influxdb on Macos","text":"","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#intro","title":"Intro","text":"<p>OSS 2.0 is a release candidate at this time, so this may change once it's released.</p> <p>It wasn't quite clear to me how to get up and running quickly with a docker based setup for OSS 2.0 version, so this may save you some time if you are interested. It also should be very similar to the Windows workflow excepting the basic <code>brew</code> commands and service install commands you'll just want to flip over to <code>choco install telegraf</code> .</p>","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#docker-compose","title":"Docker Compose","text":"<p>Grabbed this from a comment and modified the ports as the were flipped from the <code>9999</code> range used during first early access.</p> <pre><code># docker exec -it influxdb /bin/bash\n\nversion: \"3.1\"\nservices:\n  influxdb:\n    restart: always  # It will always restart on rebooting machine now, no need to manually manage this\n    container_name: influxdb\n    ports:\n      - '8086:8086'\n    images: 'quay.io/influxdb/influxdb:2.0.0-rc'\n    volumes:\n      - influxdb:/var/lib/influxdb2\n    command: influxd run --bolt-path /var/lib/influxdb2/influxd.bolt --engine-path /var/lib/influxdb2/engine --store bolt\nvolumes:\n  influxdb:\n</code></pre> <p>The main modifications I made was ensuring it auto started.</p> <p>Access the instance on <code>localhost:8086</code>.</p>","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#telegraf","title":"Telegraf","text":"<p>It's pretty straight-forward using homebrew. <code>brew install telegraf</code></p> <p>The configuration file is created by default at: <code>/usr/local/etc/telegraf.conf</code> as well as the <code>telegraf.d</code> directory.</p> <p>I'm still a bit new on macOS, so once I opened Chronograf, I wanted to try the new http based configuration endpoint, so I used the web gui to create a telegraf config for system metrics and tried replacing the <code>telegraf.conf</code> reference in the plist file. This didn't work for me as I couldn't get the environment variable for the token to be used, so I ended up leaving it as is, and instead edited the configuration.</p> <ul> <li><code>brew services stop telegraf</code></li> <li><code>micro /usr/Local/Cellar/telegraf/1.15.3/homebrew.mxcl.telegraf.plist</code></li> </ul> <p>I updated the configuration (see line 16) unsuccessfully with the http config endpoint.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n  &lt;dict&gt;\n    &lt;key&gt;KeepAlive&lt;/key&gt;\n    &lt;dict&gt;\n      &lt;key&gt;SuccessfulExit&lt;/key&gt;\n      &lt;false/&gt;\n    &lt;/dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;homebrew.mxcl.telegraf&lt;/string&gt;\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n      &lt;string&gt;/usr/local/opt/telegraf/bin/telegraf&lt;/string&gt;\n      &lt;string&gt;-config&lt;/string&gt;\n      &lt;string&gt;/usr/local/etc/telegraf.conf&lt;/string&gt;\n      &lt;string&gt;-config-directory&lt;/string&gt;\n      &lt;string&gt;/usr/local/etc/telegraf.d&lt;/string&gt;\n    &lt;/array&gt;\n    &lt;key&gt;RunAtLoad&lt;/key&gt;\n    &lt;true/&gt;\n    &lt;key&gt;WorkingDirectory&lt;/key&gt;\n    &lt;string&gt;/usr/local/var&lt;/string&gt;\n    &lt;key&gt;StandardErrorPath&lt;/key&gt;\n    &lt;string&gt;/usr/local/var/log/telegraf.log&lt;/string&gt;\n    &lt;key&gt;StandardOutPath&lt;/key&gt;\n    &lt;string&gt;/usr/local/var/log/telegraf.log&lt;/string&gt;\n  &lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre> <p>What worked for me was to edit: <code>micro /usr/local/etc/telegraf.conf</code> and add the following (I set the token explicitly in my test case).</p> <pre><code> [[outputs.influxdb_v2]]\n  urls = [\"http://localhost:8086\"]\n  token = \"$INFLUX_TOKEN\"\n  organization = \"sheldonhull\"\n  bucket = \"telegraf\"\n</code></pre> <ul> <li>Start service with <code>brew services restart telegraf</code> and it should start sending data.</li> <li>NOTE: I'm still getting the hang of brew and service management on Linux/macOS, so the first time I did this it didn't work and I ended up starting it using <code>telegraf -config http://localhost:8086/api/v2/telegrafs/068ab4d50aa24000</code> and just running initially in my console (having already set the <code>INFLUX_TOKEN</code> environment variable) Any comments on if I did something wrong here would be appreciated  I'm pretty sure the culprit is the need for the <code>INFLUX_TOKEN</code> environment variable and I'm not sure if the service load with brew is actually sourcing the <code>.profile</code> I put this in. Maybe I can pass it explicitly?</li> </ul>","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#additional-monitoring","title":"Additional Monitoring","text":"<p>This is a work in progress. I found GitHub Issue #3192 and used it as a starting point to experiment with getting a \"top processes\" for evaluating what specifically was impacting my systems at the time of a spike. I'll update this once I've gotten things further improved.</p> <pre><code># # Monitor process cpu and memory usage\n# https://github.com/influxdata/telegraf/tree/master/plugins/inputs/procstat\n[[inputs.procstat]]\n    pattern = \"${USER}\"\n    fieldpass = [\n      \"cpu_time_user\",\n      \"cpu_usage\",\n      \"memory_rss\",\n    ]\n\n[[processors.topk]]\n  namepass = [\"*procstat*\"]\n  fields = [\n      \"cpu_time_user\",\n      \"cpu_usage\",\n      \"memory_rss\",\n  ]\n  period = 20\n  k = 3\n  # group_by = [\"pid\"]\n\n[[processors.regex]]\n  namepass = [\"*procstat*\"]\n  [[processors.regex.tags]]\n    key = \"process_name\"\n    pattern = \"^(.{60}).*\"\n    replacement = \"${1}...\"\n</code></pre>","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#final-result","title":"Final Result","text":"<p>I like the final result. Dark theme for the win.</p> <p>I've had some spikes in Vscode recently, impacting my CPU so I've been meaning to do something like this for a while, but finally got it knocked out today once I realized there was a 2.0 docker release I could use to get up and running easily. Next step will be to add some process level detail so I can track the culprit (probably VScode + Docker Codespaces).</p> <p></p>","tags":["development","influxdb","macOS"]},{"location":"quick-start-to-using-influxdb-on-macos/#wishlist","title":"Wishlist","text":"<ul> <li>Pretty formatting of date/time like Grafana does, such as converting seconds into hour/minutes.</li> <li>Log viewing api so I could query cloudwatch logs like Grafana offers without needing to ingest.</li> <li>Edit existing telegraf configuration in the load data section. Right now I can't edit.</li> <li>MSSQL Custom SQL Server query plugin to be released  Issue 1894 &amp; PR 3069 Right now I've done custom exec based queries using dbatools and locally included PowerShell modules. This sorta defeats the flexibility of having a custom query call so I can minimize external dependencies.</li> </ul>","tags":["development","influxdb","macOS"]},{"location":"unable-to-resolve-provider-aws-with-terraform-013/","title":"Unable To Resolve Provider AWS with Terraform Version 0.13.4","text":"<p>I couldn't get past this for a while when I accidentally stumbled across a fix. I believe the fix was merged, however this problem still existed in <code>0.13.4</code> so I stuck with it.</p> <p>{{&lt; admonition type=info title=\"GitHub Issues\" open=true &gt;}} When investigating the cause, I found this PR which intended this to be the installer behaviour for the implicit global cache, in order to match 0.12. Any providers found in the global cache directory are only installed from the cache, and the registry is not queried. Note that this behaviour can be overridden using provider_installation configuration. That is, you can specify configuration like this <code>~/.terraform.d/providercache.tfrc</code></p> <p>GitHub Issue Comment</p> <p>{{&lt; /admonition &gt;}}</p> <p>I used the code snippet here: <code>micro ~/.terraform.d/providercache.tfrc</code></p> <p>Wasn't sure if it was interpreted with shell, so I didn't use the relative path <code>~/.terraform.d/plugins</code>, though that might work as well.</p> <pre><code>provider_installation {\n  filesystem_mirror {\n    path = \"/Users/sheldonhull/.terraform.d/plugins\"\n  }\n  direct {\n    exclude = []\n  }\n}\n</code></pre> <p>After this <code>terraform init</code> worked.</p>","tags":["tech","development","microblog","terraform","troubleshooting"]},{"location":"improving-local-development-workflow-with-go-task/","title":"Improving Local Development Workflow With Go Task","text":"","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#workflow-tooling","title":"Workflow Tooling","text":"<p>Development workflow, especially outside of a full-fledged IDE, is often a disjointed affair. DevOps oriented workflows that often combine cli tools such as terraform, PowerShell, bash, and more all provide more complexity to getting up to speed and productive.</p> <p>Currently, there is a variety of frameworks to solve this problem. The \"gold standard\" most are familiar with in the open-source community would be Make.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#considering-cross-platform-tooling","title":"Considering Cross-Platform Tooling","text":"<p>This is not an exhaustive list, it's focused more on my journey, not saying that your workflow is wrong.</p> <p>I've looked at a variety of tooling, and the challenge has typically that most are very unintuitive and difficult to remember.</p> <p>Make...it's everywhere. I'm not going to argue the merits of each tool as I mentioned, but just bring up that while cMake is cross platform, I've never considered Make a truly cross platform tool that is first class in both environments.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#invokebuild-psake","title":"InvokeBuild &amp; Psake","text":"<p>In the Windows world, my preferred framework would be InvokeBuild or PSake.</p> <p>The thing is, not every environment will always have PowerShell, so I've wanted to experiment with minimalistic task framework for intuitive local usage in a project when the tooling doesn't need to be complex. While <code>InvokeBuild</code> is incredibly flexible and intuitive, there is an expectation of familarity with PowerShell to fully leverage.</p> <p>If you want a robust framework, I haven't found anything better. Highly recommend examining if you are comfortable with PowerShell. You can generate VSCode tasks from your defined scripts and more.</p> <p><code>InvokeBuild</code> &amp; <code>Psake</code> aren't great for beginners just needing to run some tooling quickly in my experience. The power comes with additional load for those not experienced in PowerShell.</p> <p>If you are needing to interact with AWS.Tools SDK, complete complex tasks such as generating objects from parsing AST (Abstract Syntax Trees) and other, then I'd lead towards <code>InvokeBuild</code>.</p> <p>However, if you need to initialize some local dependencies, run a linting check, format your code, get the latest from main branch and rebase, and other tasks that are common what option do you have to get up and running more quickly on this?</p>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#task","title":"Task","text":"<p>Go Task</p> <p>I've been pleasantly surprised by this cross-platform tool based on a simple <code>yaml</code> schema. It's written in go, and as a result it's normally just a single line or two to immediately install in your system.</p> <p>Here's why you might find some value in examining this.</p> <ol> <li>Cross-platform syntax using this go interpreter sh</li> <li>Very simple <code>yaml</code> schema to learn.</li> <li>Some very nice features that make it easy to ignore already built assets, setup task dependencies (that run in parallel too!), and simple cli interactivity.</li> </ol> <p>My experience has been very positive as I've found it very intuitive to build out basic commands as I work, rather than having to deal with more more complex schemas.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#get-started","title":"Get Started","text":"<pre><code>version: 3\ntasks:\n  default: task --list\n  help: task --list\n\n  fmt:\n    desc: Apply terraform formatting\n    cmds:\n      - terraform fmt -recursive=true\n</code></pre> <p>The docs are great for this project, so I'm not going to try and educate you on how to use this, just point out some great features.</p> <p>First, with a quick VSCodee snippet, this provides you a quick way to bootstrap a new project with a common interface to run basic commands.</p> <p>Let's give you a scenario... assuming you aren't using an already built Docker workspace.</p> <ol> <li>I need to initialize my 2 terraform directories.</li> <li>I want to also ensure I get a few go dependencies for a project.</li> <li>Finally, I want to validate my syntax is valid among my various directories, without using pre-commit.</li> </ol> <p>This gets us started...</p> <pre><code>version: 3\ntasks:\n</code></pre> <p>Next, I threw together some examples here.</p> <ul> <li>Initialize commands for two separate directories.</li> <li>A <code>fmt</code> command to apply standardized formatting across all <code>tf</code> files.</li> <li>Finally, wrap up those commands with a <code>dep: []</code> value that will run the <code>init</code> commands in parallel, and once that is finished it will run <code>fmt</code> to ensure consistent formatting.</li> </ul> <pre><code>version: '3'\nenv:\n  TF_IN_AUTOMATION: 1\ntasks:\n  init-workspace-foo:\n    dir: terraform/foo\n    cmds:\n      - terraform init\n  init-workspace-bar:\n    dir: terraform/bar\n    cmds:\n      - terraform init\n  fmt:\n    desc: Recursively apply terraform fmt to all directories in project.\n    cmds:\n      - terraform fmt -recursive=true\n  init:\n    desc: Initialize the terraform workspaces in each directory in parallel.\n    deps: [init-workspace-foo,init-workspace-bar]\n    cmds:\n      - task: fmt\n</code></pre> <p>You can even add a task in that would give you a structured git interaction, and not rely on git aliases.</p> <pre><code>  sync:\n      desc: In GitHub flow, I should be getting lastest from main and rebasing on it so I don't fall behind\n      cmds:\n        - git town sync\n</code></pre>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#why-not-just-run-manually","title":"Why not just run manually","text":"<p>I've seen many folks online comments about why even bother? Can't the dev just run the commands in the directory when working through it and be done with it?</p> <p>I believe tasks like this should be thrown into a task runner from the start. Yes, it's very easy to just type <code>terraform fmt</code>, <code>go fmt</code>, or other simple commands... if you are the builder of that project.</p> <p>However:</p> <ul> <li>it increases the cognitive load for tedious tasks that no one should have to remember each time the project grows.</li> <li>It makes your project more accessible to new contributors/teammates.</li> <li>It allows you to simply moving to automation by wrapping up some of these automation actions in GitHub Actions or equivalent, but simply having the CICD tooling chosen run the same task you can run locally.</li> </ul> <p>Minimal effort to move it to automation from that point!</p> <p>I think wrapping up things with a good task runner tools considers the person behind you, and prioritizes thinking of others in the course of development. It's an act of consideration.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"improving-local-development-workflow-with-go-task/#choose-the-right-tooling","title":"Choose the Right Tooling","text":"<p>Here's how I'd look at the choices:</p> <ul> <li>Run as much in Docker as you can.</li> <li>If simple actions, driven easily on cli such as build, formatting, validation, and other then start with <code>Task</code> from the beginning and make your project more accessible.</li> <li>If requirements grow more complex, with interactions with AWS, custom builds for Lambda, combined with other more complex interactions that can't easily be wrapped up in a few lines of shell scripting... use <code>InvokeBuild</code> or equivalent. This gives you access to the power of <code>.NET</code> and the large module collection provided.</li> </ul> <p>Even if you don't really need it, think of the folks maintaining or enabling others to succeed with contributions more easily, and perhaps you'll find some positive wins there. \ud83c\udf89</p>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/","title":"Consider the Benefits of Powershell for Developer Workflows","text":"","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#who-am-i-talking-to","title":"Who Am I Talking To","text":"<ul> <li>You use bash or python.</li> <li>PowerShell seems wordy, extra verbose, and annoying.</li> <li>It's a windows thing, you say... why would I even look at it.</li> <li>Pry bash out of my fingers if yuo dare (probably not for you \ud83d\ude01)</li> </ul>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#what-powershell-is","title":"What PowerShell Is","text":"<ul> <li>The best language for automating Windows... period.</li> <li>A great language for development tooling and productivity scripts.</li> <li>One of the best languages for automation with interactivity. Python is fantastic. The REPL isn't meant for the same interactivity you get with PowerShell. PowerShell prompt is sorta like mixing Python &amp; fish/bash in a happy marriage.</li> <li>A rich language (not just scripting) for interacting with AWS using AWS.Tools.</li> <li>A rich object-oriented pipeline that can handle very complex actions in one-liners based on object-oriented pipelines.</li> <li>Intuitive and consistent mostly for command discovery.<ul> <li>a common complaint from bash pros.</li> <li>The point of the verbosity <code>Verb-Noun</code> is discoverability. <code>tar</code> for example is a bit harder to figure out than <code>Expand-Archive -Path foo -DestinationPath foo</code></li> </ul> </li> <li>A language with a robust testing framework for unit, integration, infrastructure, or any other kinda testing you want! (Pester is awesome)</li> </ul>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#what-powershell-isnt","title":"What PowerShell Isn't","text":"<ul> <li>Python \ud83e\udd23</li> <li>Good at datascience.</li> <li>Succinct</li> <li>Meant for high-concurrency</li> <li>Good at GUI's... but come-on we're devs... guis make us weak \ud83d\ude1c</li> <li>A good webserver</li> <li>Lots more.</li> </ul>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#the-right-tool-for-the-job","title":"The Right Tool for the Job","text":"<p>I'm not trying to tell you never to use bash. It's what you know, great!</p> <p>However, I'd try to say if you haven't explored it, once you get past some of the paradigm differences, there is a rich robust set of modules and features that can improve most folks workflow.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#why-even-consider-powershell","title":"Why Even Consider PowerShell","text":"<p>As I've interacted more and more with folks coming from a mostly Linux background, I can appreciate that considering PowerShell seems odd. It's only recently that it's cross platform in the lifecycle of things, so it's still a new thing to most.</p> <p>Having been immersed in the .NET world and now working on macOS and using Docker containers running Debian and Ubuntu (sometimes Alpine Linux), I completely get that's not even in most folks purview.</p> <p>Yet, I think it's worth considering for developer workflows that there is a lot of gain to be had with PowerShell for improving the more complex build and development workflows because of the access to .NET.</p> <p>No, it's not \"superior\". It's different. Simple cli bash scripting is great for many things (thus prior article about Improving development workflow <code>Task</code> which uses shell syntax).</p> <p>The fundemental difference in bash vs PowerShell is really text vs object, in my opinion. This actually is where much of the value comes in for considering what to use.</p> <p>{{&lt; admonition type=\"info\" title=\"Go For CLI Tools\" &gt;}} Go provides a robust cross-platform single binary with autocomplete features and more.</p> <p>I'd say that for things such as exporting pipelines to Excel, and other \"automation\" actions it's far more work in Go.</p> <p>Focus Go on tooling that makes the extra plumbing and stronger typing give benefit rather than just overhead. AWS SDK operations, serverless/lambda, apis, complex tools like Terraform, and more fit the bill perfectly and are a great use case. {{&lt; /admonition &gt;}}</p>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#scenario-working-with-aws","title":"Scenario: Working with AWS","text":"<p>If you are working with the AWS SDK, you are working with objects. This is where the benefit comes in over cli usage.</p> <p>Instead of parsing json results and using tools like <code>jq</code> to choose arrays, instead, you can interact with the object by named properties very easily.</p> <pre><code>$Filters = @([Amazon.EC2.Model.Filter]::new('tag:is_managed_by','muppets')\n$InstanceCollection = (Get-EC2Instance -Filter $Filters)).Instances | Select-PSFObject InstanceId, PublicIpAddress,PrivateIpAddress,Tags,'State.Code as StateCode', 'State.Name as StateName'  -ScriptProperty @{\n    Name = @{\n        get  = {\n            $this.Tags.GetEnumerator().Where{$_.Key -eq 'Name'}.Value\n        }\n    }\n}\n</code></pre> <p>With this <code>$InstanceCollection</code> variable, we now have access to an easily used object that can be used with named properties.</p> <ul> <li>Give me all the names of the EC2 instances: <code>$InstanceCollection.Name</code></li> <li>Sort those: <code>$InstanceCollection.Name | Sort-Object</code> (or use alias shorthand such as <code>sort</code>)</li> <li>For each of this results start the instances: <code>$InstanceCollection | Start-EC2Instance</code></li> </ul>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#practical-examples","title":"Practical Examples","text":"<p>Beyond that, we can do many things with the rich eco-system of prebuilt modules.</p> <p>Here are some example of some rich one-liners using the power of the object based pipeline.</p> <ul> <li>Export To Json: <code>$InstanceCollection | ConvertTo-Json -Depth 10 | Out-File ./instance-collection.json</code></li> <li>Toast notification on results: <code>Send-OSNotification -Title 'Instance Collection Results' -Body \"Total results returned: $($InstanceCollection.Count)\"</code></li> <li>Export To Excel with Table:  <code>$InstanceCollection | Export-Excel -Path ./instance-collection.json -TableStyle Light8 -TableName 'FooBar'</code></li> <li>Send a rich pagerduty event to flag an issue: <code>Send-PagerDutyEvent -Trigger -ServiceKey foo -Description 'Issues with instance status list' -IncidentKey 'foo' -Details $HashObjectFromCollection</code></li> <li>Use a cli tool to flip to yaml (you can use native tooling often without much issue!): <code>$InstanceCollection | ConvertTo-Json -Depth 10 | cfn-flip | Out-File ./instance-collection.yml</code></li> </ul> <p>Now build a test (mock syntax), that passes or fails based on the status of the instances</p> <p>{{&lt; admonition type=\"Note\" title=\"Disclaimer\" open=true &gt;}}</p> <p>I'm sure there's great tooling with <code>jq</code>, <code>yq</code>, excel clis and other libraries that can do similar work.</p> <p>My point is that it's pretty straight forward to explore this in PowerShell as object-based pipelines are a lot less work with complex objects than text based parsing.</p> <p>{{&lt; /admonition &gt;}}</p> <pre><code>Describe \"Instance Status Check\" {\n  Context \"Instances That Should Be Running\" {\n    foreach($Instance in $InstanceCollection)\n    {\n        It \"should be running\" {\n        $Instance.StatusName | Should -Be 'Running'\n        }\n    }\n  }\n}\n</code></pre> <p>Now you have a test framework that you could validate operational issues across hundreds of instances, or just unit test the output of a function.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#exploring-the-object","title":"Exploring the Object","text":"<p>I did this comparison once for a coworker, maybe you'll find it useful too!</p> <pre><code>\"Test Content\" | Out-File ./foo.txt\n$Item = Get-Item ./foo.txt\n\n## Examine all the properties and methods available. It's an object\n$Item | Get-Member\n</code></pre> <p>This gives you an example of the objects behind the scene. Even though your console will only return a small set of properties back, the actual object is a .NET object with all the associated methods and properties.</p> <p>This means that <code>Get-Item</code> has access to properties such as the base name, full path, directory name and more.</p> <p>You can access the actual <code>datetime</code> type of the <code>CreationTime</code>, allowing you to do something like:</p> <pre><code>($item.LastAccessTime - $Item.CreationTime).TotalDays\n</code></pre> <p>This would use two date objects, and allow you to use the relevant <code>Duration</code> methods due to performing math on these.</p> <p>The methods available could be anything such as <code>$Item.Encrypt(); $Item.Delete; $Item.MoveTo</code> and more all provided by the .NET namespace <code>System.IO.FileInfo</code>.</p> <p>I know many of these things you can do in bash as well, but the object pipeline here I'd wager provides a very solid experience for more complex operations based on the .NET framework types available.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"consider-the-benefits-of-powershell-for-developer-workflows/#wrap-up","title":"Wrap Up","text":"<p>This was meant to give a fresh perspective on why some folks have benefited from PowerShell over using shell scripting. It's a robust language that for automation/build/cloud automation can give a rich reward if you invest some time to investigate.</p> <p>For me the basic \"right tool for the job\" would like like this:</p> <ul> <li>data: python</li> <li>serverless: go &amp; python (powershell can do it too, but prefer the others)</li> <li>web: go &amp; python</li> <li>basic cli stuff: shell (using <code>Task</code> which uses shell syntax)</li> <li>complex cli project tasks: powershell &amp; go</li> <li>automation/transformation: powershell &amp; python</li> <li>high concurrency, systems programming: go</li> </ul> <p>Maybe this provided a fresh perspective for why PowerShell might benefit even those diehard shell scripters of you out there and maybe help convince you to take the plunge and give it a shot.</p>","tags":["development","cool-tools","golang","automation"]},{"location":"go-r1-day-14/","title":"Go R1 Day 14","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/#day-14-of-100","title":"Day 14 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/#progress","title":"progress","text":"<ul> <li>Migrated my new aws lambda logger from zap to zerolog. Zap gave me some problems initially so zerolog is my favorite structured logger right now, much simpler.</li> <li>Constructed <code>go-task</code> runner file for launching go test and go build/run.</li> <li>Structured logging required a little bit of refactor but worked.</li> </ul> <p>Here's an example of providing back a logged string (don't log secrets normally, but I'm in testing phase) with structure.</p> <pre><code>    log.Debug().\n        Str(\"decodedBinarySecret\", decodedBinarySecret).\n        Str(\"secretString\", secretString).\n        Msg(\"Depending on whether the secret is a string or binary, one of these fields will be populated.\")\n</code></pre> <p>Based on my improved understanding of conversions vs type assertions, the need to convert using a \"cast\" (Go calls these conversions, and yes it makes a copy in memory for this):</p> <pre><code>log.Info().Str(\"requestDump\", string(requestDump)).Msg(\"request information\")\n</code></pre> <p>Type assertions are done when working with an interface. I'm still working on my understanding of interfaces as they are their own beast in Go. Unlike most other languages, a Go type implements an interface when all the required methods are matched. This provides a great deal of the flexibility in Go interfaces.</p> <p>The scoping of the interfaces is important, and while I listened to a lecture on this, I didn't yet work through the interface design principles to ensure the best resusability/narrowness of scope concepts. I think that's going to take more \"getting my hands dirty\" for it to click.</p>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/#links","title":"links","text":"<ul> <li>zerolog</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/","title":"Go R1 Day 14","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/#day-14-of-100","title":"Day 14 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-14/#progress","title":"progress","text":"<ul> <li>built golang function with aws-sdk that returned ec2 instances, images</li> <li>Joined the data together together to search for matching image from the ec2 metadata</li> <li>generated json report from results and final struct</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-15/","title":"Go R1 Day 15","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-15/#day-15-of-100","title":"Day 15 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-15/#progress","title":"progress","text":"<ul> <li>figured out scope issues with pointer and struct</li> <li>Used <code>omitempty</code> in struct</li> <li>exported final report in json format after searching for matching image id from ec2 instance image id</li> <li>Find it interesting how much more wordy the go search method was, but appreciate it in a way as the \"syntactic\" sugar that's missing also is the reason there is more complication at times in languages like PowerShell/C#.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"filtering-results-in-go/","title":"Filtering Results in Go","text":"","tags":["development","golang","tech"]},{"location":"filtering-results-in-go/#where-filtering","title":"Where Filtering","text":"<p>I explored a bit on filtering objects with Go for the AWS SDK v1.</p> <p>Coming from PowerShell, I'm used to a lot of one-liners that do a lot of magic behind the scenes, so Go is both refreshing in it's clarity, and a bit verbose at times since the language features are more sparse.</p> <p>In PowerShell, you can filter results with a variety of methods, including examples like below (but not limited to)</p> <ul> <li><code>$collection.Where{$_.Value -eq 'foo'}</code></li> <li><code>$collection | Where-Object {$_.Value -eq 'foo'}</code></li> <li><code>$collection | Where-Object Value -eq 'foo'</code></li> </ul> <p>When exploring the an unmarshalled result in Go, I found it a bit verbose compared to what I was used to, and wondered if there are no \"Where\" clause helper libraries that cut down on this verbosity, and also still considered idiomatic (Go's favorite word \ud83d\ude03).</p>","tags":["development","golang","tech"]},{"location":"filtering-results-in-go/#scenario","title":"Scenario","text":"<p>Let's get all the EC2 Image results for a region and match these with all the EC2 instances running. Filter down the results of this to only the ami matching what the EC2 instance is using.</p> <p>In PowerShell this might look like <code>$AmiId = $Images.Where{$_.ImageId -eq $Search}.ImageId</code>.</p> <p>As a newer gopher, this is what I ended up doing,and wondering at my solution. This is without sorting optimization.</p> <pre><code>amiCreateDate, ImageName, err := GetMatchingImage(resp.Images, inst.ImageId)\nif err != nil {\n  log.Err(err).Msg(\"failure to find ami\")\n}\n</code></pre> <p>Then I created a search function to iterate through the images for a match. Yes, there was a lot of logging as I worked through this.</p> <pre><code>// GetMatchingImage will search the ami results for a matching id\nfunc GetMatchingImage(imgs []*ec2.Image, search *string) (parsedTime time.Time, imageName string, err error) {\n    layout := time.RFC3339 //\"2006-01-02T15:04:05.000Z\"\n    log.Debug().Msgf(\"searching for: %s\", *search)\n    // Look up the matching image\n    for _, i := range imgs {\n        log.Trace().Msgf(\"\\t %s &lt;--&gt; %s\", *i.ImageId, *search)\n        if strings.ToLower(*i.ImageId) == strings.ToLower(*search) {\n            log.Trace().Msgf(\"\\t %s == %s\", *i.ImageId, *search)\n\n            p, err := time.Parse(layout, *i.CreationDate)\n            if err != nil {\n                log.Err(err).Msg(\"failed to parse date from image i.CreationDate\")\n            }\n            log.Debug().Str(\"i.CreationDate\", *i.CreationDate).Str(\"parsedTime\", p.String()).Msg(\"ami-create-date result\")\n            return p, *i.Name, nil\n            // break\n        }\n    }\n    return parsedTime, \"\", errors.New(\"no matching ami found\")\n}\n</code></pre> <p>I was impressed with the performance without any optimization of the api calls, and could see that with a proper approach to sorting the image ids I could improve the performance further. However, the verbosity of doing some filtering to find and return the object was surprising, so I wrote this up to get feedback from other gophers and see what other idiomatic approaches are a solid way to filter down matching properties from result set.</p> <p>Is there any library used by many to do this type of filtering, or is my .NET background coloring my perspective with dreams of Linq?</p>","tags":["development","golang","tech"]},{"location":"go-r1-day-16/","title":"Go R1 Day 16","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-16/#day-16-of-100","title":"Day 16 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-16/#progress","title":"progress","text":"<ul> <li>refactored AWS SDK call to export a named file using flags.</li> <li>Iterated through regions so cli call aggregated all results from all regions into single JSON.</li> <li>Working with v1 makes me want v2 so much more. The level of pointers required is ridiculous. At one point I had something like <code>&amp;*ec2</code> due to the SDK requirements. Having to write a filter with: <code>Filters: { Name: aws.String(\"foo\")}</code> is so clunky. I believe in v2 this is greatly simplified, and the code is much cleaner.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-16/#links","title":"links","text":"<ul> <li>DescribeRegionsInput</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-17/","title":"Go R1 Day 17","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-17/#day-17-of-100","title":"Day 17 of 100","text":"","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"go-r1-day-17/#progress","title":"progress","text":"<ul> <li>reviewed adding excel conversion to cli</li> <li>shelved this after reviewing implementation requirements</li> <li>this is one of those cases where PowerShell makes much more sense for adhoc work as converts <code>pscustomobject</code> (similar to struct) via pipeline automatically to excel sheet.</li> </ul>","tags":["tech","development","100DaysOfCode","microblog","golang"]},{"location":"leave-me-alone/","title":"Leave Me Alone","text":"","tags":["tech","cool-tools","microblog","noise"]},{"location":"leave-me-alone/#free-means-you-are-the-product","title":"Free Means You Are the Product","text":"<p>Over time, I've begun to look at products that are free with more judgment. The saying is: \"If it's free, you are the product\". This often means your data and privacy are compromised as the product.</p> <p>This has resulted in me looking more favorably at apps I would have dismissed in the past, such as Leave Me Alone.</p>","tags":["tech","cool-tools","microblog","noise"]},{"location":"leave-me-alone/#leave-me-alone","title":"Leave Me Alone","text":"<p>The notion of buying credits for something I could script, click, or do myself made me use sporadically last year. This year, I took the plunge and spent $10 and appreciate the concept and cost.</p> <p>If you have a lot of tech interaction, you'll have a slew of newsletter and marketing subscriptions coming your way. This noise can drown your email.</p> <p>I saw one Children's clothing place that got my email on a receipt generate an average of 64 emails a month!</p> <p>Leave Me Alone helps simplify the cleanup process by simplifying the summary of noisiest offenders, and one-click unsubscribes to any of these.</p> <p>You can use an automatically generated rating based on ranked value on mailing lists, read engagement, number of emails sent monthly, and more.</p> <p>Take a look, the free start is enough to figure out if you like it.</p>","tags":["tech","cool-tools","microblog","noise"]},{"location":"leave-me-alone/#other-tools","title":"Other Tools","text":"<p>Combine this type of tool with:</p> <ul> <li>Kill The Newsletter</li> <li>Inoreader (RSS Reader)</li> <li>Subscription Score: a really promising tool made by the same folks, but haven't added at this time as price seems a bit high for this specific feature if I'm already using their app. (at this time $49 a year). Be nice if this was a feature provided automatically to those who bought 250 credits or more since it's powered by the data mining of lists users unsubscribe from the most.</li> </ul> <p>You'll be more likely to keep up to date with this noise reduced. Last tip: Add GitHub Release notes like Terraform and others as a subscription in your RSS reader, and it might reduce the noise via email and slack on releases.</p>","tags":["tech","cool-tools","microblog","noise"]},{"location":"five/","title":"Five","text":"<p>I asked my daughter (3) how much she loved me. She held up her hands and said: \"Five\".</p> <p>I'll take that as a win considering that's all the fingers on that hand. \ud83d\ude02</p>","tags":["ramblings","dadlife","microblog"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/","title":"Using AWS SDK With Go for EC2 AMI Metrics","text":"","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#source","title":"Source","text":"<p>The source code for this repo is located here:</p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#what-this-is","title":"What This Is","text":"<p>This is a quick overview of some AWS SDK Go work, but not a detailed tutorial. I'd love feedback from more experienced Go devs as well.</p> <p>Feel free to submit a PR with tweaks or suggestions, or just comment at the bottom (which is a GitHub issue powered comment system anyway).</p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#image-age","title":"Image Age","text":"<p>Good metrics can help drive change. If you identify metrics that help you quantify areas of progress in your DevOps process, you'll have a chance to show the progress made and chart the wins.</p> <p>Knowing the age of the image underlying your instances could be useful if you wanted to measure how often instances were being built and rebuilt.</p> <p>I'm a big fan of making instances as immutable as possible, with less reliance on changes applied by configuration management and build oriented pipelines, and more baked into the image itself.</p> <p>Even if you don't build everything into your image and are just doing \"golden images\", you'll still benefit from seeing the average age of images used go down. This would represent more continual rebuilds of your infrastructure. Containerization removes a lot of these concerns, but not everyone is in a place to go straight to containerization for all deployments yet.</p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#what-using-the-sdk-covers","title":"What Using the SDK Covers","text":"<p>I decided this would be a good chance to use Go as the task is relatively simple and I already know how I'd accomplish this in PowerShell.</p> <p>If you are also on this journey, maybe you'll find this detail inspiring to help you get some practical application in Go.</p> <p>There are a few steps that would be required:</p> <ol> <li>Connection &amp; Authorization</li> <li>Obtain a List of Images<ol> <li>Filtering required</li> </ol> </li> <li>Obtain List of Instances</li> <li>Match Images to Instances where possible</li> <li>Produce artifact in file form</li> </ol> <p>Warning... I discovered that the SDK is pretty noisy and probably makes things a bit tougher than just plain idiomatic Go.</p> <p>If you want to learn pointers and derefrencing with Go... you'll be a pro by the time you are done with it \ud83d\ude02</p> <p></p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#why-this-could-be-useful-in-learning-more-go","title":"Why This Could Be Useful In Learning More Go","text":"<p>I think this is a pretty great small metric oriented collector focus as it ties in with several areas worth future versions.</p> <p>Since the overall logic is simple there's less need to focus on understanding AWS and more on leveraging different Go features.</p> <ol> <li>Version 1: MVP that just products a JSON artifact</li> <li>Version 2: Wrap up in lambda collector and product s3 artifact</li> <li>Version 3: Persist metrics to Cloudwatch instead as a metric</li> <li>Version 4: Datadog or Telegraf plugin</li> </ol> <p>From the initial iteration I'll post, there's quite a bit of room for even basic improvement that my quick and dirty solution didn't implement.</p> <ol> <li>Use channels to run parallel sessions to collect multi-region metrics in less time</li> <li>Use sorting with the structs properly would probably cut down on overhead and execution time dramatically.</li> <li>Timeseries metrics output for Cloudwatch, Datadog, or Telegraf</li> </ol>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#caveat","title":"Caveat","text":"<ol> <li>Still learning Go. Posting this up and welcome any pull requests or comments (comments will open GitHubub issue automatically).</li> <li>There is no proper isolation of functions and tests applied. I've determined it's better to produce and get some volume under my belt that focus on immediately making everything best practices. Once I've gotten more familiar with Go proper structure, removing logic from <code>main()</code> and more will be important.</li> <li>This is not a complete walkthrough of all concepts, more a few things I found interesting along the way.</li> </ol>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#some-observations-notes-on-v1-attempt","title":"Some Observations &amp; Notes On V1 Attempt","text":"","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#omitempty","title":"omitempty","text":"<p>Writing to JSON is pretty straight forward, but what I found interesting was handling null values.</p> <p>If you don't want the default initialized value from the data type to be populated, then you need to specific additional attributes in your struct to let it know how to properly serialize the data.</p> <p>For instance, I didn't want to populate a null value for <code>AmiAge</code> as <code>0</code> would mess up any averages you were trying to calculate.</p> <pre><code>type ReportAmiAging struct {\n    Region             string     `json:\"region\"`\n    InstanceID         string     `json:\"instance-id\"`\n    AmiID              string     `json:\"image-id\"`\n    ImageName          *string    `json:\"image-name,omitempty\"`\n    PlatformDetails    *string    `json:\"platform-details,omitempty\"`\n    InstanceCreateDate *time.Time `json:\"instance-create-date\"`\n    AmiCreateDate      *time.Time `json:\"ami-create-date,omitempty\"`\n    AmiAgeDays         *int       `json:\"ami-age-days,omitempty\"`\n}\n</code></pre> <p>In this case, I just set <code>omitempty</code> and it would set to null if I passed in a pointer to the value. For a much more detailed walk-through of this:  Go's Emit Empty Explained</p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#multi-region","title":"Multi-Region","text":"<p>Here things got a little confusing as I wanted to run this concurrently, but shelved that for v1 to deliver results more quickly.</p> <p>To initialize a new session, I provided my starting point.</p> <pre><code>sess, err := session.NewSession(&amp;aws.Config{\n        Region: aws.String(\"eu-west-1\"),\n        },\n)\nif err != nil {\n    log.Err(err)\n}\nlog.Info().Str(\"region\", string(*sess.Config.Region)).Msg(\"initialized new session successfully\")\n</code></pre> <p>Next, I had to gather all the regions. In my scenario, I wanted to add flexibility to ignore regions that were not opted into, to allow less regions to be covered when this setting was correctly used in AWS.</p> <pre><code>// Create EC2 service client\nclient := ec2.New(sess)\nregions, err := client.DescribeRegions(&amp;ec2.DescribeRegionsInput{\n    AllRegions: aws.Bool(true), Filters: []*ec2.Filter{\n        {\n            Name:   aws.String(\"opt-in-status\"),\n            Values: []*string{aws.String(\"opted-in\"), aws.String(\"opt-in-not-required\")},\n        },\n    },\n},\n                                      )\nif err != nil {\n    log.Err(err).Msg(\"Failed to parse regions\")\n    os.Exit(1)\n}\n</code></pre> <p>The filter syntax is pretty ugly. Due to the way the SDK works, you can't just pass in <code>*[]string{\"opted-in\",\"opt-in-not-required}</code> and then reference this. Instead, you have to set the AWS functions to create pointers to the strings and then dereference. Deep diving into this further was beyond my time allotted, but made my first usage feel somewhat clunky.</p> <p>After gathering the regions you'd iterate and create a new session per region similar to this.</p> <pre><code>for _, region := range regions.Regions {\n        log.Info().Str(\"region\", *region.RegionName).Msg(\"--&gt; processing region\")\n        client := ec2.New(sess, &amp;aws.Config{Region: *&amp;region.RegionName})\n    // Do your magic\n}\n</code></pre>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#structured-logging","title":"Structured Logging","text":"<p>I've blogged about this before (mostly on microblog).</p> <p>As a newer gopher, I've found that zerolog is pretty intuitive.</p> <p>Structured logging is really important to being able to use log tools and get more value out of your logs in the future, so I personally like the idea of starting with them from the beginning.</p> <p>Here you could see how you can provide name value pairs, along with the message.</p> <pre><code>log.Info().Int(\"result_count\", len(respInstances.Reservations)).Dur(\"duration\", time.Since(start)).Msg(\"\\tresults returned for ec2instances\")\n</code></pre> <p>Using this provided some nice readable console feedback, along with values that a tool like Datadog's log parser could turn into values you could easily make metrics from.</p>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#performance-in-searching","title":"Performance In Searching","text":"<p>From my prior blog post Filtering Results In Go I also talked about this.</p> <p>The lack of syntactic sugar in Go means this seemed much more verbose than I was expecting.</p> <p>A few key things I observed here were:</p> <ol> <li>Important to set your default layout for time if you want any consistency.</li> <li>Sorting algorithms, or even just basic sorting, would likely reduce the overall cost of a search like this (I'm better pretty dramatically)</li> <li>Pointers. Everywhere. Coming from a dynamic scripting language like PowerShell/Python, this is a different paradigm. I'm used to isolated functions which have less focus on passing values to modify directly (by value). In .NET you can pass in variables by reference, which is similar in concept, but it's not something I found a lot of use for in scripting. I can see the massive benefits when at scale though, as avoiding more memory grants by using existing memory allocations with pointers would be much more efficient. Just have to get used to it!</li> </ol> <pre><code>// GetMatchingImage will search the ami results for a matching id\nfunc GetMatchingImage(imgs []*ec2.Image, search *string) (parsedTime time.Time, imageName string, platformDetails string, err error) {\n    layout := time.RFC3339 //\"2006-01-02T15:04:05.000Z\"\n    log.Debug().Msgf(\"\\t\\t\\tsearching for: %s\", *search)\n    // Look up the matching image\n    for _, i := range imgs {\n        log.Trace().Msgf(\"\\t\\t\\t%s &lt;--&gt; %s\", *i.ImageId, *search)\n        if strings.ToLower(*i.ImageId) == strings.ToLower(*search) {\n            log.Trace().Msgf(\"\\t\\t\\t %s == %s\", *i.ImageId, *search)\n\n            p, err := time.Parse(layout, *i.CreationDate)\n            if err != nil {\n                log.Err(err).Msg(\"\\t\\t\\tfailed to parse date from image i.CreationDate\")\n            }\n            log.Debug().Str(\"i.CreationDate\", *i.CreationDate).Str(\"parsedTime\", p.String()).Msg(\"\\t\\t\\tami-create-date result\")\n            return p, *i.Name, *i.PlatformDetails, nil\n            // break\n        }\n    }\n    return parsedTime, \"\", \"\", errors.New(\"\\t\\t\\tno matching ami found\")\n}\n</code></pre>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#multiple-return-properties","title":"Multiple Return Properties","text":"<p>While this can be done in PowerShell, I rarely did it in the manner Go does.</p> <pre><code>amiCreateDate, ImageName, platformDetails, err := GetMatchingImage(respPrivateImages.Images, inst.ImageId)\nif err != nil {\n    log.Err(err).Msg(\"failure to find ami\")\n}\n</code></pre>","tags":["tech","development","aws","golang","metrics"]},{"location":"using-aws-sdk-with-go-for-ec2-ami-metrics/#feedback-welcome","title":"Feedback Welcome","text":"<p>As stated, feedback welcome from any more experienced Gophers would be welcome. Anything for round 2.</p> <p>Goals for that will be at a minimum:</p> <ol> <li>Use <code>go test</code>  to run.</li> <li>Isolate main and build basic tests for each function.</li> <li>Decide to wrap up in lambda or plugin.</li> </ol>","tags":["tech","development","aws","golang","metrics"]},{"location":"sql-server-meets-aws-systems-manager/","title":"SQL Server Meets AWS Systems Manager","text":"<p>Excited. Have a new solution in the works to deploy Ola Hallengren via SSM Automation runbook across all SQL Server instances with full scheduling and synchronization to S3. Hoping to get the ok to publish this soon, as I haven't seen anything like this built.</p> <p>Includes:</p> <ul> <li>Building SSM Automation YAML doc from a PS1 file using AST &amp; metadata</li> <li>Download dependencies from s3 automatically</li> <li>Credentials pulled automatically via AWS Parameter Store (could be adapted to Secrets Manager as well)</li> <li>Leverage s5cmd for roughly 40x faster sync performance with no <code>aws-cli</code> required. It's a Go executable. #ilovegolang</li> <li>Deployment of a job that automates flipping instances to <code>FULL</code> or <code>SIMPLE</code> recovery similar to how RDS does this, for those cases where you can't control the creation scripts and want to flip SIMPLE to full for immediate backups.</li> <li>Formatted deployment summary card sent with all properties to Microsoft Teams. #imissslack</li> <li>Management of these docs via terraform.</li> <li>Snippet for the setup of an S3 lifecycle policy automatically cleanup old backups. (prefer terraform, but this is still good to know for retro-active fixes)</li> </ul> <p>I'm pretty proud of this being done, as it is replacing Cloudberry, which has a lot of trouble at scale in my experience. I've seen a lot of issues with Cloudberry when dealing with 1000-3000 databases on a server.</p> <p>Once I get things running, I'll see if I can get this shared in full since it's dbatools + Ola Hallengren Backup Solution driven.</p> <p>Also plan on adding a few things like on failure send a PagerDuty incident and other little enhancements to possible enable better response handling.</p>","tags":["tech","development","microblog","sql-server","site-reliability-engineering"]},{"location":"sql-server-meets-aws-systems-manager/#other-resources","title":"Other Resources","text":"<ul> <li>dbatools</li> <li>Ola Hallengren</li> <li>AWS Docs on Automation Runbooks</li> <li>s5cmd</li> <li>AWS Quick Start for Microsoft SQL Server</li> </ul>","tags":["tech","development","microblog","sql-server","site-reliability-engineering"]},{"location":"go-r1-day-18/","title":"Go R1 Day 18","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-18/#day-18-of-100","title":"Day 18 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-18/#progress","title":"progress","text":"<p>Hackerrank challenge \"Sales by Match\"</p> <p>{{&lt; gist sheldonhull  \"f7671b1d78705c329fe25b3c4af7c3ec\" &gt;}}</p>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-19/","title":"Go R1 Day 19","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-19/#day-19-of-100","title":"Day 19 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-19/#progress","title":"progress","text":"<p>Hackerrank challenge \"Counting Valleys\"</p> <p>{{&lt; gist sheldonhull  \"b1b7e583b927997e79aee13bb7b51a4e\" &gt;}}</p>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-20/","title":"Go R1 Day 20","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-20/#day-20-of-100","title":"Day 20 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-20/#progress","title":"progress","text":"<p>Hackerrank challenge \"Bubble Sort\"</p> <p>{{&lt; gist sheldonhull  \"4aae8e98dd228530d97ce7c6e8208444\" &gt;}}</p>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-22/","title":"Go R1 Day 22","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-22/#day-22-of-100","title":"Day 22 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-22/#progress","title":"progress","text":"<p>Using Dash, I read through much of the language specification. Dry reading for sure, but helped a bit in understanding a little more on stuff like arrays, slices, loops, etc.</p> <p>Nothing profound to add, except to say I don't think I want to write a language specification.</p>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"experiments-with-go-arrays-and-slices/","title":"Experiments With Go Arrays and Slices","text":"","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#simplicity-over-syntactic-sugar","title":"Simplicity Over Syntactic Sugar","text":"<p>As I've been learning Go, I've grown to learn that many decisions to simplify the language have removed many features that provide more succinct expressions in languages such as Python, PowerShell, C#, and others. The non-orthogonal features in the languages result in many expressive ways something can be done, but at a cost, according to Go's paradigm.</p> <p>My background is also heavily focused in relational databases and set based work, so I'm realizing as I study more programming paradigms seperate from any database involvement, that it's a fundamental difference in the way a database developer and a normal developer writing backend code look at this. Rather than declarative based syntax, you need to focus a lot more on iterating through collections and manipulating these.</p> <p>As I explored my assumptions, I found that even in .NET Linq expressions are abstracting the same basic concept of loops and iterations away for simpler syntax, but not fundamentally doing true set selections. In fact, in some cases I've read that Linq performance is often worse than a simple loop (see this interesting stack overflow answer) The catch to this is that the Linq expression might be more maintainable in an enterprise environment at the cost of some degraded performance (excluding some scenarios like deferred execution).</p> <p>For example, in PowerShell, you can work with arrays in a multitude of ways.</p> <pre><code>$array[4..10] | ForEach-Object {}\n# or\nforeach($item in $array[$start..$end]){}\n</code></pre> <p>This syntactic sugar provides brevity, but as two ways among many I can think of this does add such a variety of ways and performance considerations. Go strips this cognitive load away by giving only a few ways to do the same thing.</p>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#using-for-loop","title":"Using For Loop","text":"<p>This example is just int slices, but I'm trying to understand the options as I <code>range</code> through a struct as well.</p> <p>When working through these examples for this question, I discovered thanks to the Rubber Duck debugging, that you can simplify slice selection using <code>newSlice := arr[2:5]</code>.</p>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#simple-loop","title":"Simple Loop","text":"<p>As an example: Goplay Link To Run</p> <pre><code>package main\nimport \"fmt\"\n\nfunc main() {\n    startIndex := 2\n    itemsToSelect := 3\n    arr := []int{10, 15, 20, 25, 35, 45, 50}\n    fmt.Printf(\"starting: arr: %v\\n\", arr)\n\n    newCollection := []int{}\n    fmt.Printf(\"initialized newCollection: %v\\n\", newCollection)\n    for i := 0; i &lt; itemsToSelect; i++ {\n        newCollection = append(newCollection, arr[i+startIndex])\n        fmt.Printf(\"\\tnewCollection: %v\\n\", newCollection)\n    }\n    fmt.Printf(\"= newCollection: %v\\n\", newCollection)\n    fmt.Print(\"expected: 20, 25, 35\\n\")\n}```\n\nThis would result in:\n\n```text\nstarting: arr: [10 15 20 25 35 45 50]\ninitialized newCollection: []\n    newCollection: [20]\n    newCollection: [20 25]\n    newCollection: [20 25 35]\n= newCollection: [20 25 35]\nexpected: 20, 25, 35\n</code></pre>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#moving-loop-to-a-function","title":"Moving Loop to a Function","text":"<p>Assuming there are no more effective selection libraries in Go, I'm assuming I'd write functions for this behavior such as Goplay Link To Run.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    startIndex := 2\n    itemsToSelect := 3\n    arr := []int{10, 15, 20, 25, 35, 45, 50}\n    fmt.Printf(\"starting: arr: %v\\n\", arr)\n    newCollection := GetSubselection(arr, startIndex, itemsToSelect)\n    fmt.Printf(\"GetSubselection returned: %v\\n\", newCollection)\n    fmt.Print(\"expected: 20, 25, 35\\n\")\n}\n\nfunc GetSubselection(arr []int, startIndex int, itemsToSelect int) (newSlice []int) {\n    fmt.Printf(\"newSlice: %v\\n\", newSlice)\n    for i := 0; i &lt; itemsToSelect; i++ {\n        newSlice = append(newSlice, arr[i+startIndex])\n        fmt.Printf(\"\\tnewSlice: %v\\n\", newSlice)\n    }\n    fmt.Printf(\"= newSlice: %v\\n\", newSlice)\n    return newSlice\n}\n</code></pre> <p>which results in:</p> <pre><code>starting: arr: [10 15 20 25 35 45 50]\nnewSlice: []\n    newSlice: [20]\n    newSlice: [20 25]\n    newSlice: [20 25 35]\n= newSlice: [20 25 35]\nGetSubselection returned: [20 25 35]\nexpected: 20, 25, 35\n</code></pre> <p>Trimming this down further I found I could use the slice syntax (assuming the consecutive range of values) such as: Goplay Link To Run</p> <pre><code>func GetSubselection(arr []int, startIndex int, itemsToSelect int) (newSlice []int) {\n    fmt.Printf(\"newSlice: %v\\n\", newSlice)\n    newSlice = arr[startIndex:(startIndex + itemsToSelect)]\n    fmt.Printf(\"\\tnewSlice: %v\\n\", newSlice)\n    fmt.Printf(\"= newSlice: %v\\n\", newSlice)\n    return newSlice\n}\n</code></pre>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#range","title":"Range","text":"<p>The <code>range</code> expression gives you both the index and value, and it works for maps and structs as well.</p> <p>Turns outs you can also work with a subselection of a slice in the <code>range</code> expression.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    startIndex := 2\n    itemsToSelect := 3\n    arr := []int{10, 15, 20, 25, 35, 45, 50}\n    fmt.Printf(\"starting: arr: %v\\n\", arr)\n\n    fmt.Printf(\"Use range to iterate through arr[%d:(%d + %d)]\\n\", startIndex, startIndex, itemsToSelect)\n    for i, v := range arr[startIndex:(startIndex + itemsToSelect)] {\n        fmt.Printf(\"\\ti: %d v: %d\\n\", i, v)\n    }\n    fmt.Print(\"expected: 20, 25, 35\\n\")\n}\n</code></pre>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#slices","title":"Slices","text":"<p>While the language is simple, understanding some behaviors with slices caught me off-guard.</p> <p>First, I needed to clarify my language. Since I was looking to have a subset of an array, slices were the correct choice. For a fixed set with no changes, a standard array would be used.</p> <p>Tour On Go says it well with:</p> <p>An array has a fixed size. A slice, on the other hand, is a dynamically-sized, flexible view into the elements of an array. In practice, slices are much more common than arrays.</p> <p>For instance, I tried to think of what I would do to scale performance on a larger array, so I used a pointer to my int array. However, I was using a <code>slice</code>.</p> <p>This means that using a pointer wasn't valid. This is because whenever I pass the slice it is a pass by reference already, unlike many of the other types.</p> <pre><code>newCollection := GetSubSelection(&amp;arr,2,3)\n\nfunc GetSubSelection(arr *[]int){ ...\n</code></pre> <p>I think some of these behaviors aren't quite intuitive to a new Gopher, but writing them out helped clarify the behavior a little more.</p>","tags":["powershell","tech","golang","development"]},{"location":"experiments-with-go-arrays-and-slices/#resources","title":"Resources","text":"<p>This is a bit of a rambling about what I learned so I could solidify some of these discoveries by writing them down. #learninpublic</p> <p>For some great examples, look at some examples in:</p> <ul> <li>A Tour Of Go - Slices</li> <li>Go By Example</li> <li>Prettyslice GitHub Repo</li> </ul> <p>If you have any insights, feel free to drop a comment here (it's just a GitHub powered comment system, no new account required).</p>","tags":["powershell","tech","golang","development"]},{"location":"go-r1-day-21/","title":"Go R1 Day 21","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-21/#day-21-of-100","title":"Day 21 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-21/#progress","title":"progress","text":"<ul> <li>Signed up for exercism.io, which is a pretty great website to work through progressively harder exercises.</li> <li>Did Hello world to start with as requires progressive steps through the exercises.</li> <li>Did a string concatenation exercise as well (Two Fer).</li> </ul> <p>I like the mentor feedback system concept and submission of work. After I finish this, would be good to add myself as a mentor and contribute back to this community. This is a fantastic concept to help get acclimated to a new language and do progressively harder exercises to better learn the language usage.</p>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-21/#links","title":"links","text":"<ul> <li>Two Fer</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog","puzzles","algorithms"]},{"location":"go-r1-day-23/","title":"Go R1 Day 23","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-23/#day-23-of-100","title":"Day 23 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-23/#progress","title":"progress","text":"<ul> <li>Used Viper to load configuration for a CLI tool.</li> <li>Ran into problems with trying to print out <code>map[string]interface{}</code> type. Lots of magic performed by .NET to do this type of action. This is a lot more hands-on \ud83d\ude01.</li> <li>Had stability issues with VSCode today, so I finally switched over to Intellij with the Go plugin and it worked well. The keyboard mappings are different so that was painful, but still overall a good experience that got me unblocked.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-23/#links","title":"links","text":"<ul> <li>Viper</li> <li>Tablewriter</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-24/","title":"Go R1 Day 24","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-24/#day-24-of-100","title":"Day 24 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-24/#progress","title":"progress","text":"<ul> <li>Used glamour to render some nice console output.</li> <li>Formatted text testing some text tooling.</li> <li>Progressbar ui tested.</li> <li>Connected successfully to MSSQL Docker instance and returned results of query, rendering to to console .</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-24/#links","title":"links","text":"<ul> <li>Glamour</li> <li>go-mssqldb</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-25/","title":"Go R1 Day 25","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-25/#day-25-of-100","title":"Day 25 of 100","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-25/#progress","title":"progress","text":"<ul> <li>Worked with Gorm, my first experience with ORM based querying.</li> <li>Used <code>gofakeit</code> with gorm to connect to MSSQL Server in Docker and randomize name entries.</li> <li>Learned a bit about <code>Context</code> package and how it helps with propagation of cancellation.</li> <li>As a new user to this, the examples were very confusing as it mixed up go-mssqldb along with examples using just gorm. As I get better with it, I'll try to log better introductory examples.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-25/#links","title":"links","text":"<ul> <li>Gorm</li> <li>Gofakeit</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"remove-chrome-autocomplete-suggestion/","title":"Remove Chrome Autocomplete Suggestion","text":"<p>Do you have a Chrome suggestion for a domain, url, or specific site location that you don't want anymore?</p> <p>You can remove an invalid suggestion that you don't want popping up each time by using <code>shift+delete</code>.</p> <p>If it's the first entry it will remove it, or use arrow keys to highlight a different entry and press the same command.</p> <p>Also relevant: To Delete Chrome Address Bar Suggestions with Mouse</p>","tags":["tech","microblog","poweruser"]},{"location":"leverage-renovate-for-easy-dependency-updates/","title":"Leverage Renovate for Easy Dependency Updates","text":"<p>{{&lt; admonition type=\"Note\" title=\"Update 2021-06-30\" open=\"true\"&gt;}}</p> <p>Added example from renovate documentation with some notes on the Azure DevOps Pipeline to leverage their free renovate service. GitHub users benefit from the Renovate app, but Azure Pipelines should use an Azure Pipeline definition.</p> <p>Follow the instructions from the <code>Renovate Me</code> task linked in resources, and ensure the appropriate rights are granted for the build service to manage branches and pull requests.</p> <p>{{&lt; /admonition &gt;}}</p> <p>Renovate is a great tool to know about. For Go, you can keep modules updated automatically, but still leverage a pull request review process to allow automated checks to run before allowing the update.</p> <p>This is particularly useful with Terraform dependencies, which I consider notoriously difficult to keep updated. Instead of needing to use ranges for modules, you can start specifying exact versions and this GitHub app will automatically check for updates periodically and submit version bumps.</p> <p>Why? You can have a Terraform plan previewed and checked for any errors on a new version update with no work. This means your blast radius on updates would be reduced as you are staying up to date and previewing each update as it's available.</p> <p>No more 5 months of updates and figuring out what went wrong \ud83d\ude01</p> <p>Here's an example json config that shows how to allow automerging, while respecting minor/major version updates not enabling automerge.</p> <p>Note that you'd want to install the auto-approver app they document in the marketplace if you have pull request reviews required.</p> <p>In addition, if you use <code>CODEOWNERS</code> file, this will still block automerge. Consider removing that if you aren't really leveraging it.</p>","tags":["tech","development","microblog","terraform","devops"]},{"location":"leverage-renovate-for-easy-dependency-updates/#resources","title":"Resources","text":"<ul> <li>Renovate Me Azure DevOps Task</li> </ul>","tags":["tech","development","microblog","terraform","devops"]},{"location":"create-an-s3-lifecycle-policy-with-powershell/","title":"Create an S3 Lifecycle Policy with PowerShell","text":"<p>First, I'm a big believer in doing infrastructure as code.</p> <p>Using the AWS SDK with any library is great, but for things like S3 I'd highly recommend you use a Terraform module such as Cloudposse terraform-aws-s3-bucket module. Everything Cloudposse produces has great quality, flexibility with naming conventions, and more.</p> <p>Now that this disclaimer is out of the way, I've run into scenarios where you can have a bucket with a large amount of data such as databases which would be good to do some cleanup on before you migrate to newly managed backups.</p> <p>In my case, I've run into 50TB of old backups due to tooling issues that prevented cleanup from being successful. The backup tooling stored a sqlite database in one subdirectory and in another directory the actual backups.</p> <p>I preferred at this point to only perform the lifecycle cleanup on the backup files, while leaving the sqlite file alone. (side note: i always feel strange typing sqlite, like I'm skipping an l \ud83d\ude01).</p> <p>Here's an example of how to do this from the AWS PowerShell docs.</p> <p>I modified this example to support providing multiple key prefixes. What wasn't quite clear when I did this the need to create the entire lifecycle policy collection as a single object and pass this to the command.</p> <p>If you try to run a loop and create one lifecycle policy for each <code>Write-S3LifecycleConfiguration</code> command, it only kept what last ran. Instead, ensure you create the entire object as shown in the example, and then you'll be able to have multiple lifecycle policies get attached to your bucket.</p> <p>Good luck!</p>","tags":["tech","development","aws","powershell","devops"]},{"location":"ensuring-profile-environment-variables-available-to-intellij/","title":"Ensuring Profile Environment Variables Available to Intellij","text":"<p>Open IntelliJ via terminal: <code>open \"/Users/$(whoami)/Applications/JetBrains Toolbox/IntelliJ IDEA Ultimate.app\"</code></p> <p>This will ensure your <code>.profile</code>, <code>.bashrc</code>, and other profile settings that might be loading some default environment variables are available to your IDE. For macOS, you'd have to set in the <code>environment.plist</code> otherwise to ensure they are available to a normal application.</p> <p>ref: OSX shell environment variables \u2013 IDEs Support (IntelliJ Platform) | JetBrains</p>","tags":["tech","development","microblog","macOS"]},{"location":"github-pages-now-supports-private-pages/","title":"Github Pages Now Supports Private Pages","text":"<p>I'm a huge static site fan (lookup jamstack).</p> <p>What I've historically had a problem with was hosting. For public pages, it's great.</p> <p>For private internal docs, it's been problematic. It's more servers and access control to manage if you want something for a specific group inside a company to access.</p> <p>This new update is a big deal for those that want to provide an internal hugo, jekyll, mkdocs, or other static generate based documentation site for their team.</p> <p>Access control for GitHub Pages - GitHub Changelog</p>","tags":["tech","development","microblog"]},{"location":"go-r1-day-27/","title":"Go R1 Day 27","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-27/#progress","title":"progress","text":"<ul> <li>Iterated through AWS SDK v1 S3 buckets to process IAM policy permissions.</li> <li>Unmarshaled policy doc into struct using <code>Json-To-Struct</code>.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-27/#links","title":"links","text":"<ul> <li>JSON-to-Go</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"nativefier/","title":"Nativefier","text":"<p>{{&lt; admonition type=\"Info\" title=\"Update 2021-09-20\" open=\"true\"&gt;}} Updated with improved handling using public docker image. {{&lt; /admonition &gt;}} {{&lt; admonition type=\"Info\" title=\"Update 2021-05-10\" open=\"true\"&gt;}} Added additional context for setting <code>internal-urls</code> via command line. {{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"Info\" title=\"Update 2021-05-13\" open=\"true\"&gt;}} Added docker run commands to simplify local build and run without global install. {{&lt; /admonition &gt;}}</p> <p>Ran across this app, and thought was kinda cool. I've had some issues with Chrome apps showing up correctly in certain macOS windows managers to switch context quickly.</p> <p>Using this tool, you can generate a standalone electron app bundle to run a webpage in as it's own dedicated window.</p> <p>It's cross-platform.</p> <p>If you are using an app like Azure Boards that doesn't offer a native app, then this can provide a slightly improved experience over Chrome shortcut apps. You can pin this to your tray and treat it like a native app.</p>","tags":["tech","development","microblog","cool-tools"]},{"location":"nativefier/#docker-setup","title":"Docker Setup","text":"<p>{{&lt; admonition type=\"Note\" title=\"Optional - Build Locally\" open=false &gt;}} This step is no longer required per public docker image.</p> <pre><code>cd ~/git\ngh repo clone nativefier/nativefier\ncd nativefier\ndocker build -t local/nativefier .\n</code></pre> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","microblog","cool-tools"]},{"location":"nativefier/#docker-build","title":"Docker Build","text":"<p>Highly recommend using docker for the build as it was by far the less complicated.</p> <pre><code>docker run --rm -v ~/nativefier-apps:/target/ local/nativefier:latest --help\n\n$MYORG = 'foo'\n$MYPROJECT = 'bar'\n$AppName      = 'myappname'\n$Platform = ''\nswitch -Wildcard ([System.Environment]::OSVersion.Platform)\n{\n    'Win32NT' { $Platform = 'windows' }\n    'Unix'    {\n                if ($PSVersionTable.OS -match 'Darwin')\n                {\n                    $Platform = 'darwin';\n                    $DarkMode = '--darwin-dark-mode-support'\n                }\n                else\n                {\n                    $Platform = 'linux'\n                }\n            }\n    default { Write-Warning 'No match found in switch' }\n}\n$InternalUrls = '(._?contacts\\.google\\.com._?|._?dev.azure.com_?|._?microsoft.com_?|._?login.microsoftonline.com_?|._?azure.com_?|._?vssps.visualstudio.com._?)'\n$Url          = \"https://dev.azure.com/$MYORG/$MYPROJECT/_sprints/directory?fullScreen=true/\"\n\n$HomeDir = \"${ENV:HOME}${ENV:USERPROFILE}\" # cross platform support\n$PublishDirectory = Join-Path \"${ENV:HOME}${ENV:USERPROFILE}\" 'nativefier-apps'\n$PublishAppDirectory = Join-Path $PublishDirectory \"$AppName-$Platform-x64\"\n\nRemove-Item -LiteralPath $PublishAppDirectory -Recurse -Force\ndocker run --rm -v  $HomeDir/nativefier-apps:/target/ nativefier/nativefier:latest --name $AppName --platform $Platform $DarkMode --internal-urls $InternalUrls $Url /target/\n</code></pre>","tags":["tech","development","microblog","cool-tools"]},{"location":"nativefier/#running-the-cli","title":"Running The CLI","text":"<p>For a site like Azure DevOps, you can run:</p> <pre><code>$MYORG = 'foo'\n$MYPROJECT = 'bar'\n$BOARDNAME = 'bored'\nnativefier --name 'board' https://dev.azure.com/$MYORG/$MYPROJECT/_boards/board/t/$BOARDNAME/Backlog%20items/?fullScreen=true ~/$BOARDNAME\n</code></pre> <p>Here's another example using more custom options to enable internal url authentication and setup an app for a sprint board.</p> <pre><code>nativefier --name \"sprint-board\" --darwin-dark-mode-support `\n  --internal-urls '(._?contacts.google.com._?|._?dev.azure.com_?|._?microsoft.com_?|._?login.microsoftonline.com_?|._?azure.com_?|._?vssps.visualstudio.com._?)' `\n  \"https://dev.azure.com/$MYORG/$MYPROJECT/_sprints/directory?fullScreen=true\"\n  ` ~/sprint-board\n</code></pre> <p>If redirects for permissions occur due to external links opening, you might have to open the application bundle and edit the url mapping. GitHub Issue #706 This can be done proactively in the <code>--internal-urls</code> command line argument shown earlier to bypass the need to do this later.</p> <pre><code>/Users/$(whoami)/$BOARDNAME/APP-darwin-x64/$BOARDNAME.app/Contents/Resources/app/nativefier.json\n</code></pre> <p>Ensure your external urls match the redirect paths that you need such as below. I included the standard oauth redirect locations that Google, Azure DevOps, and Microsoft uses. Add your own such as github to this to have those links open inside the app and not in a new window that fails to recieve the postback.</p> <pre><code>\"internalUrls\": \"(._?contacts\\.google\\.com._?|._?dev.azure.com_?|._?microsoft.com_?|._?login.microsoftonline.com_?|._?azure.com_?|._?vssps.visualstudio.com._?)\",\n</code></pre>","tags":["tech","development","microblog","cool-tools"]},{"location":"working-with-powershell-objects-to-create-yaml/","title":"Working With Powershell Objects to Create Yaml","text":"","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#who-this-might-be-for","title":"Who This Might Be For","text":"<ul> <li>PowerShellers wanting to know how to create json and yaml dynamically via <code>pscustomobject</code>.</li> <li>Anyone wanting to create configs like Datadog or other tools dynamically without the benefit of a configuration management tool.</li> <li>Anyone else wanting to fall asleep more quickly. (I can think of better material such as the Go spec docs, but hey, I can't argue with your good taste \ud83d\ude04)</li> </ul>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#yaml","title":"YAML","text":"<p>It's readable.</p> <p>It's probably cost all of us hours when debugging yaml that's nested several layers and an errant whitespace got in.</p> <p>It's here to stay.</p> <p>I prefer it over JSON for readability, but I prefer JSON for programmability.</p> <p>Sometimes though, tooling uses yaml, and we need to be able to flip between both.</p> <p>Historically I've used <code>cfn-flip</code> which is pretty great.</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#enter-yq","title":"Enter yq","text":"<p>The problem I have with using <code>cfn-flip</code> is dependencies. It's a bit crazy to setup a docker image and then need to install a bunch of python setup tools to just get this one tool when it's all I need.</p> <p>I thought about building a quick <code>Go</code> app to do this and give me the benefit of a single binary, as there is a pretty useful <code>yaml</code> package already. Instead, I found a robust package that is cross-platform called <code>yq</code> and it's my new go to. \ud83c\udf89</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#just-plain-works","title":"Just plain works","text":"<p>The docs are great</p> <p>Reading <code>STDIN</code> is a bit clunky, but not too bad, though I wish it would take more of a pipeline input approach natively. Instead of passing in <code>{\"string\":\"value\"} | yq</code> it requires you to specify <code>stringinput | yq eval - --prettyPrint</code> . Note the single hyphen after eval. This is what signifies that the input is <code>STDIN</code>.</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#dynamically-generate-some-configs","title":"Dynamically Generate Some Configs","text":"<p>I was working on some Datadog config generation for SQL Server, and found this tooling useful, especially on older Windows instances that didn't have the capability to run the nice module powershell-yaml.</p> <p>Here's how to use PowerShell objects to help generate a yaml configuration file on demand.</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#install","title":"Install","text":"<p>See install directions for linux/mac, as it's pretty straightforward.</p> <p>For windows, the chocolatey package was outdated as of the time of the article using the version 3.x.</p> <p>I used a PowerShell 4.0 compatible syntax here that should work on any instances with access to the web.</p> <pre><code>[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\nif (-not (Test-Path 'C:\\tools\\yq.exe' -PathType Leaf))\n    {\n        $ProgressPreference = 'SilentlyContinue'\n        New-Item 'C:\\tools' -ItemType Directory -Force\n        Invoke-WebRequest 'https://github.com/mikefarah/yq/releases/download/v4.4.1/yq_windows_amd64.exe' -OutFile 'C:\\tools\\yq.exe' -UseBasicParsing\n        Unblock-File 'C:\\tools\\yq.exe' -Confirm:$false\n    }\n</code></pre> <p>Once this was downloaded, you could either make sure <code>C:\\tools</code> was in <code>PATH</code> or just use the fully qualified path for our simple use case.</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#get-aws-metadata","title":"Get AWS Metadata","text":"<p>In AWS, I parsed the metadata for the AccountID and InstanceID to generate a query to pull the Name tag dynamically.</p> <p>{{&lt; admonition type=\"Tip\" title=\"Permissions Check\" &gt;}} You must have the required permissions for the instance profile for this to work. This is not an instance level permission, so you'll want to add the required DescribeTags and ListInstances permissions for using a command such as <code>Get-EC2Tag</code> {{&lt; /admonition &gt;}}</p> <pre><code>Import-Module AWSPowershell -Verbose:$false *&gt; $null\n# AWSPowerShell is the legacy module, but is provided already on most AWS instances\n$response = Invoke-RestMethod -Uri 'http://169.254.169.254/latest/dynamic/instance-identity/document' -TimeoutSec 5\n$AccountId = $response.AccountId\n</code></pre>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#pull-back-ec2-tags","title":"Pull Back EC2 Tags","text":"<p>Now we can pull back the tag using an EC2 instance filter object.</p> <pre><code>$filters = @(\n      [Amazon.EC2.Model.Filter]::new('resource-id', $response.InstanceId)\n  )\n  $tags = Get-EC2Tag -Filters $filters\n  $tagcollection = $tags.ForEach{\n      $t = $_\n      [pscustomobject]@{\n          Name  = $t.name\n          Value = $t.value\n      }\n  }\n  Write-Host \"Tags For Instance: $($tagcollection | Format-Table -AutoSize -Wrap | Out-String)\"\n  $HostName = $Tags.GetEnumerator().Where{ $_.Key -eq 'Name' }.Value.ToLower().Trim()\n  $SqlInstance = $HostName\n</code></pre>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#switch-things-up-with-a-switch","title":"Switch Things Up With A Switch","text":"<p>The next step was to alias the instance.</p> <p>The better way to do this would be to use a tag that it reads, but for my quick ad-hoc use, this just let me specific an explicit alias to generate as a tag in the yaml. Again, try to use the Datadog tagging feature to do this automatically if possible.</p> <p>{{&lt; admonition type=\"Tip\" title=\"Switch Statements\" &gt;}} If you aren't familiar with PowerShell's switch statement, it's a nice little feature for making this evaluation easy to read.</p> <p>For the breadth of what this cool language feature can do, check this article out:</p> <p>Everything you ever wanted to know about the switch statement {{&lt; /admonition &gt;}}</p> <pre><code>switch ($AccountId)\n{\n    '12345' { $AWSAccountAlias  = 'mydevenv' ; $stage = 'qa' }\n    '12345' { $AWSAccountAlias  = 'myprodenv' ; $stage = 'prod' }\n    default\n    {\n        throw \"Couldn't match a valid account number to give this an alias\"\n    }\n}\n</code></pre> <p>Now, preview the results of this Frankenstein.</p> <pre><code>Write-Host -ForegroundColor Green (\"\n`$HostName        = $HostName\n`$SqlInstance     = $SqlInstance\n`$AWSAccountAlias = $AWSAccountAlias\n`$stage           = $stage\n \")\n</code></pre>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#ready-to-generate-some-yaml-magic","title":"Ready To Generate Some Yaml Magic","text":"<pre><code>$TargetConfig = (Join-Path $ENV:ProgramData 'Datadog/conf.d/windows_service.d/conf.yaml')\n$Services = [pscustomobject]@{\n    'instances' = @(\n        [ordered]@{\n            'services'                   =  @(\n                'SQLSERVERAGENT'\n                'MSSQLSERVER'\n                'SQLSERVERAGENT'\n            )\n            'disable_legacy_service_tag' = $true\n            'tags'                       = @(\n                \"aws_account_alias:$AWSAccountAlias\"\n                \"sql_instance:$SqlInstance\"\n                \"stage:$stage\"\n            )\n        }\n    )\n}\n\n$Services | ConvertTo-Json -Depth 100 | &amp;'C:\\tools\\yq.exe' eval - --prettyPrint | Out-File $TargetConfig -Encoding UTF8\n</code></pre> <p>This would produce a nice json output like this</p> <p></p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#one-more-complex-example","title":"One More Complex Example","text":"<p>Start with creating an empty array and some variables to work with.</p> <pre><code>$UserName = 'TacoBear'\n$Password = 'YouReallyThinkI''dPostThis?Funny'\n$TargetConfig = (Join-Path $ENV:ProgramData 'Datadog/conf.d/sqlserver.d/conf.yaml')\n$Queries = @()\n</code></pre> <p>Next include the generic Datadog collector definition.</p> <p>This is straight outta their Github repo with the benefit of some tagging.</p> <pre><code>$Queries += [ordered]@{\n    'host'      ='tcp:localhost,1433'\n    'username'  =$UserName\n    'password'  = $Password\n    'connector' ='adodbapi'\n    'driver'    = 'SQL Server'\n    'database'  = 'master'\n    'tags'      = @(\n        \"aws_account_alias:$AWSAccountAlias\"\n        \"sql_instance:$SqlInstance\"\n        \"stage:$stage\"\n    )\n}\n</code></pre> <p>{{&lt; admonition type=\"Tip\" title=\"Using += for Collections\" &gt;}} Using <code>+=</code> is a bit of an anti-pattern for high performance PowerShell, but it works great for something like this that's ad-hoc and needs to be simple. For high performance needs, try using something like <code>$list = [Systems.Collections.Generic.List[pscustomobject]]:new()</code> for example. This can then allow you to use the <code>$list.Add([pscustomobject]@{}</code> to add items.</p> <p>A bit more complex, but very powerful and performance, with the benefit of stronger data typing. {{&lt; /admonition &gt;}}</p> <p>This one is a good example of the custom query format that Datadog supports, but honestly I found pretty confusing in their docs until I bumbled my way through a few iterations.</p> <pre><code>$Queries +=    [ordered]@{\n    # description: Not Used by Datadog, but helpful to reading the yaml, be kind to those folks!\n    'description'             = 'Get Count of Databases on Server'\n    'host'                    ='tcp:localhost,1433'\n    'username'                = $UserName\n    'database'                = 'master'\n    'password'                = $Password\n    'connector'               ='adodbapi'\n    'driver'                  = 'SQL Server'\n    'min_collection_interval' = [timespan]::FromHours(1).TotalSeconds\n    'command_timeout'         = 120\n\n    'custom_queries'          = @(\n        [ordered]@{\n            'query'   = \"select count(name) from sys.databases as d where d.Name not in ('master', 'msdb', 'model', 'tempdb')\"\n            'columns' = @(\n                [ordered]@{\n                    'name' = 'instance.database_count'\n                    'type' = 'gauge'\n                    'tags' = @(\n                        \"aws_account_alias:$AWSAccountAlias\"\n                        \"sql_instance:$SqlInstance\"\n                        \"stage:$stage\"\n                    )\n                }\n            )\n        }\n    )\n}\n</code></pre> <p>Let me do a quick breakdown, in case you aren't as familiar with this type of syntax in PowerShell.</p> <ol> <li><code>$Queries +=</code> takes whatever existing object we have and replaces it with the current object + the new object. This is why it's not performant for large scale work as it's basically creating a whole new copy of the collection with your new addition.</li> <li>Next, I'm using <code>[ordered]</code> instead of <code>[pscustomobject]</code> which in effect does the same thing, but ensures I'm not having all my properties randomly sorted each time. Makes things a little easier to review. This is a shorthand syntax for what would be a much longer tedious process using <code>New-Object</code> and <code>Add-Member</code>.</li> <li>Custom queries is a list, so I cast it with <code>@()</code> format, which tells PowerShell to expect a list. This helps json/yaml conversion be correct even if you have just a single entry. You can be more explicit if you want, like <code>[pscustomobject[]]@()</code> but since PowerShell ignores you mostly on trying to be type specific, it's not worth it. Don't try to make PowerShell be Go or C#. \ud83d\ude01</li> </ol>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#flip-to-yaml","title":"Flip To Yaml","text":"<p>Ok, we have an object list, now we need to flip this to yaml.</p> <p>It's not as easy as <code>$Queries | yq</code> because of the difference in paradigm with .NET.</p> <p>We are working with a structured object.</p> <p>Just look at <code>$Queries | Get-Member</code> and you'll probably get: <code>TypeName: System.Collections.Specialized.OrderedDictionary.</code> The difference is that Go/Linux paradigm is focused on text, not objects. With <code>powershell-yaml</code> module you can run <code>ConvertTo-Yaml $Queries</code> and it will work as it will handle the object transformation.</p> <p>However, we can actually get there with PowerShell, just need to think of a text focused paradigm instead. This is actually pretty easy using <code>Converto-Json</code>.</p> <pre><code>$SqlConfig = [ordered]@{'instances' = $Queries }\n$SqlConfig | ConvertTo-Json -Depth 100 | &amp;'C:\\tools\\yq.exe' eval - --prettyPrint | Out-File $TargetConfig -Encoding UTF8\n</code></pre> <p>This takes the object, converts to json uses the provided cmdlet from PowerShell that knows how to properly take the object and all the nested properties and magically split to <code>JSON</code>.  Pass this into the <code>yq</code> executable, and behold, the magic is done.</p> <p>You should have a nicely formatted yaml configuration file for Datadog.</p> <p>If not, the dog will yip and complain with a bunch of red text in the log.</p>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#debug-helper","title":"Debug Helper","text":"<p>Use this on the remote instance to simplify some debugging, or even connect via SSM directly.</p> <pre><code>&amp; \"$env:ProgramFiles\\Datadog\\Datadog Agent\\bin\\agent.exe\" stopservice\n&amp; \"$env:ProgramFiles\\Datadog\\Datadog Agent\\bin\\agent.exe\" start-service\n\n#Stream Logs without gui if remote session using:\nGet-Content 'C:\\ProgramData\\Datadog\\logs\\agent.log' -Tail 5 -Wait\n\n# interactive debugging and viewing of console\n# &amp; \"$env:ProgramFiles\\Datadog\\Datadog Agent\\bin\\agent.exe\" launch-gui\n</code></pre>","tags":["development","tech","powershell","devops"]},{"location":"working-with-powershell-objects-to-create-yaml/#wrap-up","title":"Wrap Up","text":"<p>Ideally, use Chef, Ansible, Saltstack, DSC, or another tool to do this. However, sometimes you just need some flexible options for generating this type of content dynamically. Hopefully, you'll find this useful in your PowerShell magician journey and save some time.</p> <p>I've already found it useful in flipping json content for various tools back and forth. \ud83c\udf89</p> <p>A few scenarios that tooling like yq might prove useful could be:</p> <ul> <li>convert simple query results from json to yaml and store in git as config</li> <li>Flip an SSM Json doc to yaml</li> <li>Review a complex json doc by flipping to yaml for more readable syntax</li> <li>Confusing co-workers by flipping all their cloudformation from yaml to json or yaml from json. (If you take random advice like this and apply, you probably deserve the aftermath this would bring \ud83e\udd23.)</li> </ul>","tags":["development","tech","powershell","devops"]},{"location":"incremental-and-consistent/","title":"Incremental and Consistent","text":"<p>Saying that we do knowledge work but have no time for learning is like saying we do farm work but have no time for planting.</p>\u2014 Allen Holub (@allenholub) February 13, 2021 <p>It's really hard to prioritize when life gets busy, but it's important that continued improvement is a priority. Great at Work: How Top Performers Do Less, Work Better, and Achieve More was a really interesting book. The fact that small incremental improvement done daily can make such a difference is pretty interesting. It's similar to Agile tenets in how to approach software design. Smaller iterations with rapid feedback is better than large isolated batches work delivered without regular feedback. If you find yourself saying, \"But I don't have time\" or \"When I have some time\" it might be indicative of a failure to grasp this. When I catch myself saying this I try to reword it and say \"Whenever I make time for this\" instead. You'll always have pressure on you. The further along in your career and life you go, the more pressure is likely to be on you. You have to \"make\" time for improvement and learning if it's a priority.</p>","tags":["tech","development","microblog"]},{"location":"go-r1-day-28/","title":"Go R1 Day 28","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-28/#progress","title":"progress","text":"<ul> <li>Solved [Hamming Distance] on exercism.io</li> <li>Simple problem, but reminded me of how to use string split.</li> </ul> <pre><code>diffCount := 0\naString := strings.Split(a, \"\")\nbString := strings.Split(b, \"\")\n\nfor i, x := range aString {\n  if x != bString[i] {\n    diffCount++\n  }\n}\n</code></pre> <ul> <li>Reviewed other solutions, and found my first attempt to split the string wasn't necessary. Looks like I can just iterate on the string directly. I skipped this as it failed the first time. The error is: <code>invalid operation: x != b[i] (mismatched types rune and byte)</code>.</li> </ul> <p>This threw me for a loop initially, as I'm familar with .NET <code>char</code> datatype.</p> <p>Golang doesn't have a char data type. It uses byte and rune to represent character values. The byte data type represents ASCII characters and the rune data type represents a more broader set of Unicode characters that are encoded in UTF-8 format. Go Data Types</p> <p>Explictly casting the data types solved the error. This would be flexibly for UTF8 special characters.</p> <pre><code>for i, x := range a {\n  if rune(x) != rune(b[i]) {\n    diffCount++\n  }\n}\n</code></pre> <p>With this simple test case, it's it's subjective if I'd need <code>rune</code> instead of just the plain ascii <code>byte</code>, so I finalized my solution with <code>byte(x)</code> instead.</p> <pre><code>for i, x := range a {\n  if byte(x) != byte(b[i]) {\n    diffCount++\n  }\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-28/#links","title":"links","text":"<ul> <li>hamming-solution</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"git-workflow-with-git-town/","title":"Git Workflow With Git Town","text":"","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#resources","title":"Resources","text":"<p>Git-Town</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#painful-but-powerful","title":"Painful But Powerful","text":"<p>Let's get this out of the way.</p> <p>Git isn't intuitive.</p> <p>It has quite a bit of a learning curve.</p> <p>However, with this flexibility comes great flexibility. This tool has powered so much of modern open-source development.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#optimize-for-the-pain","title":"Optimize for the Pain","text":"<p>To improve the development experience some tools can help provide structure.</p> <p>This won't be an attempt to compare every git GUI, or push any specific tooling. It's more sharing my experience and what I've found helps accelerate my usage.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#tools-ive-relied-on","title":"Tools I've Relied On","text":"<ul> <li>Git-Town</li> <li>Bit</li> <li>GitHub CLI</li> <li>Git Graph</li> <li>Git Lens</li> </ul> <p>I'm not going to go into full detail on each, but check these out to help expedite your workflow.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#the-challenge-in-keeping-up-to-date-with-main","title":"The Challenge In Keeping Up To Date With Main","text":"<p>I use what's normally called <code>trunk-based</code> development. This entails regularly moving commits from branches into the main branch, often rebasing while maintaining it in a functional state.</p> <p>I'll create a feature branch, bug fix, or refactor branch and then merge this to <code>main</code> as soon as functional.</p> <p>I prefer a rebase approach on my branches, and when many ci/fix type commits, to squash this into a single unit of work as the results of the PR. This can result in \"merge hell\" as you try rebase on a busy repo.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#enter-git-town","title":"Enter Git Town","text":"<p>This tool solves so many of the basic workflow issues, that it's become one of the most impactful tools to my daily work.</p> <p>{{&lt; admonition type=\"Tip\" title=\"Enable Aliases\" closed=false &gt;}} The examples that follow use <code>git sync</code>, <code>git hack feat/new-feature</code>, etc as examples because I've run the command <code>git-town alias true</code> which enables the alias configuration for git town, reducing verbosity. Instead of <code>git town sync</code>, you can run <code>git sync</code>. {{&lt; /admonition &gt;}}</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#example-1-create-a-branch-for-a-new-unit-of-work-while-you-are-already-on-another-branch","title":"Example 1: Create a Branch for a New Unit of Work While You Are Already On Another Branch","text":"<p>Normally this would require:</p> <ol> <li>Stash/Push current work</li> <li>Checkout master</li> <li>Fetch latest and pull with rebase</li> <li>Resolve any conflicts from rebase</li> <li>Create the new branch from main</li> <li>Switch to the new branch</li> </ol> <p>With Git Town</p> <ol> <li><code>git hack feat/new-feature</code></li> </ol>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#example-2-sync-main","title":"Example 2: Sync Main","text":"<p>The following steps would be performed by: <code>git sync</code></p> <pre><code>[master] git fetch --prune --tags\n[master] git add -A\n[master] git stash\n[master] git rebase origin/master\n[master] git push --tags\n[master] git stash pop\n</code></pre>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#example-3-new-branch-from-main","title":"Example 3: New Branch From Main","text":"<p>Easy to quickly ensure you are up to date with remote and generate a new branch with your current uncommitted changes.</p> <pre><code>git town hack fix/quick-fix\n</code></pre> <pre><code>[master] git fetch --prune --tags\n[master] git add -A\n[master] git stash\n[master] git rebase origin/master\n[master] git branch feat/demo-feature master\n[master] git checkout feat/demo-feature\n[feat/demo-feature] git stash pop\n</code></pre>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#example-4-quickly-create-a-pr-while-on-a-branch-for-seperate-set-of-changes","title":"Example 4: Quickly Create a PR While On A Branch for Seperate Set of Changes","text":"<p>This workflow is far too tedious to do without tooling like this.</p> <p>Let's say I'm on a branch doing some work, and then I recognize that another bug, doc improvements, or other change unrelated to my current work would be good to submit.</p> <p>With git town, it's as simple as:</p> <pre><code>git town hack feat/improve-docs\n</code></pre> <p>I can stage individual lines using VSCode for this fix if I want to, and then after committing:</p> <pre><code>[feat/demo-feature] git fetch --prune --tags\n[feat/demo-feature] git add -A\n[feat/demo-feature] git stash\n[feat/demo-feature] git checkout master\n[master] git rebase origin/master\n[master] git branch feat/demo-feature-2 master\n[master] git checkout feat/demo-feature-2\n[feat/demo-feature-2] git stash pop\n</code></pre> <pre><code>git town new-pull-request\n</code></pre>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#example-5-ship-it","title":"Example 5: Ship It","text":"<p>When not using a PR-driven workflow, such as solo projects, then you can still branch and get your work over to main to keep a cleaner history with:</p> <pre><code>git town ship\n</code></pre> <p>This command ensures all the sync features are run, while then initiating a squash of your branch, allow you to edit the squash message, rebase merge this onto main, and finally clean-up the stale branch.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#more-examples","title":"More Examples","text":"<p>Check out the documentation from the creators: Git Town Tutorials</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#other-cool-features","title":"Other Cool Features","text":"<ul> <li>Automatically prune stale branches after PR merge when syncing</li> <li>Handles perennial branches if you are using Git Flow methodology.</li> <li>Extensible for other git providers.</li> <li>Rename a local branch + remote branch in a single command</li> <li>Handles a lot of edge cases and failures</li> </ul>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#wrap-up","title":"Wrap-Up","text":"<p>When using git, leveraging some tooling like this can accelerate your workflow. I don't think you need to be an expert in git to use this, as it helps simplify many workflows that are just too tedious to be diligent on when running manually.</p> <p>You can also do much of this with git aliases, but Git Town has a pretty robust feature-set with a testing framework in place, edge condition handling, and it's fast. Consider using it you'd like to improve your git workflow while simplifying all the effort to do it right.</p>","tags":["tech","development","blog","git","devops"]},{"location":"git-workflow-with-git-town/#backlinks","title":"Backlinks","text":"<ul> <li>[Git Hub Desktop Quick Look]2021-06-18-git-hub-desktop-quick-look/)<ul> <li>Update from <code>main</code> already built in. This is fantastic, and I can see how this provides a UI to do something similar to Git Town which I blogged on earlier here: [2021-02-23-git-workflow-with-git-town]2021-02-23-git-workflow-with-git-town/)</li> </ul> </li> </ul>","tags":["tech","development","blog","git","devops"]},{"location":"azure-pipelines-template-iteration/","title":"Azure Pipelines Template Iteration","text":"","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#templates","title":"Templates","text":"<p>This isn't meant to be an exhaustive template overview. I'm just going to give an example of what I found useful as I've been meaning to leverage templates for a while and finally got around to having a great use for it.</p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#my-use-case","title":"My Use Case","text":"<p>I am a heavy user of InvokeBuild. It's a very robust task runner tool that I've used to coordinate many project oriented actions, similar to Make, but also DevOps oriented work like AWS SSM tasks and more.</p> <p>In this scenario, I needed to run a query across: multiple queries -&gt; across multiple servers -&gt; across hundreds of databases --&gt; and finally produce a single jsonl artifact<sup>3</sup></p> <p>Originally, I did this in a single Invoke-Build task, but what I discovered was with a long running job I wanted to have a bit more granular view of the progress and percentage complete. This also helped me visualize a bit more what specific queries cost the most in time.</p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#azure-pipeline-construction","title":"Azure Pipeline Construction","text":"<p>I've extracted out the key essential pieces here to explain the core concepts.</p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#build-number","title":"Build Number","text":"<p>Build name is dynamically generated. This is my standard format for most pipelines, but you can adjust the naming with parameters (which are a compile time, before runtime) as well to add other descriptive values.</p> <pre><code>name: $(BuildDefinitionName).$(Configuration).$(Build.QueuedBy).$(DayOfYear)$(Rev:.r)\n</code></pre> <p>{{&lt; admonition type=\"Info\" title=\"Parameters\" open=\"false\"&gt;}}</p> <p>Parameters are evaluated at compile time, rather than during the build run phase.</p> <p>This means you can use something like the example below to update the queued build name on run.</p> <pre><code>name: $(BuildDefinitionName).$(Configuration).$(Build.QueuedBy).${{ parameters.SELECTED_VALUE }}.$(DayOfYear)$(Rev:.r)\n</code></pre> <p>Using a build variable might require updating the build name if the build variable isn't set on queue, as it won't pick it up without this command.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#trigger","title":"Trigger","text":"<p>Setting the following values ensures this is a manual pipeline. Otherwise, once the pipeline is linked it would automatically trigger on PR and main branch commits.</p> <p>There's other customization in terms of filtering triggers based on the path of the changed file, branch names, batching changes with multiple commits, and more in the docs.</p> <pre><code>trigger: none\npr: none\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#parameters","title":"Parameters","text":"<p>Designed for user input, the parameters provide a good experience in customizing runs easily at queue time.</p> <p>This can be a full yaml defined object, but my examples here are the simple ones.</p> <pre><code>parameters:\n  - name: Configuration\n    type: string\n    default: qa\n    values:\n      - qa\n      - prod\n  - name: QUERY_NAME\n    type: string\n    default: 'no-override'\n    displayName: If no-override, then run everything, else specify a specific query to run.\n  - name: SERVER_LIST\n    type: string\n    default: 'tcp:123.123.123.1;tcp:123.123.123.2' #split this in the task code\n    displayName: Example Input that InvokeBuild would split to array\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#variables","title":"Variables","text":"<p>Parameters won't be set in the environment variables, so if you want these exposed to the next tasks, you have to set the variable from the parameter.</p> <p>This command will now mean the tasks run will have a <code>$ENV:CONFIGURATION</code> set automatically.</p> <pre><code>variables:\n  - name: CONFIGURATION\n    value: ${{ parameters.Configuration }}\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#job","title":"Job","text":"<p>The pipelines allow you to only put the level of complexity you need in your runbook.</p> <p>This means if you just have tasks, you can put those, but if you have a deployment job then you can include tasks in the appropriate child section.</p> <p>For my default template here I like control of multi-stage yaml builds, so I use the following format.</p> <pre><code>jobs:\n  - deployment: my-query-runbook\n    displayName: Run Query in ${{ parameters.Configuration }}\n    timeoutInMinutes: 480\n    continueOnError: false\n    environment: 'my-environment-${{ parameters.Configuration }}'  #could setup approval requirements for environments by specifying a name like `my-environment-prod` requires manual approval or is limited to specific folks\n    pool:\n      name: my-own-internal-agent  # OR use hosted container config if you want\n      demands:\n        - agent.os -equals Windows_NT  # OR use Ubuntu if you have linux container. This is customizable to help you filter to desired agent if working with private subnets etc.\n        - env -equals ${{ parameters.Configuration }}\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n            - checkout: self\n              persistCredentials: true\n              fetchDepth: 0  # Unlimited in case you need more history\n              clean: false\n            - task: printAllVariables@1\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#using-the-template","title":"Using the Template","text":"<p>At the same level as the task, the template can be called.</p> <pre><code>            - template: templates/run-query.yml\n              parameters:\n                SERVER_LIST: ${{ parameters.QUERY_NAME}}\n                ${{ if ne(parameters.QUERY_NAME,'no-override') }}:\n                  querynames:\n                    - '${{ parameters.QUERY_NAME }}'\n                ${{ if eq(parameters.QUERY_NAME,'no-override') }}:\n                  querynames:\n                    - 'Query1'\n                    - 'Query2'\n                    - 'Query3'\n</code></pre> <p>A few concepts to unpack:</p> <ul> <li>Parameters must be passed into the template, as any build variables automatically in scope.</li> <li>Variable reuse <sup>6</sup> has it's own set of quirks with templates.</li> </ul> <p>Within a template expression, you have access to the parameters context that contains the values of parameters passed in. Additionally, you have access to the variables context that contains all the variables specified in the YAML file plus many of the predefined variables (noted on each variable in that topic). Importantly, it doesn't have runtime variables such as those stored on the pipeline or given when you start a run. Template expansion happens very early in the run, so those variables aren't available. <sup>4</sup></p> <ul> <li>Expressions allow some conditional evaluation and change in behavior of the pipeline.<sup>5</sup></li> </ul>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#template-structure","title":"Template Structure","text":"<pre><code>parameters:\n  - name: 'QUERY_NAME'\n    type: object\n    default: {}\n  - name: 'CONFIGURATION'\n    type: string\n  - name: 'SERVER_LIST'\n    type: string\n</code></pre> <p>Now that we have the parameters defined, we can use a steps block and loop on the <code>QUERY_NAME</code> parameter that could be a single or multiple entry input.</p> <pre><code>steps:\n  - ${{ each query in parameters.QUERY_NAME }}:\n      - task: PowerShell@2\n        displayName: Query ${{ query }}\n        inputs:\n          targetType: inline\n          script: |\n            &amp;./build.ps1 -Tasks 'run-my-query' -Configuration '${{ parameters.CONFIGURATION }}' -QueryName '${{ query }}'\n          errorActionPreference: 'Stop'\n          pwsh: true\n          failOnStderr: true\n          workingDirectory: $(Build.SourcesDirectory)\n        env:\n          OPTIONAL_ENV_VARS: ${{ parameters.EXAMPLE }}\n</code></pre> <p>This could also be slightly altered if you don't want inline scripts to use the following.</p> <pre><code>filePath: build.ps1\nargumentList: \"-Tasks 'run-my-query' -Configuration '${{ parameters.CONFIGURATION }}' -QueryName '${{ query }}'\"\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#reporting-progress","title":"Reporting Progress","text":"<p>As the task runs, you can output percent complete so that your task shows how far along it is. I find this great for long running tasks, helping me check on them and know it's not stuck.</p> <pre><code>Write-Host \"##vso[task.setprogress value=$PercentComplete;]MyTask\"\n</code></pre>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#final-result","title":"Final Result","text":"<p>This allows the job to set dynamically the individual tasks to run, report progress on each, and log the timing.</p> <p>While it could be run as a single task, I prefer this type of approach because a long running job is now much more easily tracked as it progresses.</p> <p></p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#further-features","title":"Further Features","text":"<p>Templates allow for a wide range of usage and flexibility that I've barely touched. Selecting entire sets of tasks at runtime, variable sets, and more are all available.</p> <p>This was a first round usage of them, as I really want to leverage the potentional for DRY with pipelines more, and templates offer a really flexible option to reusing core code across multiple pipelines without having to version each individually and try to keep them up to date.</p>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"azure-pipelines-template-iteration/#more-resources","title":"More Resources","text":"<ul> <li>Azure DevOps Tips: \u201cEach\u201d Loops. Applying DRY principle in CI/CD\u200a\u2014\u200aUsing\u2026 | by Jordan Lee | Medium</li> <li>YAML schema - Azure Pipelines | Microsoft Docs <sup>1</sup></li> <li>Predefined variables - Azure Pipelines | Microsoft Docs <sup>2</sup></li> <li>Expressions <sup>5</sup></li> </ul> <ol> <li> <p>YAML schema - Azure Pipelines | Microsoft Docs \u21a9</p> </li> <li> <p>Predefined variables - Azure Pipelines | Microsoft Docs \u21a9</p> </li> <li> <p>A standard for jsonl \u21a9</p> </li> <li> <p>Azure DevOps Template Context \u21a9</p> </li> <li> <p>Expressions - Azure Pipelines | Microsoft Docs \u21a9\u21a9</p> </li> <li> <p>Variable reuse \u21a9</p> </li> </ol>","tags":["tech","development","azure-devops","powershell","devops"]},{"location":"fix-terraform-provider-path-in-state/","title":"Fix Terraform Provider Path in State","text":"<p>Fixing Terraform provider paths in state might be required after upgrading to 0.13-0.14 if your prior state has the following paths.</p> <p>First, get the terraform providers from state using: <code>terraform providers</code></p> <p>The output should look similar to this:</p> <p></p> <p>To fix these, try running the commands to fix state. Please adjust to the required providers your state uses, and make sure your tooling has a backup of the state file in case something goes wrong. Terraform Cloud should have this backed up automatically if it's your backend.</p> <pre><code>terraform state replace-provider -- registry.terraform.io/-/aws registry.terraform.io/hashicorp/aws\nterraform state replace-provider -- registry.terraform.io/-/random registry.terraform.io/hashicorp/random\nterraform state replace-provider -- registry.terraform.io/-/null registry.terraform.io/hashicorp/null\nterraform state replace-provider -- registry.terraform.io/-/azuredevops registry.terraform.io/microsoft/azuredevops\n</code></pre> <p>The resulting changes can be seen when running <code>terraform providers</code> and seeing the dash is now gone.</p> <p></p> <p>Upgrading to Terraform v0.13 - Terraform by HashiCorp</p> <p>{{&lt; admonition type=\"Example\" title=\"Loop\" open=\"false\"&gt;}}</p> <p>If you have multiple workspaces in the same folder, you'll have to run fix on their seperate state files.</p> <p>This is an example of a quick adhoc loop with PowerShell to make this a bit quicker, using <code>tfswitch</code> cli tool.</p> <pre><code>tf workspace list | ForEach-Object {\n    $workspace = $_.Replace('*','').Trim()\n    Write-Build Green \"Selecting workspace: $workspace\"\n    tf workspace select $workspace\n    tfswitch 0.13.5\n    tf 013.upgrade\n    tfswitch\n    tf init\n    # Only use autoapprove once you are confident of these changes\n    terraform state replace-provider -auto-approve -- registry.terraform.io/-/aws registry.terraform.io/hashicorp/aws\n    terraform state replace-provider -auto-approve -- registry.terraform.io/-/random registry.terraform.io/hashicorp/random\n    terraform state replace-provider -auto-approve -- registry.terraform.io/-/null registry.terraform.io/hashicorp/null\n    terraform state replace-provider -auto-approve -- registry.terraform.io/-/azuredevops registry.terraform.io/microsoft/azuredevops\n    tf validate\n}\n</code></pre> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","microblog","terraform"]},{"location":"go-r1-day-29/","title":"Go R1 Day 29","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-29/#progress","title":"progress","text":"<ul> <li>Evaluated Mage as a replacement for bash/pwsh based tasks for automation with Azure Pipelines.</li> <li>Was able to get terraform to run with dynamic configuration using the following approach:</li> </ul> <p>Install with</p> <pre><code>go get -u github.com/magefile/mage/mg\ngo mod init mage-build\ngo get github.com/magefile/mage/mg\ngo get github.com/magefile/mage/sh\ngo mod tidy\n</code></pre> <p>Then to get <code>mage-select</code> run:</p> <pre><code>GO111MODULE=off go get github.com/iwittkau/mage-select\ncd $GOPATH/src/github.com/iwittkau/mage-select\nmage install\n</code></pre> <p>Configure some constants, which I'd probably do differently later. For now, this is a good rough start.</p> <pre><code>const (\n    repo          = \"myrepopath\"\n    name          = \"myreponame\"\n    buildImage    = \"mcr.microsoft.com/vscode/devcontainers/base:0-focal\"\n    terraformDir  = \"terraform/stack\"\n    config_import = \"qa.config\"\n)\n</code></pre> <pre><code>func TerraformInit() error {\n    params := []string{\"-chdir=\" + terraformDir}\n    params = append(params, \"init\")\n    params = append(params, \"-input=false\")\n    params = append(params, \"-var\", \"config_import=\"+config_import+\".yml\")\n\n    // Backend location configuration only changes during the init phase, so you do not need to provide this to each command thereafter\n    // https://github.com/hashicorp/terraform/pull/20428#issuecomment-470674564\n    params = append(params, \"-backend-config=./\"+config_import+\".tfvars\")\n    fmt.Println(\"starting terraform init\")\n    err := sh.RunV(\"terraform\", params...)\n    if err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre> <p>Once terraform was initialized, it could be planned.</p> <pre><code>func TerraformPlan() error {\n    mg.Deps(TerraformInit)\n    params := []string{\"-chdir=\" + terraformDir}\n    params = append(params, \"plan\")\n    params = append(params, \"-input=false\")\n    params = append(params, \"-var\", \"config_import=\"+config_import+\".yml\")\n    fmt.Println(\"starting terraform plan\")\n    err := sh.RunV(\"terraform\", params...)\n    if err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre> <ul> <li>Of interest as well was mage-select, providing a new gui option for easier running by others joining a project.</li> </ul> <p></p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-29/#links","title":"links","text":"<ul> <li>mage</li> <li>mage-select</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-30/","title":"Go R1 Day 30","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-30/#progress","title":"progress","text":"<ul> <li>Built some go functions for build tasks work with terraform and setup of projects using taskflow.</li> </ul> <p>Learned one one to pass in arguments using slices. I'm pretty sure you can use some stringbuilder type functionality to get similar behavior, but this worked fine for my use case.</p> <pre><code>cmdParams := []string{}\ncmdParams = append(cmdParams, \"-chdir=\"+tfdir)\ncmdParams = append(cmdParams, \"init\")\ncmdParams = append(cmdParams, \"-input=false\")\ncmdParams = append(cmdParams, \"-backend=true\")\ncmdParams = append(cmdParams, \"-backend-config=\"+tfconfig)\nterraformCmd := tf.Cmd(terraformPath, cmdParams...)\nif err := terraformCmd.Run(); err != nil {\n  tf.Errorf(\"\u2b55 terraform init failed: [%v]\", err)\n  return\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-31/","title":"Go R1 Day 31","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-31/#progress","title":"progress","text":"<ul> <li>Learned a bit about idiomatic patterns wtih error handling.</li> <li>Learned about inline block intiailization of variables using <code>if err := method(); err != nil {...}</code> approach.</li> <li>Considered a bit more idiomatic patterns when I noticed excessibe nested if blocks.</li> </ul> <pre><code>tfdir := tf.Params().String(\"tfdir\")\nif tfdir != \"\" {\n  tf.Logf(\"tfdir set to: [%s]\", tfdir)\n} else {\n  tf.Errorf(\"\ud83e\uddea failed to get tfdir parameter: [%v]\", tfdir)\n}\n</code></pre> <p>This would probably be more in alignment with Go standards by writing as:</p> <pre><code>tfdir := tf.Params().String(\"tfdir\")\nif tfdir == \"\" {\n  tf.Errorf(\"\ud83e\uddea failed to get tfdir parameter: [%v]\", tfdir)\n  return\n}\ntf.Logf(\"tfdir set to: [%s]\", tfdir)\n</code></pre> <p>This reduces the noise and keeps things pretty flat.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-31/#links","title":"links","text":"<p>When Should I Use One Liner if...else Statements in Go?)</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-32/","title":"Go R1 Day 32","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-32/#progress","title":"progress","text":"<ul> <li>created some structured logging improvements with zerolog</li> <li>began exploration of middleware concepts for logging</li> <li>generated test stubs using gotests</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-33/","title":"Go R1 Day 33","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-33/#progress","title":"progress","text":"<ul> <li>successfully created logging package using zerolog</li> <li>learned about scoping with packages</li> <li>linked to a private internal repository and how to leverage the module replace operator to temporarily alter path import from url to local override.</li> <li>middleware is a newer concept, so I need to learn more on this later so I can understand how to use to inject special log handling for http requests and other actions.</li> </ul> <p>Thoughts for today are that the pressure of jumping into an existing codebase is resulting in me moving faster than I probably should. I'm going to take some time to keep doing the web fundamentals, lambda, and exercisms to ensure I'm setting a better foundation long-term, and not just winging it. \ud83d\ude04</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-34/","title":"Go R1 Day 34","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-34/#progress","title":"progress","text":"<ul> <li>figured out how to import <code>util/logger.go</code> as a package</li> <li>after much confusion due to <code>logger</code>, <code>log</code>, <code>*zerolog.Logger</code> and more variables all deviously similar in name... how to pass around the initialized package logger that I configure.</li> <li>learned that global scope and package scoped loggers being initialized at run is concerned an anti-pattern</li> <li>properly wrapping to avoid the log initialization on import with <code>type Logger struct { logger: *zerolog.Logger; }</code> as an example avoids the same behavior as: <code>var Log *zerolog.Logger</code></li> <li>will evaluate better scoping in the future, but for now figured it would be a \ud83d\ude80 #shipit moment to improve as I can later. <sup>1</sup></li> </ul> <ol> <li> <p>The package level logger anti pattern \u2013 The acme of foolishness \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-35/","title":"Go R1 Day 35","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-35/#progress","title":"progress","text":"<ul> <li>Worked with Taskflow a bit more.</li> <li>Need to identify better error handling pattern on when to resolve vs handle internal to a function, as it feels like I'm doing needless error checking.</li> <li>Wrote func to run terraform init, plan, and apply.</li> <li>This takes dynamical inputs for vars and backend file.</li> <li>Also dynamically switches terraform versions by running tfswitch.</li> </ul> <p>Definitely more verbose code than powershell, but it's a good way to get used to Go while achieving some useful automation tasks I need to do.</p> <p>Example of some code for checking terraform path.</p> <pre><code>func terraformPath(tf *taskflow.TF) (terraformPath string, err error) {\n    terraformPath = path.Join(toolsDir, \"terraform\")\n    if _, err := os.Stat(terraformPath); os.IsNotExist(err) {\n        tf.Errorf(\"\u2757 cannot find terraform at: [%s] -&gt; [%v]\", terraformPath, err)\n        return \"\", err\n    }\n    tf.Logf(\"\u2705 found terraform at: [%s]\", terraformPath)\n    return terraformPath, nil\n}\n</code></pre> <pre><code>terraformPath, err := terraformPath(tf)\nif err != nil {\n  tf.Errorf(\"\u2757 unable to proceed due to not finding terraform installed [%v]\", err)\n  return\n}\n</code></pre> <p>However, once I call this, I'm see more effort in handling, which feels like I'm double double work at times.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-36/","title":"Go R1 Day 36","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-36/#progress","title":"progress","text":"<ul> <li>more taskflow work</li> <li>learned how to use io.Multiwriter to have both a stdout stream as well as output captured to a <code>stringbuilder</code>.</li> <li>got confused on reading docs, only to realize I was reading docs for a C# solution. \ud83d\ude02</li> <li>used Pterm<sup>1</sup> to beautiful console output, which was pretty cool to see in action.</li> </ul> <ol> <li> <p>pterm/pterm: \u2728 #PTerm is a modern go module to beautify console output \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-37/","title":"Go R1 Day 37","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-37/#progress","title":"progress","text":"<ul> <li>initialized new start of cli tool to try to parse markdown content using Goldmark<sup>1</sup> and then convert markdown links into reference style links</li> <li>Improved default template for starting new basic cli tools with Go to include zerolog and run based design instead of the majority of logic being in main.</li> </ul> <ol> <li> <p>goldmark \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"diagrams-as-code/","title":"Diagrams as Code","text":"","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#why","title":"Why","text":"<p>It's not for everyone.</p> <p>I prefer to write diagrams out logically and then let the visual aspect be generated.</p> <p>This helps me by making the visual presentation an artifact of what is basically a written outline.</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#presentation-matters","title":"Presentation Matters","text":"<p>I've learned that communicating flows of information can often be better presented visually than trying to write them out, especially once it involves more than a few \"nodes\" of responsibility. Visualizing a complicated process can be done more easily in a visual way, as well as help expose some possible issues when ownership is transfered between lanes too many times.</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#options","title":"Options","text":"<p>LucidChart, Draw.io and other tools are great for a quick solution.</p> <p>Mermaid also provides a nice simple text based diagramming tool that is integrated with many markdown tools.</p> <p>For me, this just never fit. I like a bit of polish and beauty in a visual presentation and most of these are very utilitarian in their output.</p> <p>I came across diagrams<sup>2</sup> and found it a perfect fit for intuitive and beautiful diagram rendering of cloud architecture, and figured it would be worth a blog post to share this.</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#getting-started","title":"Getting Started","text":"<p>{{&lt; admonition type=\"Warning\" title=\"Name Of Project\" open=\"true\"&gt;}} Do not create a project named the same thing as your dependency, ie project name = diagrams for example.</p> <p>This will error out and send you down a search in github issues to discover this issue.</p> <p>{{&lt; /admonition &gt;}}</p> <p>Install Poetry<sup>1</sup> and create a new poetry project in your directory using <code>poetry init</code>.</p> <p>Once it gets to the package additions add <code>diagrams</code> to your poetry file.</p> <p>Run <code>poetry install</code></p> <p>Finally, create a new file called <code>diagram.py</code> in your directory.</p> <p>Once you populate this file, you can run your diagram using the virtual env it manages by calling <code>poetry run python diagram.py</code>.</p> <p>Additionally, any command line arguments you want to pass would just go through like <code>poetry run python diagram.py --outdirectory foobar</code></p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#diagrams","title":"Diagrams","text":"<p>The documentation is pretty thorough, but detailed examples and shortcuts are very hard to find. You'll have to dig through the repo issues on occasion if you find yourself wanting to do something that isn't obvious. This project seems to be a wrapper around graphviz, so a lot of the documentation for parameters and customizations will be in it's documentation, not in this project.</p> <p>To find available nodes and shapes, you'll need to look at the diagram docs<sup>5</sup></p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#simple-example","title":"Simple Example","text":"<p>Using defaults you can create a simple diagram such as this:</p> <p></p> <p>{{&lt; gist sheldonhull \"cc8abcb86c9463b0c74bb9e4d82ffac9\" \"01-diagram-vpc-example.py\"&gt;}}</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#add-some-helpers","title":"Add Some Helpers","text":"<p>From the github issues and my own customizations, I added a few additions to make the edge (ie, lines) flow easier to work with.</p> <p></p> <p>{{&lt; gist sheldonhull \"cc8abcb86c9463b0c74bb9e4d82ffac9\" \"02.helpers.py\"&gt;}}</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#a-more-complex-example","title":"A More Complex Example","text":"<p>I went through the AWS Reference Architecture Diagrams <sup>3</sup> and used this to provide a more complex example.</p> <p>Take a look at the AWS PDF<sup>4</sup> and compare.</p> <p></p> <p>{{&lt; gist sheldonhull \"cc8abcb86c9463b0c74bb9e4d82ffac9\" \"03-diagram-complex.py\"&gt;}}</p>","tags":["tech","development","devops","python"]},{"location":"diagrams-as-code/#reference","title":"Reference","text":"Graphviz Reference Colors Available Nodes Individual Node Edits Reference for Graph Attributes <ol> <li> <p>Introduction | Documentation | Poetry - Python dependency management and packaging made easy. \u21a9</p> </li> <li> <p>Diagram GitHub Project \u21a9</p> </li> <li> <p>AWS Reference Architecture Diagrams \u21a9</p> </li> <li> <p>Pull Request Continuous Integration Reference Architecture \u21a9</p> </li> <li> <p>Diagram Project - AWS Diagram Node List \u21a9</p> </li> </ol>","tags":["tech","development","devops","python"]},{"location":"hackattic/","title":"Hackattic","text":"<p>Came across this pretty nifty site HackAttic that provides a small set of practical oriented exercises. Will look at using further in my Go journey, as seems really useful to stretch one's development skills in less familar, but still practical ways.</p> <p>A few of the challenges:</p> <ul> <li>Generate an SSL certificate according to given data.</li> <li>Extract some data from Redis. In a way.</li> <li>Handle some incoming JWTs for us.</li> </ul> <p>Take a look at the newsletter topics covered. I love that sense of humor.</p>","tags":["tech","development","microblog"]},{"location":"azure-devops-pr-automation-requires-generic-contribute/","title":"Azure Devops PR Automation Requires Generic Contribute","text":"<p>Big fan of renovate for terraform dependency management.</p> <p>Ran into some complications with restarting an onboarding with Whitesource Renovate in Azure DevOps Repos and the Azure Pipeline automation. I've run into this before, so logging this for my sanity.</p> <ul> <li>If you failed to onboard with Azure DevOps, you can't rename the PR like in GitHub to simplify restarting the onboarding process.</li> <li>Instead, delete the <code>renovate.json</code> file and commit to your default branch.</li> <li>Then re-add and get the <code>renovate.json</code> committed back to your default branch.</li> <li>Run your Azure DevOps Pipeline to trigger the dependency assessment.</li> <li>If you didn't add the project build service account to your repo with <code>Contribute</code>, <code>Contribute to Pull Requests</code>, <code>Force Push</code> (to allow force update of branches it creates), and <code>Create Tag</code>.</li> </ul> <p>{{&lt; admonition type=\"Tip\" title=\"Where do you add the permissions for build?\" open=\"true\"&gt;}}</p> <p>Go to your repository security settings (in this case I'm adding to all to simplify, but you can do on a repo by repo basis if you enjoy tedium):</p> <p><code>https://dev.azure.com/MYORG/MYPROJECT/_settings/repositories?_a=permissions</code></p> <p>Then in the search type your project name and you should see a <code>PROJECTNAME\\Build Service (PROJECTNAME)</code> show up.</p> <p>If you are using a custom service account with on-premise agents instead of hosted, then adjust your approach accordingly.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","microblog","azure-devops","devops"]},{"location":"cool-way-to-compare-features-in-note-apps/","title":"Cool Way to Compare Features in Note Apps","text":"<p>Nice little site I came across for comparing various popular knowledge &amp; note taking tools with ability to select features most important to you.</p> <p>NoteApps</p> <p>Note typically a fan of compare based tools, as they often feel like marketing junk, but this was a a pretty cool little implementation.</p>","tags":["tech","microblog","cool-sites"]},{"location":"make-vscode-annoy-me-when-i-make-a-typo/","title":"Make Vscode Annoy Me When I Make a Typo","text":"<p>Not sure why, but I've had 2 typos that keep plauging me.</p> <ul> <li><code>ngnix</code> should be <code>nginx</code></li> <li><code>chocolatey</code> should be <code>chocolatey</code></li> </ul> <p>With Go, I get compile errors with typos, so no problem. With PowerShell or Bash, this can be really annoying and waste time in debugging.</p> <p>You can configure many autocorrect tools on a system level, but I wanted a quick solution for making it super obvious in my code as I typed without any new spelling extensions.</p> <p>Install Highlight: <code>fabiospampinato.vscode-highlight</code> <sup>1</sup></p> <p>Configure a rule like this:</p> <pre><code>\"highlight.regexes\": {\n    \"(ngnix)\": [\n            {\n                \"overviewRulerColor\": \"#ff0000\",\n                \"backgroundColor\": \"#ff0000\",\n                \"color\": \"#1f1f1f\",\n                \"fontWeight\": \"bold\"\n            },\n            {\n                \"backgroundColor\": \"#d90000\",\n                \"color\": \"#1f1f1f\"\n            }\n        ],\n}\n</code></pre> <ol> <li> <p>vscode-highlight \u21a9</p> </li> </ol>","tags":["tech","development","microblog","vscode"]},{"location":"cloudflare-analytics-for-your-jamstack-site/","title":"Cloudflare Analytics for Your Jamstack Site","text":"<p>A new alternative to Google Analytics if you want to keep things simple.</p> <p>Cloudflare Web Analytics seems promising.</p> <p>I'd probably have started with this if I was setting up my site from scratch.</p> <p>The problem I have with adopting a tool like this is no basic import of metrics from Google Analytics means I'd lose years of history.</p> <p>Does the history matter signficantly? Not really for me. I just don't like losing out the entire history to see how things have improved over time.</p>","tags":["tech","development","microblog","jamstack"]},{"location":"the-open-guide-to-amazon-web-services/","title":"The Open Guide to Amazon Web Services","text":"<p>The guide you didn't know you needed for AWS. Came across this when Corey Quinn <sup>2</sup> mentioned this on a podcast.</p> <p>This is a resource I wish I'd known about earlier. It covers the core essentials on services, and cuts through the confusion of the AWS Docs pretty well.</p> <p>For example, I noticed the ELB references for ALB docs and this caused me some confusion.</p> <p>Right front and center on Load Balancer Basics <sup>1</sup>:</p> <p>Before the introduction of ALBs, \"Classic Load Balancers\" were known as \"Elastic Load Balancers\" (ELBs), so older documentation, tooling, and blog posts may still reference \"ELBs\".</p> <p>I'm going to make this a first stop when getting familar with new AWS concepts.</p> <p>{{&lt; admonition type=\"Tip\" title=\"macOS lifehack\" open=\"true\"&gt;}}</p> <p>If you have Dash installed, set the repo as a GitHub datasouce, and you can have the entire handbook cached offline as an offline documentation library.</p> <p>{{&lt; /admonition &gt;}}</p> <ol> <li> <p>og-aws-loadbalancers \u21a9</p> </li> <li> <p>Corey Quinn \u21a9</p> </li> <li> <p>Dash Documentation Tool for macOS \u21a9</p> </li> </ol>","tags":["tech","development","microblog","aws","devops"]},{"location":"dance-time/","title":"Dance Time","text":"<p>Uploaded a few to my gallery from time I had with my daughter watching her live performance this weekend. Take a peek.</p> <p>I need to catch-up on a few posts with the family with all the great stuff they've been doing lately. (My son just got his black belt in Taekwondo \ud83d\udc4f)</p>","tags":["ramblings","art","family"]},{"location":"value-local-art/","title":"Value Local Art","text":"<p>Our culture is driven on some much virtual comsumption of media and performances. It's important to remember just how critical it is to support local art and performance.</p> <p>In a YouTube culture, it's easy to default to virtualized arts. I think it's easy to lose the appreciation for the disciplined effort creating music, dance, art requires when it's all just something on a screen.</p> <p>It also gives an unrealistic expectation for live performances if our only consumption is the polished and santized version of someone's best take. Seems to be a similar problem to the fake reality Instagram and Facebook can give if you expect the true state of a person's life is represented in their social media posts.</p> <p>{{&lt; youtube \"waRss-9jVis\" &gt;}}</p> <p>As far as flaws go, this beautiful improv session shows in the comments just how much viewers appreciated the honesty of leaving flaws in instead. I'd like to see more of this from artists. We all crave authenticity.</p>","tags":["ramblings","art","microblog"]},{"location":"go-r1-day-38/","title":"Go R1 Day 38","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-38/#progress","title":"progress","text":"<ul> <li>Worked on cross platform binary builds and CICD with GoReleaser.</li> <li>Lots of container work here.</li> <li>Started blog post on how to use GoReleaser.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"the-dark-side/","title":"The Dark Side","text":"<p>Saw this on a Twitter thread and had to save it. If you've ever debated light vs dark themes, you might find this as funny as I did.</p> <p>Nearly choked on my dinner.</p> <p>{{&lt; youtube XlgqZeeoOtI &gt;}}</p>","tags":["tech","development","microblog","humor"]},{"location":"highlight-problem-characters-in-code/","title":"Highlight Problem Characters in Code","text":"<p>Use Vscode Gremlins to help flag characters that shouldn't be in your code.</p>","tags":["vscode","development"]},{"location":"highlight-problem-characters-in-code/#specifying-a-range-of-invalid-characters","title":"Specifying a Range of Invalid Characters","text":"<p>You can give a range to flag multiple characters with a single rule.</p> <p>For example, if using macOS and the option key is set to a modifier, it's easy to accidentally include a Latin-1 Supplemental Character that can be difficult to notice in your code.</p> <p>To catch the entire range, the Latin-1-Supplement link provided shows a unicode range of: <code>0080\u201400FF</code></p> <p>Configure a rule like this:</p> <pre><code>\"gremlins.characters\": {\n    \"0080-00FF\": {\n        \"level\": \"error\",\n        \"zeroWidth\": false,\n        \"description\": \"Latin-1 Supplement character identified\",\n        \"overviewRulerColor\": \"rgba(255,127,80,1)\",\n    },\n}\n</code></pre> <p>I submitted this as a PR to project repo but figured I'd document here as well in case it takes a while to get merged.</p>","tags":["vscode","development"]},{"location":"career-advice-from-a-drunk-dev-on-reddit/","title":"Career Advice From a Drunk Dev on Reddit","text":"<p>Drunk Post: Things I've learned as a Sr Engineer has trending with some fantastic discussion on Twitter &amp; Reddit.</p> <p>What makes it so interesting is the raw and unfiltered message, mixed in with some great humor.</p> <p>{{&lt; admonition type=\"Note\" title=\"My Background\" open=\"true\"&gt;}}</p> <p>The author of that post comes from a Data Engineer side.</p> <p>I've been a Database Developer, Production DBA, Cloud Engineer, and other odd hats.</p> <p>Some of my perspectives is different from the roles I've been involved in.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","blog"]},{"location":"career-advice-from-a-drunk-dev-on-reddit/#agree","title":"Agree \ud83d\udc4d","text":"<p>If I'm awaken at 2am from being on-call for more than once per quarter, then something is seriously wrong and I will either fix it or quit.</p> <p>I agree you fix it! I believe in continual refactoring and improvement, applying Site Reliability Engineering principles. Don't kick the can down the street. Own it!</p> <p>For beginners, the most lucrative programming language to learn is SQL.</p> <p>Agree, it's a solid start and often can be done if you are working in a non-engineer role to bring value immediately to your company.</p> <p>Paying for classes, books, conferences is worth it.</p> <p>A caveat: a company should invest in the development of their employees, so try to leverage those benefits first.</p> <p>If people are trying to assign blame to a bug or outage, it's time to move on.</p> <p>If this is a routine occurrence, heck yes. Sometimes you can't help but run into politics.</p> <p>I love drinking with my co-workers during happy hour. I'd rather spend time with kids, family, or friends.</p> <p>Agreed. On this topic, companies shouldn't schedule team/company events in employees' off time. Schedule it during work hours. Don't impact their family.</p> <p>The best demonstration of great leadership is when my leader took the fall for a mistake that was 100% my fault. You better believe I would've walked over fire for her.</p> <p>100% agree. My first week at one of my roles, I broke internalization and required the team to deploy a hotfix to clients overseas. Release manager protected me during calls and said we had a fix in place. No blame. I stayed 7 years and would have done almost anything for that release manager.</p> <p>* those devops guys and gals are* smart. At least those mofos get paid though.</p> <p>\ud83d\ude0a We are making it up just like anyone else, but have to have flexibility in learning and change due to the wide range of things to learn. Despite focusing on a role in Backend Engineering now, I'm incredibly grateful to have worked in a Development Operations team and see growth with my ability to handle pressure, cross-functional communication, and wide range of technology exposure.</p> <p>AWS is hard.</p> <p>Linux is important even when I was working in all Windows. Why? Because I eventually worked in Linux. So happy for those weekend where I screwed around installing Arch.</p> <p>True. I wish I had started early. There is a career in purely Windows-based development, but I'd tell anyone starting out, it's better to start with Linux as a choice if possible. It feels so much simpler overall than Windows with all its API-based management, registry, and lots of complexity overall.</p> <p>I don't know why I siked myself out for so long. Linux feels natural as I'm immersed in it more.</p> <p>I've learned to be wary for ambiguous buzz words like big data.</p> <p>Agreed. My inner geek want to play with ML. My engineer side questions the value for ML in normal enterprise work. It brings value, but I'd lean towards thinking the majority of businesses not specifically doing large amounts of analytics as a core business aren't going to find quick value from the magic 8 ball.</p> <p>See Machine Learning Is A Marvelously Executed Scam</p>","tags":["tech","development","blog"]},{"location":"career-advice-from-a-drunk-dev-on-reddit/#disagree","title":"Disagree \ud83d\udc4e","text":"<p>When I first started, I was enamored with technology and programming and computer science. I'm over it.</p> <p>I still love tech. I think the tendency to tinker and experiment is just in my dna.</p> <p>The most underrated skill to learn as an engineer is how to document.</p> <p>I wish this was true. In my experience, the documentation I've written hasn't brought the value and recognization I thought it would, if the culture doesn't prize this.</p> <p>I still write &amp; blog, but unless the work culture supports reading first then meeting on something, I've found it's not given the priority it deserves.</p> <p>The older I get, the more I appreciate dynamic languages. ****, I said it. Fight me.</p> <p>I started with dynamic languages. Having started with Go in the last year, I'm finding a happy medium, and seeing that it presents a good development experience. I'd lean towards something like Go now, even for automation oriented work when possible. Seriously, I've lost so many hours with stupid typo &amp; type issues in languages like PowerShell</p> <p>I'm slowly gravitating towards Go for most work, though its verbosity can be annoying compared to a quick PowerShell one-liner.</p> <p>Titles mostly don't matter. Principal Distinguished Staff Lead Engineer from Whatever Company, whatever. What did you do and what did you accomplish. That's all people care about.</p> <p>Fellow engineers couldn't care less about your title. Titles matter to HR when dealing with compensation and determining rates, so don't ignore how important they are outside of the actual job role.</p>","tags":["tech","development","blog"]},{"location":"career-advice-from-a-drunk-dev-on-reddit/#last-thoughts","title":"Last Thoughts","text":"<p>Compiling some principles and thoughts on my experiences could be a good way to think through key areas I've improved over the years, so I'll probably take a swing at creating my own \"Career Advice to Myself\" post. TBD</p>","tags":["tech","development","blog"]},{"location":"log4-brains-improves-adr-experience/","title":"Log4 Brains Improves ADR Experience","text":"<p>I'm a fan of architectural decision records. Docs as code sets a practice for a team to track decisions in process and design.</p> <p>Log4Brains provides markdown template based adr creation, with an option to render a static site for reading.</p> <p>If you want to start managing your ADRs in a structured way, but benefit from simple markdown as the source files, then this is a great option to consider.</p> <p>It's really easy to get started with docker, no <code>npm install</code> required.</p> <p>Just run: <code>docker run --rm -p 4004:4004 -v ${PWD}:/opt/adr trivadis/log4brains</code> and open <code>http://127.0.0.1:4004</code></p> <pre><code># Proposal To Write a Microblog Post on ADRS\n\n- Status: proposed\n- Approver: Me\n- Tags: microblog,hugo,magic\n\n## Context and Problem Statement\n\nOverally inhibiting view on tacos is preventing continual tacoops from meeting required SLA.\n\n## Decision\n\n- Tacos for everyone. \ud83c\udf2e\n</code></pre>","tags":["tech","development","microblog"]},{"location":"gitpod-user-feedback/","title":"GitPod User Feedback","text":"<ol> <li>Thank you GitPod for integrating with VSCode. \ud83c\udf89</li> <li>Upcoming ssh support, so freaking cool! Are you using Chisel in the backend per the GitHub issues I found discussing workaround? As a Go dev, I'm interested \u26a1 I believe Jetbrains + VSCode users will now both be able to leverage the same backend.</li> <li>GitHub took over the codespace arena with Visual Studio Codespaces being abandoned. Enterprise customers using Azure Repos can't benefit from any of this new flexibility \ud83d\udc94. Is it in the roadmap to support this enterprise arena with Azure Repos (Part of Azure DevOps suite)? With the pull request experience in Azure Repos, I'd love to leverage pre-built GitPod workspaces, even if it required the self-hosted component.</li> <li>Feature Request: Consider 1-3 private workspaces for the free tier, with maybe 1 active at a time like the Codespaces beta as a good route towards adopting for private projects and testing. A limited private count would help adoption for those wanting to experiment with it for small private projects. I'd guess it might help make the jump to paid easier as it would allow the adoption of workflow in private and public projects.</li> <li>Lastly, beautiful job on the website changelog, and thank you @mikenikles for the RSS implementation. Your website design is beautiful.</li> </ol>","tags":["development","microblog","docker"]},{"location":"go-r1-day-39/","title":"Go R1 Day 39","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-39/#progress","title":"progress","text":"<ul> <li>built a better understanding of how to resolve Go modules from private Azure DevOps instances</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-39/#links","title":"links","text":"<ul> <li>New Blog Post \ud83c\udf89  Using Azure DevOps for Private Go Modules</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"using-azure-devops-for-private-go-modules/","title":"Using Azure DevOps for Private Go Modules","text":"<p>{{&lt; admonition type=\"Note\" title=\"2022-12-14\" open=true &gt;}}</p> <p>Provided an example of how to handle private go modules in Azure Pipeline compatible method.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#tldr","title":"TL;DR","text":"<p>This took a few hours of work to iron out, so figured maybe I'd save someone time.</p> <p> Just keep it simple and use SSH</p> <p> Use <code>dev.azure.com</code> even if using older <code>project.visualstudio.com</code> to keep things simple.</p>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#modules-support","title":"Modules Support","text":"<p>Unlike GitHub, Azure DevOps has some quirks to deal with, specifically in the odd path handling.</p> <p>My original goal was to set the default handling to be <code>https</code> support, with the SSH override in git config allowing me to use SSH.</p> <p>This didn't work.</p> <ul> <li>HTTPS requires <code>_git</code>  in the path.</li> <li>SSH will not work with that, and also trims out the org name in the url when git config set based on instructions from Microsoft<sup>1</sup>.</li> </ul> <p>There is a long-running issue with go get imports of Azure DevOps repositories due to the fact that the HTTPS URL contains a <code>_git</code> segment: <sup>2</sup></p> <p>Compare the path.</p> Type Path HTTPS <code>go get dev.azure.com/&lt;organization&gt;/&lt;project&gt;/_git/&lt;repo&gt;</code>  What I used with SSH <code>go get dev.azure.com/&lt;project&gt;/_git/&lt;repo&gt;</code> SSH <code>go get dev.azure.com/&lt;organization&gt;/&lt;project&gt;/&lt;repo&gt;.git</code>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#git-config","title":"Git Config","text":"<p>Set this in your <code>.profile, .bashrc, or $PROFILE</code></p> <pre><code>export GOPRIVATE=dev.azure.com\n</code></pre> <p>There are two approaches you can take.</p> <p>One seems focused on allowing other <code>dev.azure.com</code> public projects to be used. I've never had that need, so I'm ok with my <code>dev.azure.com</code> references being resolved only to my own organization.</p> Type Command GitConfig Support All Azure DevOps (Public) <code>git config --global url.\"git@ssh.dev.azure.com:v3/&lt;organization&gt;/\".insteadOf \"https://dev.azure.com/&lt;organization&gt;\"</code> <code>[url \"git@ssh.dev.azure.com:v3\"]&lt;br/&gt;</code><code>insteadOf = https://dev.azure.com</code>  What I Used for  Private Org <code>git config --global url.\"git@ssh.dev.azure.com:v3/&lt;organization&gt;/\".insteadOf \"https://dev.azure.com/</code> <code>[url \"git@ssh.dev.azure.com:v3/&lt;organization&gt;/\"]</code><code>insteadOf = https://dev.azure.com/</code> <p>{{&lt; admonition type=\"Info\" title=\"Organization in Dependency Path\" open=\"true\"&gt;}}</p> <p>This changes the path for dependencies to not require the organization in the dependency path. Instead, the import path will look like this: <code>import \"dev.azure.com/&lt;project&gt;/repo.git/subdirectory\"</code></p> <p>{{&lt; /admonition &gt;}}</p>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#https","title":"HTTPS","text":"<p>If you don't have restrictions on this, then you can do https with the following command to add the token in or use a more complex credential manager based process.</p> <pre><code>git config --global url.\"https://anythinggoeshere:$AZURE_DEVOPS_TOKEN@dev.azure.com\".insteadOf \"https://dev.azure.com\"\n</code></pre>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#azure-pipelines","title":"Azure Pipelines","text":"<p>If you run into timeout issues with <code>go get</code>, I found this solution worked well.</p> <p>I provided <code>ORGANIZATION</code> as a value if you are on the legacy url scheme, it's easier to just set this as variable and not worry about parsing out the org name itself from the url to place it in there. I got stuck on this recently and was pointed to the answer in this great article Using Go Modules With Private Azure Devops Repositories.</p> <pre><code>parameters:\n  - name: workingDirectory\n    type: string\n    default: $(Pipeline.Workspace)\n\nvariables:\n    - name: ORGANIZATION\n      value: myorg\nsteps:\n- checkout: self\n  fetchDepth: 0\n  path: $(Build.Repository.Name) # Note: you'll want to provide workingdirectory inputs for tasks if you have multi-repo checkout going on.\n- pwsh: |\n    git clone \"https://$(ORGANIZATION):$(System.AccessToken)@dev.azure.com/$(ORGANIZATION)/$(System.TeamProject)/_git/$(Build.Repository.Name)\"\n  displayName: git-checkout-with-pat\n# internal modules with go-get might fail without this.\n- pwsh: |\n    git config --global url.\"https://$(ORGANIZATION):$(System.AccessToken)@dev.azure.com\".insteadOf \"https://dev.azure.com\"\n  displayName: ensure-system-token-used-for-other-internal-repos\n- pwsh: |\n    Write-Host \"example, with working directory set\"\n  displayName: exampleTask\n  workingDirectory: ${{ parameters.workingDirectory }}/$(Build.Repository.Name)\n</code></pre>","tags":["azure-devops","golang","devops"]},{"location":"using-azure-devops-for-private-go-modules/#other-references","title":"Other References","text":"<ul> <li>SSH key usage in Azure Pipelines<sup>3</sup>.</li> <li>Using with docker<sup>2</sup></li> </ul> <ol> <li> <p>Go get command support in Azure Repos Git \u21a9</p> </li> <li> <p>Private Go Modules on Azure DevOps \u21a9\u21a9</p> </li> <li> <p>Install SSH Key task - Azure Pipelines | Microsoft Docs \u21a9</p> </li> </ol>","tags":["azure-devops","golang","devops"]},{"location":"go-r1-day-40/","title":"Go R1 Day 40","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-40/#progress","title":"progress","text":"<ul> <li>Learned a bit more modules vs packages, including internal package special behavior.</li> <li>Configured Azure DevOps remote module source with package level imports.</li> <li>Successfully migrated a utility logger for setup of Zerolog into a seperate remote module for importing.</li> <li>Setup 50% test coverage for logger using <code>gotest</code> test generator.</li> <li>Used Is for test setup. (this would be a cool use of gotest templates test generation)</li> <li>Modified sharedbrain forked repo to generate yaml front matter with dashes.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-40/#links","title":"links","text":"<ul> <li>GitHub - cweill/gotests: Automatically generate Go test boilerplate from your source code.</li> <li>GitHub - matryer/is: Professional lightweight testing mini-framework for Go.</li> <li>GitHub - sheldonhull/sharedbrain</li> <li>GitHub - dangoor/sharedbrain</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-41/","title":"Go R1 Day 41","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-41/#progress","title":"progress","text":"<ul> <li>Enabled Go code coverage for tests in VSCode</li> <li><code>go install github.com/jpoles1/gopherbadger@master</code> to install tooling for generating code coverage badge for readme.</li> <li>Set <code>![gopherbadger-tag-do-not-edit]()</code> in the readme, and then this gets replaced with a code coverage percentage badge.</li> <li>Generate the required code coverage reports using: <code>go test -coverprofile cover.out</code> followed by <code>go tool cover -html=cover.out -o coverage.html</code> for a visual report.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-41/#links","title":"links","text":"<ul> <li>GitHub - jpoles1/gopherbadger: Generate coverage badge images using Go!</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-42/","title":"Go R1 Day 42","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-42/#progress","title":"progress","text":"<ul> <li>Configured basic test case for GitHub - goyek/goyek: Create build pipelines in Go repo I created.</li> <li>GitHub - pterm/pterm used to provide progress on output as it installed linting tooling.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-42/#links","title":"links","text":"<ul> <li>GitHub - sheldonhull/goyek-tasks: Goyek pre-built tasks for CI/CD work</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-43/","title":"Go R1 Day 43","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-43/#progress","title":"progress","text":"<ul> <li>Modified forked SharedBrain repo to use <code>yaml</code> parser instead of toml.</li> <li>Modified tests handle invalid casting of interface, which was causing a panic.</li> </ul> <pre><code>otherDate, ok := otherDateInt.(time.Time)\nif !ok {\n  log.Printf(\"[time.Parse] probable invalid date format %s\", plainFilename)\n}\n</code></pre> <ul> <li>Improved tests to align to markdown standard formatting.</li> <li>FOSS license scanned on 4 repos to test compliance of licensing for badge.</li> <li>Use goyek templates to build out initial go based build actions.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-43/#links","title":"links","text":"<ul> <li>GitHub - sheldonhull/goyek-tasks: Goyek pre-built tasks for CI/CD work</li> <li>GitHub - sheldonhull/sharedbrain</li> <li>GitHub - dangoor/sharedbrain</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-44/","title":"Go R1 Day 44","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-44/#progress","title":"progress","text":"<ul> <li>Generated tests for Gorilla Mux and realized need more context and reading on how to properly test Gorilla Mux to avoid excessively mocking.</li> <li>Reviewed two different paradigms for struct methods, using parameters and using global configuration objects.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-44/#links","title":"links","text":"<ul> <li>go - Unit testing for functions that use gorilla/mux URL parameters - Stack Overflow</li> <li>Building and Testing a REST API in Go with Gorilla Mux and PostgreSQL - Semaphore</li> <li>Testing gorilla/mux handlers - Blog</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-45/","title":"Go R1 Day 45","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-45/#progress","title":"progress","text":"<ul> <li>Worked through merging flags and the \"run\" approach from Mat Ryer.</li> <li>Used ff for parsing.</li> </ul> <p>The resulting logic seems correct with main being very simple with:</p> <pre><code>package main\nimport(\n    \"io\"\n    \"flag\"\n    \"os\"\n    \"github.com/peterbourgon/ff/v3\"\n    \"github.com/peterbourgon/ff/v3/ffcli\"\n    \"github.com/peterbourgon/ff/v3/fftoml\"\n)\nconst (\n // exitFail is the exit code if the program\n // fails.\n exitFail           = 1\n)\n\n// main configuration from Matt Ryer with minimal logic, passing to run, to allow easier CLI tests\nfunc main() {\n if err := run(os.Args, os.Stdout); err != nil {\n  fmt.Fprintf(os.Stderr, \"%s\\n\", err)\n  os.Exit(exitFail)\n }\n}\n</code></pre> <p>The run function then handles the actual parsing:</p> <pre><code>func run(args []string, stdout io.Writer) error {\n if len(args) == 0 {\n  return errors.New(\"no arguments\")\n }\n  flags := flag.NewFlagSet(args[0], flag.ExitOnError)\n flag.BoolVar(&amp;debug, \"debug\", false, \"sets log level to debug\")\n\nif err := ff.Parse(flags, args,\n  ff.WithEnvVarNoPrefix(),\n  ff.WithConfigFileFlag(\"config\"),\n  ff.WithConfigFileParser(fftoml.Parser),\n ); err != nil {\n  return err\n }\n if debug {\n  logLevel = \"debug\"\n }\n  // proceed with initialization of logger and other tools\n  return nil\n</code></pre> <p>I like this approach, as the examples by Mat show how you can end up testing the inputs and variations on flags as well. The example from his blog post showed how easy it became with:</p> <pre><code>err := run([]string{\"program\", \"-v\", \"-debug=true\", \"-another=2\"})\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-45/#links","title":"links","text":"<ul> <li>Why you shouldn't use func main in Go by Mat Ryer - PACE.</li> <li>ff \u00b7 pkg.go.dev</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-46/","title":"Go R1 Day 46","text":"","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-46/#progress","title":"progress","text":"<ul> <li>At <code>$work</code> I focused on dockertest.</li> <li>Modified the provided test statements to ensure autoremoval of container occurred on failure.</li> <li>Had packet issues I couldn't figure out this time, so shelved for now. <code>packets.go:37: unexpected eof</code></li> <li>At home, I played with bubbleteam a bit, and decided while an epic TUI interface, the framework is far too involved for what I want to mess around with at this time. For instance, it doesn't provide multi-select, instead much of that is manually written, requiring a lot of effort. I'll look at another framework or <code>go-prompt</code> again, and just use something that provides selections out of the box.</li> <li>Further refined some goyek build statements, running docker compose multi-file based merging of docker-compose files enabled.</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-46/#links","title":"links","text":"<ul> <li>charmbracelet/bubbletea</li> <li>ory/dockertest</li> </ul>","tags":["100DaysOfCode","golang","microblog"]},{"location":"understanding-the-basics-of-sql-server-security/","title":"Understanding The Basics of SQL Server Security","text":"","tags":["tech","development","sql-server","security"]},{"location":"understanding-the-basics-of-sql-server-security/#confusing","title":"Confusing","text":"<p>As I've worked with folks using other database engines, I've realized that Microsoft SQL Server has some terminology and handling that is a bit confusing. Here's my attempt to clarify the basics for myself and others needing a quick overview. This is not comprehensive coverage of security architecture, which is a very complex topic, more just terminology.</p>","tags":["tech","development","sql-server","security"]},{"location":"understanding-the-basics-of-sql-server-security/#terminology","title":"Terminology","text":"<p>Note that it's best to consider SQL Server as it's own operating system, not just a standard application running. It has its own memory manage, cpu optimization, user security model, and more. It's helpful in understanding why a <code>Server Login != Instance Login</code> by reviewing common terminology. I've noticed that among other open-source tools like MySQL, it's much more common to hear terms like \"Database Server\", which in my mind mix up for non-dbas the actual scope being talked about.</p> Term Definition Server The operating system Instance The SQL Server Instance that can contain 1 or many databases Database The database inside the instance. <p>This can be 1 or many.</p> Term Definition Server Login Windows or Linux user at the Operating System level SQL Login Login created inside SQL Server, using SQL statement. This is internal to SQL Server and not part of the Server OS. Database User A database user is created and linked to the Instance SQL Login Server Role Roles for Instance level permissions, such <code>sysadmin (sa)</code>, <code>SecurityAdmin</code>, and others. These do not grant database-level permissions, other than <code>sa</code> having global rights. Database Role A defined role that grants read, write, or other permissions inside the database. <p>Here's a quick visual I threw together to reinforce the concept.</p> <p>Yes, I'm a talented comic artist and take commissions. \ud83d\ude00</p> <p></p>","tags":["tech","development","sql-server","security"]},{"location":"understanding-the-basics-of-sql-server-security/#best-practice","title":"Best Practice","text":"<p>When managing user permissions at a database level, it's best to leverage Active Directory (AD) groups. Once this is done, you'd create roles. The members of those roles would be the AD Groups.</p>","tags":["tech","development","sql-server","security"]},{"location":"understanding-the-basics-of-sql-server-security/#no-active-directory","title":"No Active Directory","text":"<p>SQL Logins and corresponding database users must be created if active directory groups aren't being used.</p>","tags":["tech","development","sql-server","security"]},{"location":"understanding-the-basics-of-sql-server-security/#survey-said","title":"Survey Said","text":"<p>I did a quick Twitter survey and validated that Active Directory Groups are definitely the most common way to manage.</p> <p>As a SQL Server dba, how do you grant access to less privileged devs, including production? I'm curious. I've been part of both AD managed environments and ones where I did everything with SQL Login auth. #sqlfamily #sqlserver #mssql </p>\u2014 Sheldon Hull (@sheldon_hull) June 24, 2021","tags":["tech","development","sql-server","security"]},{"location":"go-r1-day-47/","title":"Go R1 Day 47","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-47/#progress","title":"progress","text":"<ul> <li>worked with lefthook to run golang-lint tools and discovered how vast the configuration for this was, simplifying many of the other linting tools that was running seperately.</li> <li>created some goyek tasks for bringing up and down docker compose stacks.</li> <li>learned that <code>file.Close()</code> shouldn't be used with defer to avoid unsafe file handling operations.</li> </ul> <pre><code>// taskComposeDestroy tears down the docker stack including volumes\n// this is using goyek task framework for Make alternative\nfunc taskComposeDestroy() goyek.Task {\n  return goyek.Task{\n    Name:  \"compose-destroy\",\n    Usage: \"remove stack with prejudice\",\n    Action: func(tf *goyek.TF) {\n      dcBase := filepath.Join(BuildRoot, \"docker\", \"docker-compose.myservice.yml\")\n      dcMySQL := filepath.Join(BuildRoot, \"docker\", \"docker-compose.mysql.yml\")\n      compose := tf.Cmd(\"docker\", \"compose\", \"-f\", dcBase, \"-f\", dcMySQL, \"down\", \"--volumes\", \"--remove-orphans\")\n      if err := compose.Run(); err != nil {\n        tf.Fatalf(\":heavy_exclamation_mark: docker-compose down failed: [%v]\", err)\n      }\n      tf.Log(\":white_check_mark: docker-compose ran successfully\")\n    },\n  }\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"simplify-aws-developer-security-with-leapp/","title":"Simplify Aws Developer Security With Leapp","text":"","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#security-is-necessary-work","title":"Security Is Necessary Work","text":"<p>Security can add overhead, get in the way, complicate development, generally can cause challenges to development efforts. The balance of security and usability is one that all development organizations have to deal with.</p> <p>I could be ultra-secure by requiring carrier pigeon multi-factor authentication for my team, but I'm not sure that would be well received.</p> <p></p> <p>It's also critical and the responsibility of each developer, not just the security team.</p> <p>Let's take a few simple examples where a local developer can compromise security in AWS by simply taking an easy path.</p> <ul> <li>Builds a docker container and uploads to Docker Hub, for a generic tool, but includes embedded AWS credentials for a linter in it.</li> <li>Uses IAM long-lived keys to use a tool to generate infrastructure as code and commits these to the repository due to not excluding the <code>.env</code> file in the <code>.gitignore</code>.</li> <li>Uses IAM access keys that are highly privileged as the easy path (over narrowing permission scope), puts these in a vendor site that integrates with AWS, and forgets about it. Vendor has data compromised and the AWS keys are exposed.</li> </ul> <p>There are so many ways that using long-lived IAM credentials can be abused.</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#assume-roles","title":"Assume Roles","text":"<p>It's recommended to use Role Assumption instead of IAM Access keys when possible.</p> <p>This means all the tooling will leverage AWS Security Token Service and use short-lived ephemeral scoped credentials.</p> <p>This limits both the blast radius and time of any compromise.</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#security-as-the-path-of-least-resistance","title":"Security As The Path Of Least Resistance","text":"<p>The problem with this path, is one of effort.</p> <ul> <li>If using AWS SSO: You'd have to<ul> <li>login to your portal</li> <li>open target account section</li> <li>get programmatic credentials for the account</li> <li>paste them into your <code>~/.aws/credentials</code> file</li> </ul> </li> <li>If not using SSO, you'd have to use the CLI or appropriate sdk to generate your credentials on demand and then set.</li> </ul> <p>The key is that all of these approaches aren't easy as the default.</p> <p>It's easier to just use an IAM key and move on rather than battle with cli, sdks, and other options.</p> <p>Tools like aws-vault are fantastic in helping with this, but aren't intuitive, work the same way in different platforms, and overall still provide a barrier to making it the easiest path.</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#leapp","title":"Leapp","text":"<p>\ud83d\ude80 Leapp - One step away from your Cloud</p> <p></p> <p>I came across this app by chance back in March when I was just starting at my new job at <code>$Work</code> and was looking for a tool to help simplify things over <code>aws-vault</code>. Since I was benefiting from AWS SSO, I was looking for a way to simplify switching roles with SSO, while still maintaining compatibility with the lowest common denominator of the lowly <code>~/.aws/credentials</code> file that I knew worked with Terraform, PowerShell, and other tools equally.</p> <p>The result? Shipped code on day 3!</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#what-it-provides","title":"What It Provides","text":"<p>Leapp allows the path of least resistance to best practices with AWS Credentials (as well as Azure). With AWS SSO, you get the benefit of the portal accounts automatically populated as well. Role assumption becomes the easy path.</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#how-it-works","title":"How It Works","text":"<p>In my scenario, using AWS SSO provides a more complicated route that demonstrates the ease of use.</p> <ul> <li>Open App</li> <li>Click on Account &amp; Role I want to assume, say \"Dev Account - AWS Admininstrator\"</li> <li>Pop-up for SSO process using whatever SSO process is setup</li> <li>Enter name and password</li> <li>Enter MFA</li> <li>Auth dialogue proceeds to confirmation screen of signing in CLI</li> </ul> <p></p> <ul> <li>Done!</li> </ul> <p>The result:</p> <p><code>~/.aws/credentials</code> has the <code>[default]</code> profile containing my STS credentials. This token rotates every <code>n</code><sup>1</sup> minutes producing ephemeral credentials.</p> <p></p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#enhancements","title":"Enhancements","text":"","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#named-profiles","title":"Named Profiles","text":"<p>The <code>0.6.0</code> release that should be out by Jun 30th (I'm using pre-release preview) contained some nice enhancements to this.</p> <p>In prior versions, <code>[default]</code> profile was just replaced with whatever account you selected.</p> <p>Now, named profiles are supported as well, so you could have multiple assumed roles open at the same time for calling with named profiles.</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#ssm-sesssion-launcher","title":"SSM Sesssion Launcher","text":"<p>There's a built-in session launcher, allowing searching of EC2 instances and the instance terminal launch of an <code>aws ssm start-session</code> command against the target.</p> <p></p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#future","title":"Future","text":"<p>The future for Leapp is an interesting question.</p> <p>They are active in engaging for feedback, so this is built with actual user feedback, not in a bubble. I had an hour call with one of the developers talking about authentication approaches that helped me appreciate how much they want to engage their uses in a true Agile development pattern. Not only did I learn a bunch, but ended up being able to provide a few ideas for future consideration. Great discussion with @andreacavagna01. Give Andrea a follow on :(fab fa-twitter): twitter and :(fab fa-github): GitHub!</p> <p>I can see a larger market than developers if a company rolled it out to users, as it would simplify access by analysts, developers, security team members, and others.</p> <p>It's not:</p> <ul> <li>An \"essential\" service</li> <li>\"Necessary\" in a strict sense, because things work fine without this tool.</li> </ul> <p>Instead it offers:</p> <ul> <li>A usability improvement to developers authentication workflow.</li> <li>The easy path is a secure path</li> </ul> <p>For this reason, I'm interested to see how adoption by enterprises and organizations will occur. I've observed that solving painpoints can both incentive a company if painful enough to adopt new approaches, but also bypass improvements as they aren't \"essential\" and things already work, albeit unoptimally.</p> <p>For me, I'm adding it to my AWS essentials, since now I can just focus on work without and improve my credentials handling without it intruding. I'll continue recommending this to folks, and keep up with what improvements continue to be released from this team.</p> <p>Keep up with the Leapp roadmap here: GitHub Leapp Roadmap</p>","tags":["tech","development","security","aws","devops"]},{"location":"simplify-aws-developer-security-with-leapp/#resources","title":"Resources","text":"Resource Access management for AWS resources - AWS Identity and Access Management Resources to learn more about IAM - AWS Identity and Access Managemen Using IAM roles - AWS Identity and Access Management <ol> <li> <p>I believe 10-20 right now\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","security","aws","devops"]},{"location":"luminar-ai/","title":"Luminar AI","text":"<p>{{&lt; admonition type=\"Note\" title=\"Update\" open=\"true\"&gt;}}</p> <p>Customer service bumped my older version of Luminar 3 to 4. The issues with distortion on the preview screen aren't there with version 4.</p> <p>The new AI enhancements aren't there, so it's missing some of the new features. For now I'm going to try and use this as my daily driver for images and see how it goes.</p> <p>Positive experience so far, though I still miss the fluid experience of Lightroom's preview modes.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#my-credentials","title":"My Credentials","text":"<p>None. \ud83d\ude00</p> <p>I'm a software engineer working with AWS mostly.</p> <p>I'd be considered a \"prosumer\" with video and photography, in that I use high quality gear when possible, have comfort with professional video editing software, and have used Photoshop, Lightroom, Gimp, and other tools of a similar nature.</p> <p>This is a quick review from someone who found Lightroom to overall a freeing experiencing in photography workflow.</p> <p>For my occasional usage, I'm not excited to pay the subscription fee, so my Lightroom 6 standalone license is all I've continued with.</p>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#scope","title":"Scope","text":"<ul> <li>Casual impressions only.</li> <li>This is not in-depth review from a photography specialist.</li> <li> <p>My focus is:</p> <ul> <li>user experience</li> <li>application stability</li> <li>lock-in</li> <li>library management (the one thing Lightroom seems to do better than any other app I've tried)</li> </ul> </li> </ul>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#first-look","title":"First Look","text":"<p>The very first look of the library view was pretty bad.</p> <p>I'm not certain what's going on, but the raw file previews in the library view loaded in a way that made my daughter look like she was being viewed through one of those warping mirrors.</p> <p></p> <p>The library management workflow was pretty snappy, so I was overall happy with that.</p> <p>I do miss Lightroom Survey view, but no other app has had that kind of compare interface that I've discovered so far.</p>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#templates","title":"Templates","text":"<p>The template presets for portrait did well out of the gate.</p> <p>I was even able to enhance the eyes and change the color of the eyes completely with a few clicks.</p> <p>A few preset adjustments such as \"Face AI\" allowed editing the light on the face to help brighten just the face without any need to mask.</p>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#editing","title":"Editing","text":"<p>\ud83d\udd33 Many of the AI powered tools were gimmicky to me.</p> <p>\ud83d\udd33 Mystical, Glow, Dramatic, all tended to be pretty over the top when max, though perhaps a little touch of each might work.</p> <p>\u2705 The composition auto leveler was nice.</p> <p>\u2705 Enhance tool was subtle and tasteful</p> <p>\u2705 The ability to apply a preset then jump tweaking the presets applied changes was intuitive and well done.</p> <p>\u2705 The erase tool did great at removing a sign from a brick wall with seconds of effort.</p>","tags":["software","ramblings","photograophy"]},{"location":"luminar-ai/#overall-verdict","title":"Overall Verdict","text":"<p>Overall good quick intuitive workflow.</p> <p>Value propisition is more difficult to determine.</p> <p>I had Luminar 2018 and Luminar 3. Both weren't a great experience on the PC and never really got much value from them.</p> <p>I could see this being a viable raw editing workflow for me though, considering the overall license cost would be significantly less than many other competitors.</p> <p></p>","tags":["software","ramblings","photograophy"]},{"location":"go-r1-day-48/","title":"Go R1 Day 48","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-48/#progress","title":"progress","text":"<ul> <li>worked through more test concepts with learn-go-with-tests, including test helpers.</li> <li>enjoyed the results GitHub copilot was producing. I can see a lot of promise for speeding up things and reducing the need for snippets as it builds code suggestions on the fly with far more context than a general snippet could do.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-48/#links","title":"links","text":"<ul> <li>\u2705 completed Hello, World - Learn Go with tests</li> <li>feat: v1-hello world \u00b7 sheldonhull/learn-go-with-tests-applied@40ca675 \u00b7 GitHub</li> <li>refactor: move hello-world out of version folders \u00b7 sheldonhull/learn-go-with-tests-applied@489e05a \u00b7 GitHub</li> <li>feat: hello world with function \u00b7 sheldonhull/learn-go-with-tests-applied@1b70ff0 \u00b7 GitHub</li> <li>feat: adjust hello world to use constants \u00b7 sheldonhull/learn-go-with-tests-applied@f58f776 \u00b7 GitHub</li> <li>feat: include t.Helper() function \u00b7 sheldonhull/learn-go-with-tests-applied@d61c2ff \u00b7 GitHub</li> <li>feat: complete hello world with greetingPrefix and test \u00b7 sheldonhull/learn-go-with-tests-applied@4a13c61 \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-49/","title":"Go R1 Day 49","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-49/#progress","title":"progress","text":"<ul> <li>Learned about White-Box vs Black-Box testing. Apparently, you can access all indentifiers of a package if you use the same package name such as: <code>package packagename</code>. If you are testing as a consumer might be, then you can use <code>package packagename_test</code> for only accessing the exported identifiers.</li> <li>Used examples in test file to provide self-documentation of how to use the method.</li> <li>Worked further with <code>golanglint-ci</code> and found it challenging when working with multiple modules in subdirectories. The go eco system systems simplest with one repo = one module. While mono-repos can work, the CI tooling isn't quite as intuitive to setup. VSCode requires experimental support for multiple modules as well at this time.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-49/#links","title":"links","text":"<ul> <li>Integers</li> <li>Five suggestions for setting up a Go project \u2013 The acme of foolishness</li> <li>Proper package naming for testing with the Go language - Stack Overflow</li> <li>The Go Programming Language Specification - The Go Programming Language</li> <li>chore: precommit hooks and tooling \u00b7 sheldonhull/learn-go-with-tests-applied@de42a55 \u00b7 GitHub</li> <li>test: \u2705 integers minimal test code \u00b7 sheldonhull/learn-go-with-tests-applied@37923e7 \u00b7 GitHub</li> <li>test: \ud83c\udf89 finish integer testing \u00b7 sheldonhull/learn-go-with-tests-applied@92691f0 \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"how-to-reduce-noise-from-slack/","title":"How To Reduce Noise From Slack","text":"","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#noise","title":"Noise","text":"<p>Slack can be a really effective tool with asynchronous work, but the defaults aren't great out of the box.</p> <p>I want to get you a few tips on how to use Slack effectively.</p> <p>Piping through release notifications, work-item updates, alerts and more can help you reduce context switching with other tools, but without proper control you'll likely find it overwhelming.</p>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#sections","title":"Sections","text":"<p>Use sections to organize your content and customize the level of priority you want to assign to the grouped channels.</p> <p>This is for paid plans. This guide assumes you are on company plan with those features.</p> <p></p>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#individual-channel-settings","title":"Individual Channel Settings","text":"<p>Reduce noise from busy channels, especially when folks over-use <code>@here</code></p> <p></p> <p>Configure settings (especially in automated or busy rooms) to:</p> <ul> <li>mute notifications</li> <li>mute <code>@here</code> if this is not properly used in the room.</li> <li>You'll still get notified if your name is mentioned, but otherwise the channel won't keep showing up as needing your attention.</li> </ul>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#change-section-behavior","title":"Change Section Behavior","text":"<ul> <li>Sort by recent activity</li> <li>Set your section to only show unreads + sort by recent updates. This will keep your sidebar very simple and clean, autohiding after it's been read.</li> </ul>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#flag-keywords","title":"Flag Keywords","text":"<p>If someone forgets to mention your name with the <code>@Me</code> syntax, you can set your name as a keyboard to alert on as a backup.</p> <p>I set <code>sheldon</code> as a keyword, and it helps ensure I get notified even if the alert, message, response didn't properly format my name in the message or by the app integrations (very few map to user id properly).</p>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#use-all-unreads","title":"Use All Unreads","text":"<p>From your settings for the sidebar, enable the <code>All Unreads</code> section. This can help you quickly review all channel activity in a single pane similar to an email inbox.</p>","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#shortcuts","title":"Shortcuts","text":"<p>A couple basic shortcuts will set you up to use Slack effectively.</p> <p>For Windows, typically replace  <code>cmd</code> with <code>ctrl</code>.</p> Action Keyboard <code>cmd+k</code> Quick switcher for channels and conversations. Don't leave anything pinned you don't need to by using this to flip <code>cmd+/</code> Keyboard shortcut reference panel <code>cmd+left</code> <code>cmd+right</code> Navigate similar to a web browser back or forward to whatever converation or channel you were looking at. <code>cmd+up</code> edit the last message (if you are focused in the textbox) <code>option+shift+down</code> go to next unread channel (or use allunreads)","tags":["slack","development"]},{"location":"how-to-reduce-noise-from-slack/#downtime","title":"Downtime","text":"<p>Make sure to update your notification window to allow for uninteruppted deep work.</p>","tags":["slack","development"]},{"location":"go-r1-day-50/","title":"Go R1 Day 50","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-50/#progress","title":"progress","text":"<ul> <li>evaluate package level logging variables and how to properly use them.</li> <li>tested best way to abstract the common logging to avoid run time dependency with package level variables and impact to tests.</li> <li>read through Effective Go - The Go Programming Language - embedding</li> </ul> <p>At this point, I'm still struggling with the proper way to abstract a logging wrapper that calls a logging library. There's enough boilerplate for setup of my preferred defaults in zerolog that I want to include a wrapper to organize this and return the logger.</p> <p>This tends to look like:</p> <pre><code>type Logger struct {\n  Logger *zerolog.Logger\n}\n</code></pre> <p>This results in a pretty lengthy call with <code>logger.Logger.Info().Str(\"key\", \"value\").Msg(\"message\")</code>. I'm also having issues with the embedded logger not returning the correct methods transparently back to the caller.</p> <p>I've tested with <code>internal/logger</code> and <code>pkg/logger</code> with similar issues. This one I'll have to come back round to.</p> <ul> <li>Also worked a little on GitHub - sheldonhull/go-semantic-sentences: Use semantic line breaks with markdown files.</li> <li>This resulted in learning a bit of regex with Go. I found it a bit confusing intially setting up my tests, since it seems to be matching more than I'd expect. Will come back around to this soon.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-51/","title":"Go R1 Day 51","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-51/#progress","title":"progress","text":"<ul> <li>Did iteration exercise, however, I skipped ahead and did <code>strings.Repeat</code> instead of iteration because I'm lazy. \ud83d\ude00</li> <li>Moved all tests into blackbox test packages.</li> <li>Worked through variadiac functions.</li> <li>Tweaked my VSCode autotest to run on save.</li> <li>Further tweaks to <code>golanglint-ci</code> to reduce noise on linting checks.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-51/#links","title":"links","text":"<ul> <li>Iteration</li> <li>Power Up Testing</li> <li>5 Simple Tricks</li> <li>Moq (didn't use yet, but placeholder for this)</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-51/#commits","title":"Commits","text":"<ul> <li>fix: use blackbox testing \u00b7 sheldonhull/learn-go-with-tests-applied@165ef15 \u00b7 GitHub</li> <li>feat: \u2705 array sum using for loop \u00b7 sheldonhull/learn-go-with-tests-applied@6217838 \u00b7 GitHub</li> <li>refactor: \u2705 simplify array sum using range \u00b7 sheldonhull/learn-go-with-tests-applied@1377e11 \u00b7 GitHub</li> <li>refactor: \u2705 modify array to use slices for flexible input sizes \u00b7 sheldonhull/learn-go-with-tests-applied@bea49fc \u00b7 GitHub</li> <li>test: \u2705 add test case with new input size for slice \u00b7 sheldonhull/learn-go-with-tests-applied@9db5d41 \u00b7 GitHub</li> <li>test: \u2705 add sumall and test conditions \u00b7 sheldonhull/learn-go-with-tests-applied@b3ccc10 \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"use-driftctl-to-detect-infra-drift/","title":"Use Driftctl to Detect Infra Drift","text":"<p>Use Driftctl to detect drift in your your infrastructure. This snippet generates a html report to show coverage and drift figures of the target.</p> <p>For multiple states, you'll need to adapt this to provide more <code>--from</code> paths to ensure all state files are used to identify coverage.</p> <pre><code>$S3BucketUri = \"terraform-states-$AWS_ACCOUNT_NUMBER/$AWS_REGION/$TERRAFORMMODULE/terraform.tfstate\"\n$Date = $(Get-Date -Format 'yyyy-MM-dd-HHmmss')\n$ArtifactDirectory = (New-Item 'artifacts' -ItemType Directory -Force).FullName\n&amp;docker run -t --rm `\n    -v ${PWD}:/app:rw `\n    -v \"$HOME/.driftctl:/root/.driftctl\" `\n    -v \"$HOME/.aws:/root/.aws:ro\" `\n    -e \"AWS_PROFILE=default\" ` # Replace this with your aws profile name if you have multiple profiles\n    cloudskiff/driftctl scan --from \"tfstate+s3://$S3BucketUri\" --output \"html://$ArtifactDirectory/driftctl-report-$Date.html\"\n</code></pre> <p>Optionally, you can adjust to recursively scan the state file of an entire bucket (say if using Terragrunt to store in special key prefixes).</p> <ul> <li>Change to <code>--from \"tfstate+s3://mybucket/myprefix\"</code> without requiring the full path to a single tfstate file.</li> <li>Recursively search if in many subfolders with: <code>**/*.tfstate</code>.</li> </ul>","tags":["tech","development","microblog","terraform","infrastructure-as-code"]},{"location":"go-r1-day-52/","title":"Go R1 Day 52","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-52/#progress","title":"progress","text":"<ul> <li>published extension pack for Go<sup>1</sup></li> <li>Learned about magic number linter in <code>golanglint-ci</code>. For instance this would be flagged as a bad practice (while not applicable for a simple test like this, having a const makes sense in almost all other cases).</li> </ul> <pre><code>func Perimeter(width float64, height float64) float64 {\n    return 2 * (width + height)\n}\n</code></pre> <ul> <li>Learned a few extra linter violations and how to exclude including:<ul> <li><code>lll</code>: for maximum line length</li> <li><code>packagetest</code>: for emphasizing blackbox testing.</li> <li><code>gochecknoglobals</code>: for ensuring global variables aren't used</li> <li><code>nlreturn</code>: for returning without a black line before. That's a \"nit\", but nice for consistency (though I'd like to see this as an autoformatted rule with fix applied.)</li> </ul> </li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-52/#links","title":"links","text":"<p>feat: structs-methods-and-interfaces -&gt; initial functions without str\u2026 \u00b7 sheldonhull/learn-go-with-tests-applied@be9ce01 \u00b7 GitHub</p> <ol> <li> <p>Extension Pack for Go \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"my-first-vscode-extension-pack-for-go/","title":"My First Vscode Extension Pack for Go","text":"<p>Took a swing at creating my own extension pack for Go.</p> <p>:(fab fa-github fa-fw):  sheldonhull/extension-pack-go - GitHub</p> <p>This was a good chance to familarize myself with the eco-system and simplify sharing a preset group of extensions.</p> <p>Setup the repo with a <code>Taskfile.yml</code> to simplify running in the future. If frequent updates needed to happen, it would be easy to plug this into GitHub actions with a dispatch event and run on demand or per merge to main.</p> <p>Here's the marketplace link if you want to see what it looks like: Marketplace - extension-pack-go</p> <p>I could see this process being improved in the future with GitHub only requirements. At this time, it required me to use my personal Azure DevOps org to configure access and publishing.</p>","tags":["tech","development","microblog","vscode","golang"]},{"location":"my-first-vscode-extension-pack-for-go/#resources","title":"Resources","text":"<p>Publishing Extensions</p>","tags":["tech","development","microblog","vscode","golang"]},{"location":"go-r1-day-53/","title":"Go R1 Day 53","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-53/#progress","title":"progress","text":"<ul> <li>Troubleshooting on: <code>typechecking loop</code> that helped me learn a bit more on how the compiler parsing occurs. Quick fix was to simply change <code>var logger *logger.Logger</code> to <code>var log *logger.Logger</code>.</li> <li>Read up on dependency injection concepts and clean architecture design.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-53/#links","title":"links","text":"<ul> <li>Demystifying 'typechecking loop' errors in Go</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"steampipe-is-sql-magic/","title":"Steampipe Is Sql Magic","text":"","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#up-and-running-in-minutes","title":"Up And Running In Minutes","text":"<p>I tried Steampipe out for the first time today.</p> <p>:(fab fa-twitter fa-fw): Follow Steampipe On Twitter</p> <p>I'm seriously impressed.</p> <p>I built a project go-aws-ami-metrics last year to test out some Go that would iterate through instances and AMIs to build out aging information on the instances.</p> <p>I used it to help me work through how to use the AWS SDK to iterate through regions, instances, images, and more.</p> <p>In 15 mins I just solved the equivalent issue in a way that would benefit anyone on a team. My inner skeptic was cynical, thinking this abstraction would be problematic and I'd be better served by just sticking with the raw power of the SDK.</p> <p>It turns out this tool already is built on the SDK using the same underlying API calls I'd be writing from scratch.</p> <p>First example: DescribeImage</p> <p>This is the magic happening in the code.</p> <pre><code>    resp, err := svc.DescribeImages(&amp;ec2.DescribeImagesInput{\n        Owners: []*string{aws.String(\"self\")},\n    })\n    for _, image := range resp.Images {\n        d.StreamListItem(ctx, image)\n    }\n</code></pre> <p>This is the same SDK I used, but instead of having to build out all the calls, there is a huge library of data already returned.</p> <pre><code>    req, publicImages := client.DescribeImagesRequest(&amp;ec2.DescribeImagesInput{\n        Filters: []*ec2.Filter{\n            {\n                Name:   aws.String(\"is-public\"),\n                Values: []*string{aws.String(\"true\")},\n            },\n        },\n    },\n    )\n</code></pre> <p>There is no need to reinvent the wheel. Instead of iterating through regions, accounts, and more, Steampipe allows this in plain old SQL.</p> <p> For example, to gather:</p> <ul> <li>EC2 Instances</li> <li>that use AWS Owned Images</li> <li>and use an image that created greater than <code>n</code> period</li> <li>and want the aging in days</li> </ul> <pre><code>SELECT\n    ec2.instance_id,\n    ami.name,\n    ami.image_id,\n    ami.state,\n    ami.image_location,\n    ami.creation_date,\n    extract('day' FROM now()) - extract('day' FROM ami.creation_date) AS creation_age,\n    ami.public,\n    ami.root_device_name\nFROM\n    aws_ec2_ami_shared AS ami\n    INNER JOIN aws_ec2_instance AS ec2 ON ami.image_id = ec2.image_id\nWHERE\n    ami.owner_id = '137112412989'\n  AND ami.creation_date &gt; now() - INTERVAL '4 week'\n</code></pre> <p>There are plugins for GitHub, Azure, AWS, and many more.</p> <p>You can even do cross-provider calls.</p> <p>Imagine wanting to query a tagged instance and pulling the tag of the work item that approved this release. Join this data with Jira, find all associated users involved with the original request, and you now have an idea of the possibility of cross-provider data Steampipe could simplify.</p> <p>Stiching this together is complicated. It would involve at least 2 SDK's and their unique implementation.</p> <p>I feel this is like Terraform for Cloud metadata, a way to provide a consistent experience with syntax that is comfortable to many, without the need to deal with provider specific quirks.</p>","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#query-in-editor","title":"Query In Editor","text":"<ul> <li>I downloaded the recommended TablePlus with <code>brew install tableplus</code>.</li> <li>Ran <code>steampipe service start</code> in my terminal.</li> <li>Copied the Postgres connection string from the terminal output and pasted into TablePlus.</li> <li>Pasted my query, ran, and results were right there as if I was connected to a database.</li> </ul>","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#aws-already-has-this","title":"AWS Already Has This","text":"<p>AWS has lots of ways to get data. AWS Config can aggregate across multiple accounts, SSM can do inventory, and other tools can do much of this.</p> <p>AWS isn't easy. Doing it right is hard. Security is hard.</p> <p>Expertise in building all of this and consuming can be challenging.</p> <p>\ud83c\udf89 Mission accomplished!</p>","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#experience","title":"Experience","text":"<p>I think Steampipe is offering a fantastic way to get valuable information out of AWS, Azure, GitHub, and more with a common language that's probably the single most universal development language in existenance: SQL.</p> <p>One of the goals of Steampipe since we first started envisioning it is that it should be simple to install and use - you should not need to spend hours downloading pre-requisites, fiddling with config files, setting up credentials, or pouring over documentation. We've tried very hard to bring that vision to reality, and hope that it is reflected in Steampipe as well as our plugins.</p> <p>Providing a cli with features like this is incredible.</p> <ul> <li>execute</li> <li>turn into an interactive terminal</li> <li>provide prompt completion to commands</li> <li>a background service to allow connection via IDE</li> </ul>","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#the-future","title":"The Future","text":"<p>The future is bright as long as <code>truncate ec2_instance</code> doesn't become a thing. \ud83d\ude00</p>","tags":["tech","development","aws","golang"]},{"location":"steampipe-is-sql-magic/#further-resources","title":"Further Resources","text":"<p>If you want to explore the available schema, check out the thorough docs.</p> <ul> <li>AWS Tables List shows <code>212</code> tables of metadata currently available.</li> <li>Use Named queries to build a library of easy queries to call on demand such as <code>ami.aging_instances</code>.</li> <li>Use Mods to download a set of named queries and controls to validate things like security and compliance.</li> <li>Query multiple connections at once</li> <li>Design Principles</li> </ul>","tags":["tech","development","aws","golang"]},{"location":"go-r1-day-54/","title":"Go R1 Day 54","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-54/#progress","title":"progress","text":"<ul> <li>worked with tests and Goland.</li> <li>Modified table driven tests to remove hard coded test case inputs.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-55/","title":"Go R1 Day 55","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-55/#progress","title":"progress","text":"<p>In Go, when you call a function or a method the arguments are copied. <sup>1</sup></p> <ul> <li>Built some test cases for working with pointers and test methods.</li> <li>Did this in Goland to become more familar with it and figured out how to use the nice tests explorer with a floating window to auto-test.</li> <li>Built a filewatcher configuration watcher as alternative to tests panel, allowing automated run in terminal or output panel of <code>go test ./...</code>.</li> <li>Couldn't figure out how to run every test in project, as it's a multi-module repo (for tutorials). VSCode works fine with this with the multi-module support.</li> </ul> <p>These pointers to structs even have their own name: struct pointers and they are automatically dereferenced.  <sup>1</sup></p> <p>This explains something that felt a little confusing as a new Gopher. Now I know why returning a value back as an int didn't require explicit dereferencing. <sup>2</sup></p> <pre><code>func (w *Wallet) Balance() int {\n  return w.balance // &lt;---- automatically deferenced\n}\n</code></pre> <p>Not clear yet on if I need to set the <code>is := New(t)</code> in the context of each <code>t.Run()</code> or not.</p> <pre><code>t.Run(\"withdraw 2 amounts\", func(t *testing.T) {\n        wallet := pointers.Wallet{}\n        wallet.Deposit(pointers.Bitcoin(20))\n        err := wallet.Withdraw(pointers.Bitcoin(5))\n        is.NoErr(err) // Withdraw should have no errors\n        assertBalance(t, &amp;wallet, pointers.Bitcoin(15))\n    })\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-55/#links","title":"links","text":"<ul> <li>feat: \u2705 add pointer test progress \u00b7 sheldonhull/learn-go-with-tests-applied@f89a206 \u00b7 GitHub</li> <li>tests: \ud83e\uddea refactor tests for bitcoin wallet and run configurations \u00b7 sheldonhull/learn-go-with-tests-applied@9f27fe0 \u00b7 GitHub</li> <li>tests: \ud83e\uddea refactor tests for bitcoin wallet and improve final test cov\u2026 \u00b7 sheldonhull/learn-go-with-tests-applied@1f3db9f \u00b7 GitHub</li> </ul> <ol> <li> <p>Pointers \u21a9\u21a9</p> </li> <li> <p>Method Values \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"my-goland-log/","title":"My Goland Log","text":"","tags":["tech","development","goland","golang"]},{"location":"my-goland-log/#purpose","title":"Purpose","text":"<p>This is a log of my second journey in using Goland from Jetbrains for Go development.</p> <p>I've got VSCode in a great state. It's flexible, powerful, and I've highly customized it to my workflow.</p> <p>However, doing Go development, I'd like to better explore Jetbrains Goland and see if the experience proves positive.</p> <p>I'll log updates and issues here as I work through it in the hope that it might provide you better information if you are considering Goland as well.</p> Emoji Definition \ud83d\udd34 Not solved yet \ud83d\udfe0 Issue or behavior unclear \ud83d\udea9 It's an issue I believe I understand but don't like the answer to \u2705 Understand and no issues with behavior","tags":["tech","development","goland","golang"]},{"location":"my-goland-log/#goland-log","title":"Goland Log","text":"<p>{{&lt; admonition type=\"Question\" title=\":(fas fa-vials): Testing\" open=false &gt;}}</p> <p>\ud83d\udd34 Not certain yet how to run multi-module repo based tests through project (VSCode supports with experimental flag).</p> <p>\u2705 Run individual directory test. This also generates a run configuration (ephemeral) that can be persisted as a test configuration for the project.</p> <p></p> <p></p> <p>\u2705 Toggle auto-run for testing is possible, so upon save, it will run after a 1 second delay. The built in test renderer is better than VSCode's Go test output. Not only does it render the test with color<sup>1</sup> but also organizes into a nice test explorer view.</p> <p>Overall, the test output is polished, able to be undocked and run in a separate window. Works well for a TDD workflow.</p> <p></p> <p>@s0xzwasd provided some great insight on the different approach in the comments on this page. Compound configurations work well to run multiple sets of test when using multiple modules in the same repo. I tried this, and while tedious to click through the first time, It's easier to configure from a UI standpoint than trying to work through the VSCode <code>tasks.json</code> schema and build tasks by hand.</p> <p></p> <p>{{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"Question\" title=\":(fas fa-running): Run Configurations\" open=false &gt;}}</p> <p>\ud83d\udd34 Not able to set dynamic <code>$FileDir$</code> in the run configurations it seems.</p> <p>\ud83d\udfe0 Project configuration resolves path to relative once saved, but displays with absolute. This is confusing.</p> <p>\u2705 Can include configuration in project so anyone that opens in Goland can run the same prebuilt tasks.</p> <p>{{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"Question\" title=\":(fas fa-search): Linting\" open=false &gt;}}</p> <p>\ud83d\udfe0 Working on <code>golangci-lint</code> integration. There is an extension and I've also configured the FileWatcher to run on save, but neither is working as seamlessly as VSCode setting as the <code>gopls</code> linter tool.</p> <p>{{&lt; /admonition &gt;}}</p> <p>{{&lt; admonition type=\"Question\" title=\":(fas fas fa-tools): Refactoring &amp; Fixes\" open=false &gt;}}</p> <p>\ud83d\udfe0 Can't find a way to assign intentions to a keyboard shortcut. For example, <code>Add a Go Comment header</code> requires a click on a lightbulb icon, and can't find a way to allow this to be triggered by a keyboard shortcut.</p> <p>{{&lt; /admonition &gt;}}</p> <ol> <li> <p>vscode is requires <code>-nocolor</code> type argument to avoid console escaping if using a library that uses color.\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","goland","golang"]},{"location":"go-r1-day-58/","title":"Go R1 Day 58","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-58/#progress","title":"progress","text":"<ul> <li>Avoiding a panic in Go for missing dictionary match is very straight forward.   The error pattern for failed conversions and out of range index matches is the same, with: <code>ok, err := action</code>.</li> <li>TODO: Figure out if ok to reference an error in a test by: <code>is.Equal(error.Error(),\"unable to find value in map\")</code>. Linter warns me with: <code>Method call 'err.Error()' might lead to a nil pointer dereference</code>.</li> <li>Started work with dependency injection.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-58/#links","title":"links","text":"<ul> <li>Maps</li> <li>tests: \ud83e\uddea 007-maps \u00b7 sheldonhull/learn-go-with-tests-applied@11cf197 \u00b7 GitHub</li> <li>Dependency Injection</li> <li>tests: \ud83e\uddea 009-dependency-injection \u00b7 sheldonhull/learn-go-with-tests-applied@33a17c3 \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-56/","title":"Go R1 Day 56","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-56/#progress","title":"progress","text":"<ul> <li>Worked on ci-cd pipelines for Go apps.</li> <li>Learned a bit more on building Azure Pipeline templates to use with Go, learning a bit from SignalFx and how they did it.</li> <li>Read more on semver versioning, and still can't find anything as good as GitVersion</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-56/#links","title":"links","text":"<ul> <li>signalfx-agent/.azure-pipelines \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-57/","title":"Go R1 Day 57","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-57/#progress","title":"progress","text":"<ul> <li>Did some adhoc work in this repo (the hugo repo that contains this blog) testing out Mage, which is a Go based Make alternative.</li> <li>Generated dynamic target directory for hugo posts using stringify for kebab case.</li> <li>Unexpected behavior when generating dynamic file path including date.</li> </ul> <pre><code>year, month, day := time.Now().Date()\nstr := stringy.New(title)\nslugTitle := strings.Join([]string{string(year), string(month), string(day), str.KebabCase(\"?\", \"\").ToLower()}, \"-\")\n</code></pre> <p>The output failed to handle the year, resulting in some odd <code>\\x18</code> path generation.</p> <p>In reviewing the values from returned from <code>time.Now().Date</code>, I realized it wasn't an int value being returned.</p> <p>To work through the definition, I figured this would be a good chance to use the cli only to find the docs.</p> <pre><code>go doc 'time.Now'\n</code></pre> <p>returned the following:</p> <pre><code>package time // import \"time\"\n\nfunc Now() Time\n    Now returns the current local time.\n</code></pre> <p>To get the source code for the function:</p> <ul> <li><code>go doc -src 'time.Now'</code></li> <li><code>go doc -src 'time.Date'</code></li> </ul> <p>This shows the return value of <code>Date()</code> is actually <code>time</code> type, not int.</p> <p>Still couldn't see where the multiple return parameters were defined so I ran:</p> <pre><code>go install golang.org/x/tools/cmd/godoc@latest\ngodoc --http 127.0.0.1:3030\n</code></pre> <p>Ok... Figured it out. I was looking at the <code>func Date()</code>. However, what I should have been looking at was the exported method <code>func (Time) Date</code>. This correctly shows:</p> <pre><code>func (t Time) Date() (year int, month Month, day int)\n</code></pre> <p>I still couldn't figure this out until I tried running it in the playground.</p> <p>:(fa-fas fa-play): Playground - Demonstrate Problem</p> <pre><code>./prog.go:11:37: conversion from int to string yields a string of one rune, not a string of digits (did you mean fmt.Sprint(x)?)\n</code></pre> <p>Runes. Strings. I know folks say it boils down to 1's and 0's, but dang it... seems like my life always boils down to strings. \ud83d\ude06</p> <p>:(fa-fas fa-play): Playground - Fixed The Problem</p> <p>Finally got it all working.</p> <p>Strongly typed languages are awesome, but this type of behavior is not as easy to figure out coming from a background with dynamic languages. I</p> <p>In contrast, PowerShell would be: <code>Get-Date -Format 'yyyy'</code>.</p> <p>Here's an example of the Mage command then to generate a blog post with a nice selection and prompt.</p> <pre><code>// calculatePostDir calculates the post directory based on the post title and the date.\nfunc calculatePostDir(title string) string {\n    year, month, day := time.Now().Date()\n    str := stringy.New(title)\n    kebabTitle := str.KebabCase().ToLower()\n    slugTitle := strings.Join(string(year), string(month), string(day),kebabTitle, \"-\") ///stringy.ToKebabCase(title)\n\n    pterm.Success.Printf(\"Slugify Title: %s\", slugTitle)\n    filepath := filepath.Join(contentDir, string(year), slugTitle)\n    pterm.Success.Printf(\"calculatePostDir: %s\", slugTitle)\n    return filepath\n}\n</code></pre> <p>Then creation of the post:</p> <pre><code>// New namespace groups the new post generatation commands.\ntype New mg.Namespace\n// NewPost creates a new post in the Hugo format.\nfunc (New) Post() error {\n    prompt := promptui.Select{\n        Label: \"Select Type of Post j/k to navigate\",\n        Items: []string{\"100DaysOfCode\", \"microblog\", \"blog\"},\n    }\n    _, result, err := prompt.Run()\n    if err != nil {\n        pterm.Success.Printf(\"Prompt failed %v\\n\", err)\n        return err\n    }\n    pterm.Success.Printf(\"New Post: [%s]\", result)\n    promptTitle := promptui.Prompt{\n        Label: \"Enter Title\",\n    }\n    title, err := promptTitle.Run()\n    if err != nil {\n        pterm.Error.Printf(\"Prompt failed %v\\n\", err)\n        return err\n    }\n    // the archetype in archtytpes directory to use\n    var kind string\n\n    switch result {\n    case \"100DaysOfCode\":\n        kind = \"code\"\n    default:\n        kind = result\n    }\n    fileName := calculatePostDir(title)\n    if err := sh.RunV(\"hugo\", \"new\", fileName, \"--kind\", kind); err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-57/#links","title":"links","text":"<ul> <li>GitHub - gobeam/stringy: Convert string to camel case, snake case, kebab case / slugify, custom delimiter, pad string, tease string and many other functionalities with help of by Stringy package.</li> <li>Mage \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-59/","title":"Go R1 Day 59","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-59/#progress","title":"progress","text":"<ul> <li>Built some Mage tasks for setup of local projects.</li> <li>Used retool post from Nate Finch with Mage project and found it promising to vendor Go based tooling into a local project and not rely on the global package library.</li> <li>Created <code>magetools</code> repo to house some basic general mage tasks I could share with other projects.</li> </ul> <pre><code>year, month, day := time.Now().Date()\ndateString := fmt.Sprintf(\"%d-%02d-%d\", year, month, day)\n</code></pre> <p>Use padding in the <code>Sprintf</code> call to ensure a date comes back with <code>07</code> instead of <code>7</code>.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-59/#links","title":"links","text":"<ul> <li>Retooling Retool \u00b7 npf.io</li> <li>GitHub - sheldonhull/magetools: General tooling helpers for simplifying cross repository automation using Mage</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-60/","title":"Go R1 Day 60","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-60/#progress","title":"progress","text":"<ul> <li>Finished up the basics of dependency injection and how this helps with testable code.</li> <li>Worked through concurrency test using channels and go routines. This will take a bit more to get comfortable with as there is a variety of concepts here. My initial attempts finally started working using an anonymous function, but couldn't finalize due to some issue with launching the parallel executable being called. Not sure why the <code>--argname</code> seemed to be trimming the first dash from the argument when using <code>args = append(args,\"--argname 5\")</code>.</li> </ul> <p>I sure enjoy the visuals from pterm library. When not using the <code>-debug</code> flag, the concurrent counter reported a nice increase of total threads and then exited upon failure of any goroutine.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-61/","title":"Go R1 Day 61","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-61/#progress","title":"progress","text":"<p>It was a dark and stormy night. The world was bleak. A command was sent to The Compiler.</p> <p>The assumptions were flawed. The command was rejected.</p> <p>You have given me an invalid command with <code>-argument-that-shall-not-work 5</code> being invalid.</p> <p>But I've provided <code>--argument-that-shall-not-work</code>, the indomitable wizard said.</p> <p>Your command is unworthy.</p> <p>Digging into esoteric tomes of knowledge, the wizard discovered others have had similar issues when calling external processes using the legendary wizardry of <code>os/exec</code>. However, none could shine light into the darkness of his failure.</p> <p>Running in <code>pwsh</code> worked fine.</p> <p>Next, the wizard tried a variety of escaping commands.</p> <ul> <li>Using <code>shellescape</code> package.</li> <li>Using back-ticks with the arguments to escape.</li> <li>Using partially quoted arguments in the slice of the strings.</li> <li>Using no quotes.</li> <li>Went down the path of ancient texts describing similar issues.<sup>1</sup></li> </ul> <p>To the wizards dismay, copying the printed debug output worked fine in the terminal, but alas would not be executed by The Compiler.</p> <p>It began to feel like the curse of dynamic SQL queries that had long plagued the wizard until PowerShell had been discovered.</p> <p>The wizard ruminated on his plight. He thought:</p> <p>At the end of the day, all things seem to come down to strings and the cursed interpretation of my textual commands to The Compiler. How many a day have I wasteth upon the fault of a single character. The root of all evil must be a string.\"</p> <p>The wizard connected to a new remote instance, using the power of the Remote SSH plugin and began debugging in VSCode.</p> <p>The debug breakpoint config that worked was set in stone.</p> <pre><code>  {\n      \"name\": \"Run frustrating-test\",\n      \"type\": \"go\",\n      \"request\": \"launch\",\n      \"mode\": \"debug\",\n      \"program\": \"${workspaceFolder}/MyTestApp/test.go\",\n      \"args\": [\n        \"-count\",\n        \"100\",\n        \"-batch\",\n        \"10\",\n        \"-delay\",\n        \"1\",\n        \"-server\",\n        \"MyCustomIPAddress\",\n        \"-debug\",\n      ],\n      \"debugAdapter\": \"legacy\" // newer version wouldn't work remote\n    },\n</code></pre> <p>Consulting The Sage (aka Debugger), it advised the wizard of the message sent to The Compiler.</p> <p></p> <p>The wizard had a revelation. A fellow wizard advised to break the appending into individual statements instead of trying to do so much in one step.</p> <p>The incantation changed from:</p> <pre><code>command = append(command, fmt.Sprintf(`--header \"user-id: %s\"`, petname.Generate(petNameLength, \"-\"))\n</code></pre> <p>to the following:</p> <pre><code>command = append(command, \"--header\")\ncommand = append(command, fmt.Sprintf(`user-id: %s`, petname.Generate(petNameLength, \"-\")))\ncommand = append(command, \"--max-retry-count\", \"5\")\n</code></pre> <p>The foe vanquished, the The Blight of Strings was cast aside with malice. The wizard swore to never fall prey to this again.</p> <p>{{&lt; admonition type=\"Note\" title=\"Further Detail (:(fas fa-external-link-alt): expand to read)\" open=false &gt;}}</p> <p>Josesh<sup>1</sup> pointed towards: EscapeArgs. I did not find any equivalent for Darwin. The closest thing I could find was execveDarwin which I believe is the execution line, which gets the argument list from: SlicePtrFromStrings which is defined at here</p> <p>I'll have to re-examine in the future when I have more experience with Go, as it's not a simple chain to follow. :(fas fa-brain):</p> <pre><code>[test --param 1]\nstrings.Join(a)... \"test --param 1\"\nos.Command: [\"test\" \"--param 1\"]\necho \"test --param 1\\n\"\n</code></pre> <p>This pointed towards a similar issue with the <code>\\n</code> showing up.</p> <p>:(fas fa-play): Playground</p> <p>{{&lt; /admonition &gt;}}</p> <p>The Compiler's heartless gaze felt nothing. In the shadows, The String Balrock bid its time, knowing that the wizard would suffer once again.</p> <p></p> <ol> <li> <p>2016 // Prevent Escaping exec.Command Arguments in Go | Joseph Spurrier \u21a9\u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-62/","title":"Go R1 Day 62","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-62/#progress","title":"progress","text":"<ul> <li>Worked with mocks.</li> </ul> <p>Still using the <code>is</code> package to test.</p> <pre><code>if !reflect.DeepEqual(want, spySleepPrinter.Calls) {\n  t.Errorf(\"wanted calls %v got %v\", want, spySleepPrinter.Calls)\n}\n</code></pre> <p>was replaced with:</p> <pre><code>is.Equal(spySleepPrinter.Calls, want) // spySleepPrinter shows correct order of calls\n</code></pre> <p>Go is messing with my head with how everything gets simplified to the lowest common interface when possible. Instead of buffer, I'd want to use <code>io.Writer</code> for example. This abstraction is where there is so much power, but it requires a deeper understanding of following the dependency tree to know what properly satisfies the interface. I'm finding that one layer isn't enough sometimes with lower level interfaces, and requires getting comfortable with more low level packages from the standard library. Pretty cool that I didn't need to do anything more complex to do a comparison.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-62/#when-to-use-mocking","title":"When To Use Mocking","text":"<p>Without mocking important areas of your code will be untested. In our case we would not be able to test that our code paused between each print but there are countless other examples. Calling a service that can fail? Wanting to test your system in a particular state? It is very hard to test these scenarios without mocking. <sup>1</sup></p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-62/#other-good-insights","title":"Other Good Insights","text":"<p>\"When to use iterative development? You should use iterative development only on projects that you want to succeed.\" - Martin Fowler</p> <p>I really agree with this next quote. I've seen this happen so often with the pressures of a project, and feel it's the excuse that causes the escalation of technical debt that becomes difficult to untangle retroactively.</p> <p>Try to get to a point where you have working software backed by tests as soon as you can, to avoid getting in rabbit holes and taking a \"big bang\" approach. <sup>1</sup></p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-62/#links","title":"links","text":"<ul> <li>chore: formatting and devcontainer addition \u00b7 sheldonhull/learn-go-with-tests-applied@9015a0b \u00b7 GitHub</li> <li>chore: add extension \u00b7 sheldonhull/learn-go-with-tests-applied@950ccd1 \u00b7 GitHub</li> <li>chore(docker): add bit cli to dockerfile \u00b7 sheldonhull/learn-go-with-tests-applied@9be2573 \u00b7 GitHub</li> <li>refactor(009-mocking): use bytes for input \u00b7 sheldonhull/learn-go-with-tests-applied@56b8740 \u00b7 GitHub</li> <li>refactor(009-mocking): refactored with constants \u00b7 sheldonhull/learn-go-with-tests-applied@24a2709 \u00b7 GitHub</li> <li>refactor(009-mocking): add sleep for dramatic effect \u00b7 sheldonhull/learn-go-with-tests-applied@fbb7d6d \u00b7 GitHub</li> <li>refactor(009-mocking): add sleep interface and first mocking steps \u00b7 sheldonhull/learn-go-with-tests-applied@74b975c \u00b7 GitHub</li> <li>refactor(009-mocking): mocking the order successfully \u00b7 sheldonhull/learn-go-with-tests-applied@9cc2c7e \u00b7 GitHub</li> </ul> <ol> <li> <p>Mocking \u21a9\u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-63/","title":"Go R1 Day 63","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-63/#progress","title":"progress","text":"<ul> <li>Did some work on Go regex processing with a new linting tool concept I have for \"semantic line breaks\".</li> <li>I've forced myself to apply TDD from the get go, so it's been slow going initially to abstract my functions to be testable without being run as a CLI tool.</li> <li>Started on the run test for simulating the cli call as well.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-63/#links","title":"links","text":"<ul> <li>Why you shouldn't use func main in Go by Mat Ryer - PACE.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"first-pass-with-pulumi/","title":"First Pass With Pulumi","text":"","tags":["tech","development","aws","infrastructure-as-code","golang"]},{"location":"first-pass-with-pulumi/#why","title":"Why","text":"<p>Instead of learning a new domain specific language that wraps up cloud provider API's, this let's the developer use their preferred programming language, while solving several problems that using the API's directly don't solve.</p> <ul> <li>Ensure the deployment captures a state file of the changes made.</li> <li>Workflow around the previews and deployments.</li> <li>Easily automated policy checks and tests.</li> </ul> <p>This can be a really useful tool to bring infrastructure code maintainability directly into the the lifecycle of the application.</p> <p>It's subjective to those in DevOps whether this would also apply for \"Day 0-2\" type operations, which are typically less frequently changed resources such as account settings, VPC, and other more static resources.</p> <p>However, with a team experienced with Go or other tooling, I could see that this would provide a way to have much more programmatic control, loops, and other external libraries used, without resorting to the HCL DSL way of doing resource looping and inputs.</p>","tags":["tech","development","aws","infrastructure-as-code","golang"]},{"location":"first-pass-with-pulumi/#first-pass","title":"First Pass","text":"<p>First impression was very positive!</p> <p>Basic steps:</p> <ul> <li><code>brew install pulumi</code></li> <li><code>pulumi new aws-go</code></li> <li>Entered name of test stack such as <code>aws-vpc</code>.</li> <li>Copied the VPC snippet from their docs and then plugged in my own tag for naming, which by default wasn't included.</li> <li>Reproduced the example for <code>pulumi.String()</code>.<sup>1</sup></li> </ul> <pre><code>package main\n\nimport (\n    \"flag\"\n\n    petname \"github.com/dustinkirkland/golang-petname\"\n    \"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/ec2\"\n  \"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nvar (\n    words     = flag.Int(\"words\", 2, \"The number of words in the pet name\")\n    separator = flag.String(\"separator\", \"-\", \"The separator between words in the pet name\"))\n\nfunc main() {\n    pulumi.Run(func(ctx *pulumi.Context) error {\n        conf := config.New(ctx, \"\")\n        stage := conf.Require(\"stage\")\n        petname := petname.Generate(*words, *separator)\n        _, err := ec2.NewVpc(ctx, stage, &amp;ec2.VpcArgs{\n            CidrBlock: pulumi.String(\"10.0.0.0/16\"),\n            Tags: pulumi.StringMap{\n                \"Name\": pulumi.String(strings.Join([]string{stage, petname}, \"-\")),\n            },\n        })\n        if err != nil {\n            return err\n        }\n\n        return nil\n    })\n}\n</code></pre>","tags":["tech","development","aws","infrastructure-as-code","golang"]},{"location":"first-pass-with-pulumi/#positive-observations","title":"Positive Observations","text":"<ul> <li>Running <code>pulumi destroy</code> left the stack in the console for full plan history and auditing. To remove the stack from the web you'd run: <code>pulumi stack rm dev</code>. This is similar to how terraform cloud workspaces work and gives confidence of easier auditing by default.</li> <li>The console experience and browser integration was beautifully done.</li> <li><code>pulumi preview --emoji</code> provided a very clean and succint summary of changes.</li> <li><code>pulumi up</code> also was very clean, and allowed a selection to expand the details as well.</li> <li>Browser for stack provides full metadata detail, resource breakdown, audit history, and more.</li> </ul> <ul> <li> <p>The Pulumi docs for Azure DevOps were pretty solid! Full detail and walk through. As an experienced PowerShell developer, I was pleasantly suprised by quality PowerShell code that overall was structured well.<sup>2</sup></p> </li> <li> <p>Set some values via yaml easily by: <code>'pulumi config set --path 'stage' 'dev'</code> which results in:</p> </li> </ul> <pre><code>config:\n  mystack:stage: dev\n  aws:region: myregion\n</code></pre> <p>This is then read via:</p> <pre><code>conf := config.New(ctx, \"\")\nstage := conf.Require(\"stage\")\n</code></pre>","tags":["tech","development","aws","infrastructure-as-code","golang"]},{"location":"first-pass-with-pulumi/#things-to-improve","title":"Things To Improve","text":"<ul> <li>Missing the benefit of Terraform module registry.</li> <li>Pulumi Crosswalk sounds pretty awesome to help with this. However, I wasn't able to find the equivalent of a \"crosswalk module library\" to browse so that part might be a future improvement.</li> </ul> <p>This document link: AWS Virtual Private Cloud (VPC) | Pulumi seemed great as a tutorial, but wasn't clear immediately on how I could use with Go.</p> <p>I looked at the aws \u00b7 pkg.go.dev but didn't see any equivalent to the documented <code>awsx</code> package shown from nodejs library.</p> <p>Finally, found my answer.</p> <p>Pulumi Crosswalk for AWS is currently supported only in Node.js (JavaScript or TypeScript) languages. Support for other languages, including Python, is on the future roadmap. Pulumi Crosswalk for AWS | Pulumi</p> <p>I wish this was put as a big disclaimer right up at the top of the crosswalk section to ensure it was very clear.</p> <ol> <li> <p>This feels very similar in style to the AWS SDK which doesn't allow just string values, but requires pointers to strings and thus wraps up the strings with expressions such as <code>aws.String(</code>.\u00a0\u21a9</p> </li> <li> <p>Subjective, but I noticed boolean values instead of switches, which would slightly simplify the build scripts, but is more of a \"nit\" than a critical issue. Using if blocks instead of switch might also clean things up, but overall the script was pretty well written, which seems rare in vendor provided PowerShell examples. \ud83d\udc4f\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","aws","infrastructure-as-code","golang"]},{"location":"go-r1-day-64/","title":"Go R1 Day 64","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-64/#progress","title":"progress","text":"<p>Wrote: First Pass with Pulumi</p> <p>At <code>$work</code>, I'm working primarily with Go developers. This was an exploration of using Go for infrastructure.</p> <p>Read a bit on CDK for Terraform as well, which seems interesting.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"sweetops-slack-archive/","title":"SweetOps Slack Archive","text":"<p>Just wanted to give props to the Cloudposse team lead by Erik Osterman @eosterman.</p> <p>Slack provides a great community chat experience, but there are quite a few problems about using it for Q&amp;A. <sup>1</sup> Since the free plans for communities hide content over 10,000 messages, a healthy community will go over this quickly.</p> <p>With all the great conversations, I want to find prior discussions to benefit from topics already covered.</p> <p>Cloudposse archives the community discussions so they can be searched.</p> <p>Cloudposse archives the community discussions for future searches here: SweetOps Archive.</p>","tags":["tech","development","microblog"]},{"location":"sweetops-slack-archive/#pro-tip-search-aliases","title":"Pro-Tip Search Aliases","text":"<p>If you use Alfred you can setup an alias for this, or use a Chrome Search Engine Alias. To use a Chrome search engine alias, go to: Search Engines and add a new entry.</p> <ul> <li>Search Engine: <code>cloudposse</code></li> <li>Keyword: <code>cloudposse</code></li> <li>URL with %s in place of query: <code>https://archive.sweetops.com/search?query=%s</code></li> </ul> <p>For any future search, just type in cloudposse in the searchbar and whatever you type after that will open up in the archive search.</p> <p></p> <p></p> <ol> <li> <p>I don't think Cloudposse or many others deny that Slack is \"inferior\" for threaded conversation to a tool like Discourse. However, despite it being a \"walled garden\", it's a lot easier to get engagement there than a forum from what I understand. This solution provides a nice middle ground by giving the ease of Slack, while ensuring great conversation is still captured and able to be found.\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","microblog"]},{"location":"go-r1-day-65/","title":"Go R1 Day 65","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-65/#progress","title":"progress","text":"<ul> <li>Built mage tasks for go formatting and linting.</li> </ul> <p>Using this approach, you can now drop a <code>magefile.go</code> file into a project and set the following:</p> <pre><code>// +build mage\n\npackage main\n\nimport (\n\n    \"github.com/magefile/mage/mg\"\n    \"github.com/pterm/pterm\"\n\n    // mage:import\n    \"github.com/sheldonhull/magetools/gotools\"\n)\n</code></pre> <p>Calling this can be done directly now as part of a startup task.</p> <pre><code>// Init runs multiple tasks to initialize all the requirements for running a project for a new contributor.\nfunc Init() error {\n    fancy.IntroScreen(ci.IsCI())\n    pterm.Success.Println(\"running Init()...\")\n    mg.Deps(Clean, createDirectories)\n    if err := (gotools.Golang{}.Init()); err != nil {  // &lt;----- From another package.\n        return err\n    }\n\n    return nil\n}\n</code></pre> <p>Additionally, handled some Windows executable path issues by making sure to wrap up the path resolution.</p> <pre><code>// if windows detected, add the exe to the binary path\nvar extension string\nif runtime.GOOS == \"windows\" {\n  extension = \".exe\"\n}\ntoolPath := filepath.Join(\"_tools\", item+extension)\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-65/#links","title":"Links","text":"<ul> <li>feat(gotools): \u2728 add gotools for running go specific actions \u00b7 sheldonhull/magetools@ef97514 \u00b7 GitHub</li> <li>refactor(gotools): \ud83d\ude9a add namespace for Golang and update lint ci to d\u2026 \u00b7 sheldonhull/magetools@51a9c3b \u00b7 GitHub</li> <li>feat(gotools): \ud83c\udf89 add go formatting and linting helpers \u00b7 sheldonhull/magetools@541c2fc \u00b7 GitHub</li> <li>refactor(gotools): \ud83d\udd28 windows support for executables \u00b7 sheldonhull/magetools@d70712e \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/","title":"Go R1 Day 66","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/#progress","title":"progress","text":"<p>This wasn't specific to Go, but was the first step towards using Go in a distributed test.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/#dapr","title":"Dapr","text":"<p>I had an interesting project today with my first development level effort using Kubernetes. Here's my log of attempting to use Getting started with Dapr | Dapr Docs and getting two Go APIs to talk to each other with it.</p> <p>First, what is Dapr?</p> <p>Dapr is a portable, event-driven runtime that makes it easy for any developer to build resilient, stateless and stateful applications that run on the cloud and edge and embraces the diversity of languages and developer frameworks. <sup>1</sup> ... Dapr codifies the best practices for building microservice applications into open, independent building blocks that enable you to build portable applications with the language and framework of your choice. Each building block is completely independent and you can use one, some, or all of them in your application.</p> <p>From this, it sounds like Dapr helps solve issues by abstracting the \"building blocks\" away from the business logic. Rather than focusing on the implementation level concern of how to talk from service to service, Dapr can help with this.</p> <p>Instead of relying on provider specific key-value store, such as AWS SSM Parameter store, Dapr abstracts that too.</p> <p>It's interesting as this concept of abstraction on a service level is something new to me. Good abstractions in software are hard but critical to maintainability long-term. Provider-level abstractions are something on an entirely different scale.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/#setup","title":"Setup","text":"<ul> <li>Enable Kubernetes on Docker Desktop.</li> <li>Install Lens: <code>brew install lens</code></li> <li>Pop this open and <code>Cmd+,</code> to get to settings.</li> <li>Add dapr helm charts: <code>https://dapr.github.io/helm-charts/</code></li> <li>Connect to local single-node Kubernetes cluster and open the charts section in Lens.</li> <li>Install Dapr charts.</li> <li>Celebrate your master of all things Kubernetes.</li> </ul> <p>I think I'll achieve the next level when I don't do this in Lens. I'll have to eventually use some cli magic to deploy my changes via helm or level-up to Pulumi. \ud83d\ude00 Until then, I'll count myself as victorious.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/#a-practical-test","title":"A Practical Test","text":"<ul> <li>Install Dapr - Visual Studio Marketplace</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-66/#links","title":"links","text":"<ol> <li> <p>Overview | Dapr Docs \u21a9</p> </li> </ol>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-67/","title":"Go R1 Day 67","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-67/#progress","title":"progress","text":"<p>Built functionality in my blog repo to create a new 100DaysOfCode post using Mage. This provides an interactive prompt that automatically tracks the days left and increments the counter as this progresses.</p> <ul> <li>ingest toml configuration</li> <li>unmarshal to struct</li> <li>update struct</li> <li>marshal and write back to the toml configuration file</li> <li>replace matched tokens in file</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-68/","title":"Go R1 Day 68","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-68/#progress","title":"progress","text":"<ul> <li>Did exercism.io for gigasecond puzzle.</li> </ul> <pre><code>package gigasecond\n\n// import path for the time package from the standard library\nimport (\n    \"time\"\n)\n\n// gigasecond represents a very very very small portion of a second.\nconst gigasecond = 1000000000\n\n// AddGigasecond adds a very very very small portion of a second called a gigasecond to a provided time input.\nfunc AddGigasecond(t time.Time) time.Time {\n    gcDuration := gigasecond * time.Second\n    n := t.Add(gcDuration)\n    return n\n}\n</code></pre> <ul> <li>Learned a bit more about using <code>Math.Pow()</code>, conversion of floats/ints, and dealing with time.Duration.</li> <li>Tried using <code>Math.Pow()</code> to work through the issue, but got mixed up when using <code>time.Duration()</code> which expects nanoseconds, and such. Went ahead and just used a constant for the exercise as not likely to use gigaseconds anytime soon. \ud83d\ude00</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-68/#links","title":"links","text":"<ul> <li>Gigasecond Solution</li> <li>refactor: \ud83d\ude9a move directories to support exercism cli \u00b7 sheldonhull/algorithmswithgo.com@8890e57 \u00b7 GitHub</li> <li>feat: \ud83c\udf89 submitted gigasecond solution \u00b7 sheldonhull/algorithmswithgo.com@bbe62da \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-69/","title":"Go R1 Day 69","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-69/#progress","title":"progress","text":"<ul> <li>Read on functional options in Go. I've seen this used with Pterm, Zerolog, and other tools, but didn't know this pattern had a name.</li> <li>WalkDir is a newer function available in Go 1.16. It's recommended for future work, but found very few examples. Adjusted my line break code to use <code>WalkDir</code>.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-69/#links","title":"links","text":"<ul> <li>Example of WalkDir I found useful</li> <li>Go-Semantic-Linebreaks</li> <li>Functional Options Pattern</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-70/","title":"Go R1 Day 70","text":"","tags":["tech","development","100DaysOfCode","golang","microblog","goroutines"]},{"location":"go-r1-day-70/#progress","title":"progress","text":"<ul> <li>Concurrency section wrap-up with Learn Go With Tests.</li> <li>Reviewed material learned from: Go R1 Day 61</li> <li>Read material, but didn't do a lot of tests on this, per mostly concept oriented. Used concurrent progressbar example from <code>uiprogress</code> project to test concurrent UI updates.</li> <li>My last concurrency test case was to launch many concurrent processes for a load test. This didn't leverage goroutinues as typically used, since it was calling to an executable on the host machine. However, this provided a great use case for something I've done before with DevOps oriented work and showed how to use concurrency as a blocking operation. Once the user was done with the test, <code>ctrl+c</code> was used to kill the active requests and the program exited.</li> <li>I need more practice with channels. I was only wanting error stdout content, and so didn't have any need for receiving channel output back in a structured way. This is probably an atypical usage of concurrency, fitting for an external load test, but not internal Go code.</li> <li>Still found it pretty cool that I could spin up 500 processes at once, with far less overhead than doing in PowerShell.</li> </ul> <p>{{&lt; admonition type=\"Note\" title=\"Example Of Doing In PowerShell\" open=true &gt;}} Doing this in PowerShell is far more compact, but not as performant.</p> <p>This is a good example of the different in using Go for adhoc tasks. It will require more code, error handling care, but pays off in something that is likely more stable and easier to run across multiple systems with a single binary.</p> <pre><code>#!/usr/bin/env pwsh\n$Server = 'IPADDRESS'\n$ServerPort = '3000'\nWrite-Host 'Load Test Start'\n$RandomPort = '4000'\n\n$j = @(4000..4100)| ForEach-Object {\n    $c = $_\n    Start-ThreadJob -ThrottleLimit 1000 -StreamingHost $Host -InputObject $c -ScriptBlock {\n    $RandomPort = $input\n    &amp;mybinary serve --max-retry-count 5 --header \"user-id: $(petname)\" --header \"session-id: $(uuidgen)\" \"${using:Server}:${using:ServerPort}\"\n}\n}\n$j | Wait-Job | Receive-Job\n$j | Stop-Job\n</code></pre> <p>I didn't benchmark total load difference between this and Go, but I'm sure the pwsh threads were a bit more costly, though for this test case might not have been a large enough value to make much difference.</p> <p>{{&lt; /admonition &gt;}}</p>","tags":["tech","development","100DaysOfCode","golang","microblog","goroutines"]},{"location":"go-r1-day-70/#code-examples","title":"Code Examples","text":"<p>This first section is the startup. Key points:</p> <ul> <li>main() is the entry point for the program, but doesn't contain the main logic flow. Inspired by Matt Ryer's posts, I now try to ensure main is as minimal as possible to encourage easier automation in testing. Since <code>run</code> contains the main logic flow, the actual CLI itself can be called via integration test by flipping to <code>Run()</code> and calling from <code>testing</code> file using a blackbox testing approach.</li> </ul> <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"errors\"\n    \"flag\"\n    \"fmt\"\n    \"io\"\n    \"math\"\n    \"os\"\n    \"os/exec\"\n    \"strings\"\n    \"sync\"\n    \"time\"\n\n    shellescape \"github.com/alessio/shellescape\"\n    petname \"github.com/dustinkirkland/golang-petname\"\n    \"github.com/google/uuid\"\n    \"github.com/pterm/pterm\"\n    \"github.com/rs/zerolog\"\n    \"github.com/rs/zerolog/log\"\n)\n\nconst (\n    // exitFail is the exit code if the program\n    // fails.\n    exitFail = 1\n\n    // desiredPort is the port that the app forwards traffic to.\n    desiredPort = 22\n\n    // petnameLength is the length of the petname in words to generate.\n    petNameLength = 2\n\n    // startingPort is the starting port for a new connection, and will increment up from there so each connection is unique.\n    startingPort = 4000\n\n    // maxRetryCount is the number of times to retry a connection.\n    maxRetryCount = 5\n)\n\nfunc main() {\n    if err := run(os.Args, os.Stdout); err != nil {\n        fmt.Fprintf(os.Stderr, \"%s\\n\", err)\n        os.Exit(exitFail)\n    }\n}\n</code></pre> <p>Next <code>run</code> contains the main logic flow. The goal is that all main program logic for exiting and terminating is handled in this single location.</p> <pre><code>// Run handles the arguments being passed in from main, and allows us to run tests against the loading of the code much more easily than embedding all the startup logic in main().\n// This is based on Matt Ryers post: https://pace.dev/blog/2020/02/12/why-you-shouldnt-use-func-main-in-golang-by-mat-ryer.html\nfunc run(args []string, stdout io.Writer) error {\n    if len(args) == 0 {\n        return errors.New(\"no arguments\")\n    }\n    InitLogger()\n    zerolog.SetGlobalLevel(zerolog.InfoLevel)\n\n    debug := flag.Bool(\"debug\", false, \"sets log level to debug\")\n    Count := flag.Int(\"count\", 0, \"number of processes to open\")\n    delaySec := flag.Int(\"delay\", 0, \"delay between process creation. Default is 0\")\n    batchSize := flag.Int(\"batch\", 0, \"number of processes to create in each batch. Default is 0 to create all at once\")\n    Server := flag.String(\"server\", \"\", \"server IP address\")\n    ServerPort := flag.Int(\"port\", 3000, \"server port\") //nolint:gomnd\n\n    flag.Parse()\n    log.Logger.Info().Int(\"Count\", *Count).\n        Int(\"delaySec\", *delaySec).\n        Int(\"batchSize\", *batchSize).\n        Str(\"Server\", *Server).\n        Msg(\"input parsed\")\n\n    log.Logger.Info().\n        Int(\"desiredPort\", desiredPort).\n        Int(\"petNameLength\", petNameLength).\n        Int(\"startingPort\", startingPort).\n        Msg(\"default constants\")\n\n    if *debug {\n        zerolog.SetGlobalLevel(zerolog.DebugLevel)\n    }\n\n    RunTest(*Count, *delaySec, *batchSize, *Server, *ServerPort)\n    return nil\n}\n</code></pre> <p>Next, <code>InitLogger</code> is used to initialize the logger for zerolog. I don't need multiple configurations right now so this is just stdout.</p> <pre><code>// InitLogger sets up the logger magic\n// By default this is only configured to do pretty console output.\n// JSON structured logs are also possible, but not in my default template layout at this time.\nfunc InitLogger() {\n    output := zerolog.ConsoleWriter{Out: os.Stdout, TimeFormat: time.RFC3339}\n    log.Logger = log.With().Caller().Logger().Output(zerolog.ConsoleWriter{Out: os.Stderr})\n\n    output.FormatLevel = func(i interface{}) string {\n        return strings.ToUpper(fmt.Sprintf(\"| %-6s|\", i))\n    }\n    output.FormatMessage = func(i interface{}) string {\n        return fmt.Sprintf(\"%s\", i)\n    }\n    output.FormatFieldName = func(i interface{}) string {\n        return fmt.Sprintf(\"%s:\", i)\n    }\n    output.FormatFieldValue = func(i interface{}) string {\n        return strings.ToUpper(fmt.Sprintf(\"%s\", i))\n    }\n    log.Info().Msg(\"logger initialized\")\n}\n</code></pre> <p>Test the existence of the binary being run in a load test, and exit if it doesn't exist. This should more likely be handled in the run fuction, but I just did it here for simplicity in this adhoc tool.</p> <pre><code>// TestBinaryExists checks to see if the binary is found in PATH and exits with failure if can't find it.\nfunc TestBinaryExists(binary string) string {\n    p, err := exec.LookPath(binary)\n    if err != nil {\n        log.Logger.Error().Err(err).Str(\"binary\",binary).Msg(\"binary not found\")\n        os.Exit(exitFail)\n    }\n\n    return p\n}\n</code></pre> <p>Next, <code>buildCLIArgs</code> handles the argument string slice construction. I learned from this to keep each line and argument independent as escaping has some strange behavior if you try to combine too much in a single statement, especially with spaces. Best practice is to keep this very simple.</p> <pre><code>// buildCliArgs is an example function of building arguments via string slices\nfunc buildCliArgs(Server string, ServerPort int, port int) (command []string) {\n    command = append(command, \"server\")\n    command = append(command, \"--header\")\n    command = append(command, fmt.Sprintf(`user-id: %s`, petname.Generate(petNameLength, \"-\")))\n    command = append(command, \"--header\")\n    command = append(command, fmt.Sprintf(`session-id: %s`, uuid.Must(uuid.NewRandom()).String()))\n    command = append(command, \"--max-retry-count\", \"5\")\n    command = append(command, Server+\":\"+fmt.Sprintf(\"%d\", ServerPort))\n    return command\n}\n</code></pre> <p>Finally, a function that run the tests with some pretty output using pterm. This would be probably better to break-up for testing, but again, adhoc project, so this ended up working decently as I was learning concurrency.</p> <pre><code>// RunTest is the main test function that calculates the batch size and then launches the  creation using a routinue.\nfunc RunTest(Count int, delaySec int, batchSize int, Server string, ServerPort int) {\n    log.Logger.Info().Msg(\"RunTest startings\")\n    totalBatches := math.Ceil(float64(Count) / float64(batchSize))\n    log.Logger.Info().Float64(\"totalBatches\", totalBatches).Msg(\"batches to run\")\n    myBinary := TestBinaryExists(\"binaryname\")\n    port := startingPort\n    var wg sync.WaitGroup\n\n    totals := 0\n    p, _ := pterm.DefaultProgressbar.WithTotal(Count).WithTitle(\"run s\").Start()\n\n    for i := 0; i &lt; int(totalBatches); i++ {\n        log.Debug().Int(\"i\", i).Int(\"port\", port).Msg(\"batch number\")\n\n        for j := 0; j &lt; batchSize; j++ {\n            if totals == Count {\n                log.Debug().Msg(\"totals == Count, breaking out of loop\")\n\n                break\n            }\n\n            totals++\n            log.Debug().Int(\"i\", i).Int(\"\", totals).Msg(\"\")\n            cmdargs := buildCliArgs(Server, ServerPort, port)\n            wg.Add(1)\n            go func() {\n                defer wg.Done()\n                buf := &amp;bytes.Buffer{}\n                cmd := exec.Command(, cmdargs...)\n                cmd.Stdout = buf\n                cmd.Stderr = buf\n                if err := cmd.Run(); err != nil {\n                    log.Logger.Error().Err(err).Bytes(\"output\", buf.Bytes()).Msg(\" failed\")\n                    os.Exit(exitFail)\n                }\n                log.Logger.Debug().Msgf(\" %v\", shellescape.QuoteCommand(cmdargs))\n                log.Logger.Debug().Bytes(\"output\", buf.Bytes()).Msg(\"\")\n            }()\n\n            p.Title = \"port: \" + fmt.Sprintf(\"%d\", port)\n            p.Increment()\n            port++\n        }\n        time.Sleep(time.Second * time.Duration(delaySec))\n    }\n    p.Title = \"s finished\"\n    _, _ = p.Stop()\n    wg.Wait()\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog","goroutines"]},{"location":"go-r1-day-70/#links","title":"links","text":"<ul> <li>GitHub - gosuri/uiprogress: A go library to render progress bars in terminal applications</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog","goroutines"]},{"location":"go-r1-day-71/","title":"Go R1 Day 71","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-71/#progress","title":"progress","text":"<ul> <li>Learn Go With Tests -&gt; Using <code>select</code> with channels to wait for multiple goroutines.</li> <li>Of particular interest is this:</li> </ul> <p>Notice how we have to use make when creating a channel; rather than say var ch chan struct{}. When you use var the variable will be initialised with the \"zero\" value of the type. So for string it is \"\", int it is 0, etc. For channels the zero value is nil and if you try and send to it with &lt;- it will block forever because you cannot send to nil channels (go-fundamentals-select listed below)</p> <ul> <li>Used <code>httptest</code> to create mock server for faster testing, and included wrapper around a calls to allow configuration for timeout. This ensures that testing can handle in milliseconds, but default behavior in a deployment would be 10 seconds or more.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-71/#links","title":"links","text":"<ul> <li>go-fundamentals-select</li> <li>Go by Example: Select</li> <li>feat(11-select): \ud83c\udf89 11-select initial framework without goroutines \u00b7 sheldonhull/learn-go-with-tests-applied@b0a2641 \u00b7 GitHub</li> <li>refactor(11-select): \u267b\ufe0f refactor for DRY and better helper functions \u00b7 sheldonhull/learn-go-with-tests-applied@1cf7092 \u00b7 GitHub</li> <li>refactor(11-select): \u267b\ufe0f add test case for error on timeout to avoid b\u2026 \u00b7 sheldonhull/learn-go-with-tests-applied@77f01bd \u00b7 GitHub</li> <li>refactor(11-select): \u267b\ufe0f refactor to allow configurable racer timeouts \u00b7 sheldonhull/learn-go-with-tests-applied@65fe79c \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-72/","title":"Go R1 Day 72","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-72/#progress","title":"progress","text":"<ul> <li>Learned more on \"mechanical sympathy\" related to Go and memory management.</li> <li>How the Go compiler works related to frames with memory allocation.</li> <li>Built Mage tasks for licensing and validating problematic licenses aren't included in a project.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-72/#links","title":"links","text":"<ul> <li>refactor(tooling): add verbose handling to RunTool \u00b7 sheldonhull/magetools@40e7d21 \u00b7 GitHub</li> <li>feat(licensing): \u2728 add licensing checks using go-license \u00b7 sheldonhull/magetools@82964d5 \u00b7 GitHub</li> <li>refactor(gotools): \ud83d\udcdd add verbose handling for golangci-lint tooling \u00b7 sheldonhull/magetools@01ca636 \u00b7 GitHub</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-73/","title":"Go R1 Day 73","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-73/#progress","title":"progress","text":"<ul> <li>I'm more on day 90+ but haven't made the time to log it.</li> <li>Been working a ton with Mage and have some really cool work I'll be writing up soon, including using for health checks on projects, setup of core tooling, encoding credentials, and more. \ud83d\udcaf</li> <li> <p>Also been working heavily on Go mono-repo module structure and some serverless based architecture tests that will make a fun write-up. I'm thinking of a demo of \"Disappointment as a Service\" with a lambda driven api for tracking disappointing events and returning something to help bring reality-checks to us all. \ud83d\ude06</p> </li> <li> <p>I used a snippet from a blog today that allowed buffering stdout from go commands to capture and avoid streaming each line by wrapping up in a pterm spinner component. Pretty neat!</p> </li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-73/#links","title":"links","text":"<ul> <li>Magetools - Tooling</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-74/","title":"Go R1 Day 74","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-74/#progress","title":"progress","text":"<ul> <li>Worked with DynamoDB schema (NoSQL Document database).</li> <li>Invoked local lambda function using Docker and also remove invocation with <code>serverless invoke</code>. This took a bit of work to figure out as the parameters weren't quite clear. Upon using <code>--path</code> for the json template I got from AWS Lambda console, I was able to to get to invoke, and stream the logs with <code>--log</code>.</li> <li>More mage magic with promptui and other features, so I can now test a full tear down, build, publish, and invoke a test selection by running: <code>mage sls:destroy build sls:deploy sls:test remote</code>.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-75/","title":"Go R1 Day 75","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-75/#progress","title":"progress","text":"<ul> <li>Ultimate Go: Pointers Part 2 (Sharing Data)</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/","title":"Go R1 Day 76","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#progress","title":"progress","text":"<p>Lots of more technical detail on Memory &amp; Data Semantics Section.</p> <p>I value this as my background in databases means that things like Stack/Heap weren't part of my education as much. This has been useful as there are new concepts, paradigms, and ways of thinking and it's providing a lot more foundational concepts for me to better understanding the system and compiler behavior in Go.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#ultimate-go-pointers-part-3-escape-analysis","title":"Ultimate Go: Pointers-Part-3: Escape Analysis","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#notes","title":"Notes","text":"<ul> <li>Stacks are self-cleaning</li> <li>Compiler knows what goes on stack and what goes on heap at compile time. Knowing where the value is constructed is part of the static analysis the compiler performs.</li> <li>Bill said this is a really powerful feature that will probably impact future languages.</li> <li>When you see a pointer in the return value, this could be phrased \"shared up the callstack\".</li> <li>This means that the construction inside the function would be on the heap not the stack.</li> <li>If you are trying to access items further up on the stack, it requires pointers.</li> <li>Bill said what's cool about this is the ability to access an item on the heap via this pointer, as if we were working with pointers to a stack value. It abstracts the machine level details away so we benefit from easy out of frame access to values with pointers.</li> <li>We don't have to worry about where the location of the value being constructed is. Due to escape analysis, the compiler will determine the best place, but the reality is some items have to go to the heap, and the compiler is smart enough to help determine this.</li> <li>Why does this matter? Bill says that if performance matters, understanding the concepts matter because garbage collection and other latencies are impacted by these concepts.</li> <li>Can view escape analysis from compiler with: <code>go build -gcflags -m=2</code>.<ul> <li>I tested this on one of my projects, and found that function complexity can prevent inlining, lots of escape to heap references and more.</li> <li>While not needed for basic build automation type helpers, I could see the value in scalable design for Go in examining this in more details. It's almost like using SQL Server execution plans to optimize for higher performance.</li> </ul> </li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#takeaways","title":"Takeaways","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#dont-hide-the-cost","title":"Don't hide the cost","text":"<p>If we are doing construction to a variable, we use value construction. Avoid pointer semantic construction if not in the return.</p> <p>Example:</p> <pre><code>// clear visible cost of the allocation by value construction and passing of pointer back up the call stack\nfunc createSomething() *something {\n  u := something{\n    name: \"example\",\n  }\n  return &amp;u // &lt;--- This makes clear the cost and allocation back up the callstack.\n}\n// cost is obscured by construction being a pointer\n// and returning a value that is not clear to reader if value or pointer\nfunc createSomething()*something {\n  u := &amp;something{\n    name: \"example\",\n  }\n  return u // &lt;--- Not good. Hides the cost, and require reading function further to find that this is a pointer.\n}\n</code></pre> <p>Making cost obvious and visible is a big priority for readable maintainable code with a team.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-76/#links","title":"links","text":"<ul> <li>Stack vs heap allocation of structs in Go, and how they relate to garbage collection - Stack Overflow</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/","title":"Go R1 Day 77","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/#progress","title":"progress","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/#more-fun-with-golangci-lint","title":"More Fun With Golangci-lint","text":"<p>Been doing a ton the last month with <code>golangci-lint</code> tooling. I think this has been one of the most educational tools for learning Go I've come across. It has forced me to evaluate why issues are flagged, if they are opinionated preferences or best practices.</p> <p>For example, wsl ensures that statements are not cuddled. This follows Dave Cheney's writing about having an empty line seperate phrases of thought.</p> <p>It's a bit annoying to implement as a linter though, and can't be applied programaticaly so I'm not using that.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/#linting-shadowing-package-names","title":"Linting - Shadowing Package Names","text":"<p>Another one that I caught from Goland linting today, that <code>golangci-lint</code> didn't seem to catch, was the shadowing of a package name.</p> <p>In this scenario I found code where:</p> <pre><code>package taco\n\n\nfunc main() {\n taco := taco.Method()\n}\n</code></pre> <p>While this is legal, it's a confusing practice, and thereafter prohibits the usage of the <code>taco</code> package as it's been overshadowed by the variable.</p> <p>To me this is a clear violation of Go's preference for \"no magic\" and readability.</p> <p>In this scenario, the fix is simple. Change the variable name used or alias the package (my preference).</p> <pre><code>package (\n  pkgtaco \"taco\"\n)\n\n\nfunc main() {\n taco := pkgtaco.Method()\n}\n</code></pre>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/#linting-handling-errors","title":"Linting - Handling Errors","text":"<p>Also did some investigation on <code>errcheck</code> and flagging of handling file close and response body closing for http. This is one of those areas that linters flag and it's a \"it depends\" and not very consistent.</p> <p>Basically the gist is ignore, except if file writing is occuring then it's probably needing an explicit handle.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-77/#links","title":"links","text":"<ul> <li>go - Should shadowing Package Namespace with Local Variables be strictly avoided? - Stack Overflow</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-78/","title":"Go R1 Day 78","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-78/#progress","title":"progress","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-78/#ultimate-go-234-pointers-part-4-stack-growth","title":"Ultimate Go: 2.3.4 - Pointers-Part 4 (Stack Growth)","text":"<p>Scenario:</p> <ul> <li>2k stack for each goroutine.</li> <li>50,000 goroutines.</li> <li>Eventually, you'll want to make the function call and you'll want to grow the stack if the current stack limit is hit.</li> <li>We'll want to use contigous stacks.</li> <li>The new stack will be a new contigous allocated block of memory.</li> <li>The stack growth requires all the prior values to be moved over to the new doubled stack.</li> </ul> <p>A goroutine can only share values from the heap. This prevents the issues occuring from shared values in different stacks.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-78/#ultimate-go-235-pointers-part-5-gc","title":"Ultimate Go: 2.3.5 - Pointers-Part 5 (GC)","text":"<p>Mark and sweep collector.</p> <p>We don't need to worry about the implementation.</p> <p>However, this topic is useful to ensure we write code that is \"sympathetic\" to the GC.</p> <p>At this point, I opted to come back to GC details and focus on some testing and package design principles.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-79/","title":"Go R1 Day 79","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-79/#progress","title":"progress","text":"<p>Felt like revisiting Exercism since the great updates and wanting a break from the more deep concept dives on mechnical sympathy and optimization in the Ultimate Go course (which is stellar).</p> <p>Completed the following to continue on syllabus. It's taking me back to basics, but stuff I can knock out pretty quick.</p> <ul> <li>Weather Forcast code commenting concepts.</li> <li>Cars Assemble covering basic if statements... yawn. Used switch statement for kicks</li> <li>Annalyn's Infiltration boolean concepts.</li> </ul> <p>From this I determined that I'm great at writing chained boolean statements.</p> <pre><code>if goodAtBool &amp;&amp; coderIsMe {\n    fmt.Println(\"I'm a good coder\")\n} else if goodAtBool &amp;&amp; !coderIsMe {\n    fmt.Println(\"I'm a good coder\")\n} else if !goodAtBool &amp;&amp; coderIsMe {\n    fmt.Println(\"I'm a bad coder\")\n} else {\n    fmt.Println(\"I'm a bad coder\")\n}\n</code></pre> <p>Thank you GitHub Copilot for the codeblock above. I claim no responsibility to the results of what AI generated madness was created.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-80/","title":"Go R1 Day 80","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-80/#progress","title":"progress","text":"<p>Built a Mage task to handle creation of Azure DevOps pull requests. Since the tooling out there is primarily around Github, this was useful to help standardize PR creation with autocomplete, conventional commit naming, and other properties that typically require manual changes. I found a great little TUI components library that simplified using Bubbletea components: Promptkit.</p> <p>In addition, noticed some new linting help from golangci-lint for <code>varnamelen</code>.</p> <p>This was useful as it analyzes the brevity of variable names and if the variable name is too short, say 1-3 characters, but the usage extends 20 lines away, it will flag it. This is good as short variable names are designed for local context, while longer descriptive names provide better readability further away in the code.</p> <p>Practical Go: Real world advice for writing maintainable Go programs - Identifier Length</p> <p>Golangci-lint tool includes this linter: Varnamelen</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"keyboard-remapping-in-goland/","title":"Keyboard Remapping in Goland","text":"<p>This is a continuation of my evaluation using Goland.</p> <p>I tend to be a VSCode user primarily, but am working on adopting Goland incrementally to leverage some of it's features that work a bit more consistently (like refactoring).</p> <p>Losing keyboard shortcuts is painful.</p> <p>VSCode lets you assign keyboard shortcuts in command pallette by clicking on the gear icon in the command you hover over. For a while I couldn't find this in Goland. I see now I can assign any action defined from the quick open menu by pressing <code>Cmd+.</code>.</p> <p>This should expedite keyboard shortcut customization to align to my muscle memory a bit easier.</p>","tags":["tech","development","microblog","goland","golang"]},{"location":"go-r1-day-81/","title":"Go R1 Day 81","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-81/#progress","title":"progress","text":"<ul> <li>Simple string parsing, using replaceall, regexp, and more</li> <li>Listened to a talk on Hexagonal Architecture, which really helped answer some package questions I'd unclear on.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-81/#links","title":"links","text":"<ul> <li>Exercism - Tech Palace</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-82/","title":"Go R1 Day 82","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-82/#progress","title":"progress","text":"<ul> <li>A few small things like Binary search.</li> <li>More listening on hexagonal architecture design.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-83/","title":"Go R1 Day 83","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-83/#progress","title":"progress","text":"<p>Revisited Ultimate Syntax (Ardan Labs - Bill Kennedy) and went back through various topics such as:</p> <ul> <li>Variables: When to use var vs walrus operator for readability and zero value initialization.</li> <li>Type Conversions: How identical named types aren't the same in Go's eyes when named.</li> <li>Pointers: General overview. Mostly stuff I knew, but good basic overview again.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"thermal-throttling-mac-intel-woes/","title":"Thermal Throttling Mac Intel Woes","text":"<p>I lost roughly half a day in productivity. CPU hit 100% for an extended period, making it difficult to even use the terminal.</p> <p>Initially, I thought the culprit was Docker, as it was running some activity with local codespaces and linting tools. Killing Docker did nothing.</p> <p>Htop pointed out kernel as the primary hog, and you can't kill that.</p> <p>After digging around online, I found further mentions about charging on the right side, not the left to avoid thermal issues causing CPU throttling.</p> <p>The white charger cable wasn't plugged in. The phone charger was, but the white cable to the laptop charger wasn't.</p> <p>I was drawing power from the dock, which doesn't provide the same output as the Apple charger (seems to be a common issue).</p> <p>This Stack Exchange question pointed me back to checking the charging: macos - How to find cause of high kernel_task cpu usage? - Ask Different</p> <p>I was skeptical of this being the root cause of kernel CPU usage, but once I plugged in the charger, the CPU issue resolved itself within 30 seconds.</p> <p>This is completely ridiculous. If throttling is occurring, a polished user experience would be to notify of insufficient power from charger, not hammer my performance. Additionally, it seems odd how many docking stations I've looked at for my Mac don't provide the minimum required power to sustain heavy usage.</p> <p>While I still enjoy using the Mac, having 4 cables coming out from it to use at my desk compared to my older Lenovo/HP docking station experience feels like a subpar experience.</p>","tags":["tech","development","microblog"]},{"location":"go-r1-day-84/","title":"Go R1 Day 84","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-84/#progress","title":"progress","text":"<p>Ultimate Syntax (Ardan Labs - Bill Kennedy) and went back through various topics such as:</p> <ul> <li>Pointers: One thing mentioned that resonated with me was the confusion regarding pointers in parameter declarations. I also find the usage strange that the deference operator is used to denote a pointer value being dereferences in the parameter. I'd expect a pointer value to pass clearly with <code>func (mypointer &amp;int)</code> and not <code>func (mypointer int)</code> with a pointer call.</li> <li>Literal Structs: Great points on avoiding \"type exhaustion\" by using literal structs whenever the struct is not reused in multiple locations.</li> <li>Constants: Knowing that there is a parallel typing system for constants with \"kind\" vs \"type\" being significant helped me wrap my head around why constants often don't have explicit type definitions in their declaration.</li> </ul>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-84/#iota","title":"Iota","text":"<p>This is one of the most confusing types I've used.</p> <ul> <li>Iota only works in a block declaration.</li> </ul> <pre><code>const (\n  a = iota + 1  // Starts at 0\n  b             // Starts at 1\n  c             // Starts at 2\n)\n</code></pre> <p>Also showed using <code>&lt;&lt; iota</code> to do bit shifting. This is common in log packages (I'll have to look in the future, as bit shifting is something I've never really done).</p> <p>Become of kind system, you can't really make enumerators with constants.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-84/#best-practices","title":"Best Practices","text":"<p>Don't use aliases for types like <code>type handle int</code> in an effort. While it seems promising, it doesn't offer the protection thought, because of \"kind\" protection.</p> <p>This is because \"kind promotion\", it destroys the ability to truly have enumerations in Go by aliasing types.</p> <p>I've seen <code>stringer</code> used in some articles as well, but not certain yet if it's considered idiomatic to approach enum like generation this way.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-85/","title":"Go R1 Day 85","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-85/#progress","title":"progress","text":"<p>\ud83c\udf89Finished Ultimate Syntax course.</p> <p>Worked on Enumerators concept using <code>iota</code>.</p> <p>I still find this very confusing in general.</p> <p>Here's the gist I created.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"docker-healthchecks-for-spinning-up-local-stacks/","title":"Docker Healthchecks for Spinning Up Local Stacks","text":"<p>I've used a few approaches in the past with \"wait-for-it\" style containers.</p> <p>Realized there's some great features with healthchecks in Docker compose so I decided to try it out and it worked perfectly for Docker compose setup.</p> <p>This can be a great way to add some container health checks in Docker Compose files, or directly in the Dockerfile itself.</p> <pre><code>---\nversion: '3'\n\nnetworks:\n  backend:\n  database:\n\nvolumes:\n  mysql-data:\n\nservices:\n  redis:\n    image: redis\n    ports:\n      - 6379:6379\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 1s\n      timeout: 3s\n      retries: 30\n  mysql:\n    image: mysql:5.8\n    env_file: ../env/.env # or use another path\n    volumes:\n      - mysq-data:/var/lib/mysql\n      # This is the initialization path on first create\n      # Anything under the directory will be run in order (so use sorted naming like 01_init.sql, 02_data.sql, etc)\n      - ../db/myql/schema/:/docker-entrypoint-initdb.d\n    ports:\n      - 3306:3306\n    networks:\n      - database\n    healthcheck:\n      #test: \"/etc/init.d/mysql status\"  &gt; didn't work\n      # The environment variable here is loaded from the .env file in env_file\n      test: mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD\n      interval: 1s\n      timeout: 3s\n      retries: 120\n\n    ### example api service that now depends on both redis and mysql to be healthy before proceeding\n    api:\n    image: api:latest\n    env_file: ../env/.env\n    ports:\n      - 3000:3000\n    networks:\n      - backend\n      - database\n    depends_on:\n      mysql:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n</code></pre>","tags":["tech","development","microblog","docker"]},{"location":"setup-sourcegraph-locally/","title":"Setup Sourcegraph Locally","text":"<p>I went through the Sourcegraph directions, but had a few challenges due to the majority of code being behind SSH access with Azure DevOps.</p> <p>Finally figured out how to do this, with multiple repos in one command and no need to embed a token using https.</p> <p>Navigate to: manage-repos and use this.<sup>1</sup> Better yet, use Loading configuration via the file system (declarative config) - Sourcegraph docs and persist locally in case you want to upgrade or rebuild the container.</p> <pre><code>{\n  \"url\": \"ssh://git@ssh.dev.azure.com\",\n  \"repos\": [\n    \"v3/{MYORG}/{PROJECT_NAME}/{REPO}\",\n    \"v3/{MYORG}/{PROJECT_NAME}/{REPO}\"\n  ]\n\n}\n</code></pre> <p>For the json based storage try:</p> <pre><code>  {\n      \"GITHUB\": [],\n      \"OTHER\": [\n          {\n              \"url\": \"ssh://git@ssh.dev.azure.com\",\n              \"repos\": [\n                \"v3/{MYORG}/{PROJECT_NAME}/{REPO}\",\n                \"v3/{MYORG}/{PROJECT_NAME}/{REPO}\"\n              ]\n          }\n      ],\n      \"PHABRICATOR\": []\n  }\n</code></pre> <p>To ensure SSH tokens are mounted, you need to follow-up the directions here: SSH Access for Sourcegraph</p> <pre><code>cp -R $HOME/.ssh $HOME/.sourcegraph/config/ssh\ndocker run -d \\\n  -e DISABLE_OBSERVABILITY=true \\\n  -e EXTSVC_CONFIG_FILE=/etc/sourcegraph/extsvc.json \\\n  --publish 7080:7080 \\\n  --publish 127.0.0.1:3370:3370 \\\n  --volume $HOME/.sourcegraph/extsvc.json:/etc/sourcegraph/extsvc.json:delegated \\\n  --volume $HOME/.sourcegraph/config:/etc/sourcegraph:delegated \\\n  --volume $HOME/.sourcegraph/data:/var/opt/sourcegraph:delegated \\\n  sourcegraph/server:3.34.1\n</code></pre> <p></p>","tags":["tech","development","microblog","golang"]},{"location":"setup-sourcegraph-locally/#lsif-for-go","title":"LSIF For Go","text":"<p>I didn't get this to work yet with my internal repos, but it's worth pinning as Go module documentation for API docs can be generated for review as well. Change <code>darwin</code> to <code>linux</code> to use the linux version.</p> <pre><code>go install github.com/sourcegraph/lsif-go/cmd/lsif-go@latest\nsudo curl -L https://sourcegraph.com/.api/src-cli/src_darwin_amd64 -o /usr/local/bin/sourcegraph\nsudo chmod +x /usr/local/bin/sourcegraph\n</code></pre> <p>{{&lt; admonition type=\"Tip\" title=\"Docker\" open=true &gt;}}</p> <pre><code>docker pull sourcegraph/lsif-go:v1.2.0\n</code></pre> <p>{{&lt; /admonition &gt;}}</p> <p>Now index code in repo</p> <pre><code>lsif-go\nsourcegraph_host=http://127.0.0.1:7080\nsourcegraph -endpoint=$sourcegraph_host lsif upload\n</code></pre> <ol> <li> <p>I removed <code>--rm</code> from the tutorial.\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","microblog","golang"]},{"location":"go-r1-day-86/","title":"Go R1 Day 86 - Wrap Up!","text":"","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-86/#finished","title":"Finished!","text":"<p>Done! I've pretty much done above and beyond 100 days, but finding the blogging format to take a lot more effort to keep up when I'm doing a mix of puzzles, courses, and work.</p> <p>Since my full-time job has Go development as a part of it now, I've exceeded this and going to track any future training goals in a lower overhead way, such as GitHub issues or such.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-86/#was-it-worth-it","title":"Was It Worth It?","text":"<p>Yes, it was worth it. It helped me break down a large amount of learning back into a daily rythm of dedicated study. For me, doing full time development, I found it was hard since I do code a big chunk of the day to document all the time.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"go-r1-day-86/#what-would-i-do-differently","title":"What would I do differently?","text":"<p>Probably would minimize the effort of documenting the process itself. While it's great to save notes and articulate things, I feel either saving the notes as part of the git log or algorithm style repo would be less trouble. Also, some of the work is in various platforms like Leetcode, which aren't easy to straight extract. Reduce the overhead and focus on documenting core principles or concepts that would be useful in a wiki style format, but not log as much.</p> <p>Using Github Issues might work really well too, because you could post them to a log later in bulk, but otherwise the cli driven creation and kanban board approach would minimize the overhead. That would be cool too cause you could have bots run todos, stale items, and other cool things for you.</p>","tags":["tech","development","100DaysOfCode","golang","microblog"]},{"location":"helm-is-like-hugo/","title":"Helm Is Like Hugo","text":"<p>Turns out helm is pretty intuitive if you already have been working with something like Hugo, which is Go template driven.</p> <p>Was able to convert an entire K8 stack to helm with a couple hours of work and render everything.</p> <p>I have this problem of trying to avoid percieved complex tools in an attempt to reduce \"another tool\" syndrome for others I work with. Sometimes, it's important to keep in mind who is editing and doing the majority of the work, and not worry as much about the long-term solution over delivery.</p> <p>That's always a tough balance since I tend to think outside the scope of a single team due to the style of work I've done. I think I'm slowly getting there. \ud83d\ude00</p>","tags":["tech","development","microblog","kubernetes"]},{"location":"Pulumi-round-2/","title":"Pulumi Round 2","text":"<p>This is not a comprehensive article, but more a log of the issues and wins as I work through Pulumi adoption.</p>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#tldr","title":"tl;dr","text":"<ul> <li>Pulumi is pretty powerful.</li> <li>Once you get beyond the basics, it requires a lot of effort since the tooling doesn't have as many examples as I'd hope. This is especially true for Kubernetes. It's a lot easier to get moving on other providers.</li> <li>It's more intensive of a pilot to do complex things IMO than Terraform, because you aren't getting pre-built modules that just have all the stack done automatically (at least in Go).<sup>1</sup></li> <li>Certain things that were painful in Terraform just work in Pulumi. For example, renaming the underlying org was super easy, removing stuck items from state, and other actions that with Terraform are much trickier.</li> </ul>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#where-i-started","title":"Where I Started","text":"<ul> <li>I have replicated basically what Helm gives you for an external api template out of the box and am doing this in Pulumi.</li> <li>This uses the Pulumi config and loads into a config struct.</li> <li>I'm running CICD with this to build and deploy a container, and finally run a task to run Pulumi against a Kubernetes cluster to apply an updated deployment based on that new semver tag.</li> </ul>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#challenge-1-authenticating-via-k8-namespace-scoped-service-account","title":"Challenge 1: Authenticating Via K8 Namespace Scoped Service Account","text":"<p>Figuring out the balance between default provider that inherits your Kubeconfig and being more explicit by configuring this myself has been a challenge.</p> <p>I took two days to work through the proper Service account level authorization, but finally with the help of a little Mage I have a running login task that wraps up a sequence of steps for me to authenticate purely based on K8 Service account credentials.</p> <ul> <li>TODO: Also plan on checking out the Azure Key Vault integration, as this is pretty neat. You can set it as the backend provider for your secret rather than Pulumi.</li> </ul>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#attempting-to-embed-service-account-credentials-in-stack","title":"Attempting to Embed Service Account Credentials in Stack","text":"<p>Pulumi does per stack level encryption of secrets, so I thought this might be really nice to setup and try. This would mean any of my team could run the plan with the specific service account credentials already embedded to quickly apply a development fix or test against a development enviornment.</p> <p>This is in constrast this to building a kubeconfig locally and pointing to that. That works, but does feel more brittle.</p> <p>I tried this:</p> <pre><code>pulumi --cwd pulumi/myproject --stack=myorg/myproject/dev config set --secret --path 'kubernetes:kubeconfig' \"$(cat /workspaces/myproject/.cached/.kube/tmp.kube.config)\"\n</code></pre> <p>NOTE: Optionally use without <code>--path</code>.</p> <p>This temporary configuration I generated via the kubectl commands for setup.</p> <p>Now I see:</p> <pre><code>config:\n    kubernetes:cluster: mycluster\n    kubernetes:context: mycontext\n    kubernetes:kubeconfig:\n        secure: mysupersecretencryptedvaluebyPulumipus\n</code></pre> <p>Next I made some changes to the Pulumi plan that checked for <code>KUBECONFIG</code> and allowed the override as usual, however if not provided it defaulted to the value stored in the configuration. Will think on that and probably remove the extra code later.</p> <p>Didn't get this to work, so I reverted back to passing in the generated kubeconfig file instead. This was set by a command like this:</p> <pre><code>pulumi --cwd pulumi/myproject --stack=myorg/myproject/dev config set --path 'kubernetes:kubeconfig' /workspaces/myproject/.cached/.kube/tmp.kube.config\n</code></pre> <p>Failure.</p> <p>I still kept getting the dreaded:</p> <pre><code>configured Kubernetes cluster is unreachable: failed to parse kubeconfig data in kubernetes:config:kubeconfig\n</code></pre> <p>I tried <code>KUBECONFIG=/workspaces/myproject/.cached/.kube/tmp.kube.config pulumi --cwd pulumi/myproject --stack=myorg/myproject/dev up --diff</code> and it detected the new <code>kubernetes:kubeconfig</code> and put in the state.</p> <p>\u2705 BOOM! Once I did this refresh of the state, it seemed to properly allow me to connect.</p> <p>I found that ensuring <code>KUBECONFIG=path pulumi ...</code> helped a lot, but I think the KUBECONFIG path in the state file also mattered and had to be refreshed.</p> <p>Ok... to ensure this was the issue I did this test:</p> <ol> <li>Stop using my credential and point to the service account generated kubeconfig.</li> <li>Run <code>KUBECONFIG={newkubeconfig} pulumi destroy</code>.<ol> <li>It detected the change when running up, I just didn't apply.</li> </ol> </li> <li>Deletion worked.</li> </ol> <p>To me this points towards the <code>KUBECONFIG</code> correctly being used when passed into the provider, so I can run without refreshing the state even though a refresh/up will indicated it detected the change.</p> <p>I probably need to trim this code, but to support KUBECONFIG explicitly being provided I did something like this:</p> <p>{{&lt; gist sheldonhull 764d2702bf9e783fca0263bbabd598a7 &gt;}}</p>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#challenge-handle-config-values-that-change","title":"Challenge: Handle config values that change","text":"<p>I thought maybe I could dynamically set the kubeconfig at runtime using the <code>--config</code> flag. I looked in the GitHub repo and couldn't find any examples, and the cli doesn't provide any I could see.</p> <p>Here's a little taste of what I ran:</p> <pre><code>    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig /workspaces/myproject/.cached/.kube/tmp.kube.config'\n    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig' '/workspaces/myproject/.cached/.kube/tmp.kube.config'\n    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig: /workspaces/myproject/.cached/.kube/tmp.kube.config'\n    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig','/workspaces/myproject/.cached/.kube/tmp.kube.config'\n    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig,/workspaces/myproject/.cached/.kube/tmp.kube.config'\n    Pulumi up --diff --refresh --config 'kubernetes:kubeconfig' /workspaces/myproject/.cached/.kube/tmp.kube.config\n    Pulumi up --diff --refresh --config '{ \"kubernetes:kubeconfig\": /workspaces/myproject/.cached/.kube/tmp.kube.config\n    Pulumi up --diff --refresh --config '{ \"kubernetes:kubeconfig\": \"/workspaces/myproject/.cached/.kube/tmp.kube.config\" }'\n    Pulumi up --diff --refresh --config '\"kubernetes:kubeconfig\": \"/workspaces/myproject/.cached/.kube/tmp.kube.config\"'\n    Pulumi up --diff --refresh --config kubernetes:kubeconfig /workspaces/myproject/.cached/.kube/tmp.kube.config\n</code></pre> <p>The variety of changes this made the kubeconfig were entertaining at least. I moved on, as this didn't seem to be a valid way to work.</p> <p>Pretty sure the cli was laughing at me.</p> <pre><code>    config:\n        kubernetes:cluster: mycluster\n        kubernetes:context: mycluster\n        kubernetes:kubeconfig,/workspaces/myproject/.cached/.kube/tmp.kube.config: \"\"\n        kubernetes:namespace: mynamespace\n</code></pre>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#challenge-how-do-i-use-replace","title":"Challenge: How Do I Use Replace?","text":"<p>While mostly intuitive, the darn cli docs are missing some jump start examples. I'm pretty sure most of us want examples, and I'd like to see exhaustive examples on the docs page.</p> <p>I tried to force a replacement of a deployment:</p> <pre><code>    pulumi up --diff --replace 'myproject-dev-deployment'    \u274c NOT FOUND\n    pulumi up --diff --replace 'kubernetes:apps:Deployment'  \u274c NOT FOUND\n</code></pre> <p><code>--replace stringArray</code> Specify resources to replace. Multiple resources can be specified using --replace urn1 --replace urn2 pulumi up docs</p> <p>Not sure of the urn, as I couldn't find in the state file. Decided to go with destroy \ud83d\udd25 and redeploy to be (not)safe. Hey it's a dev environment after all!</p>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#mapping-from-yaml","title":"Mapping from Yaml","text":"<p>Ran into an edge case. I set config values that had a slash.</p> <pre><code>myconfig:\n    subsection:\n        io/foo: 123\n</code></pre> <p>This seemed to read all the zero values and couldn't obtain them. I went and replaced all of these with a command like: <code>pulumi --cwd pulumi/myproject --stack=myorg/myproject/dev config set --path 'data.podannotations.myservice.myport' 80</code>.</p> <p>No luck. Figured it might be something to do with maps, but I couldn't find anything with Go having issues with the key value having a slash or such.</p> <p>Gave this a shot and \ud83c\udf89 it worked.</p> <pre><code>type MyConfig struct {\n    Onlyyaml          bool   `yaml:\"onlyyaml\"`\n    SpecialNested     struct {\n        Enabled              bool   `yaml:\"enabled\"`\n    } `yaml:\"specialnested,inline\"` // \ud83d\udc48 inline is needed\n}\n</code></pre> <p>This parsed the value:</p> <pre><code>config:\n    specialnested:\n        enabled: true\n</code></pre> <p>Now I know. I didn't see anything about using this when reading the docs, so that's another one that shows it requires expertise in both Pulumi + the native language to figure out what's wrong.</p> <p>Embedded structs are not treated as embedded in YAML by default. To do that add the \",inline\" annotation below. Unmarshal for yaml.v2</p> <p>I couldn't find the exact package being used due to time, but it's possible that this was forked off the original package here: pulumi-go-yaml.</p>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#task-runner","title":"Task Runner","text":"<p>I standardize all my projects, personal and work with Mage.</p> <p>Was pretty easy to integrate with pulumi, even though their CLI is pretty awesome and easy to use.. However, I prefer all tools that aren't one off's have a simple standardized way to execute.</p> <p>For me this entailed: <code>mage pulumi:diff myproject dev</code> and it ensured all the command line flags and such were setup.</p> <p>I put some sample tasks on my magetools repo: Magetools - Examples - Pulumi.</p> <p>Some examples from that:</p> <pre><code>// Pulumi namespace contains task to help with running Pulumi tools.\ntype Pulumi mg.Namespace\n\n// Get returns the fully qualified Pulumi stack name, including the org, project, and stage.\n// This looks like `myorg/project/stage`.\nfunc GetPulumiStackName(project, stage string) string {\n    mtu.CheckPtermDebug()\n    return strings.Join([]string{PulumiOrg, project, stage}, \"/\")\n}\n\n\n// \ud83d\ude80 Up \ud83d\udc49 Parameters(project, stack string): Eg: `mage pulumi:up myproject dev`.\n// Runs pulumi up/apply to target.\n//\n// Example: `mage pulumi:up myproject dev`.\nfunc (Pulumi) Up(project, stage string) error {\n    mtu.CheckPtermDebug()\n    return sh.RunV(\n        \"pulumi\",\n        \"--cwd\",\n        filepath.Join(PulumiProjectDir, project),\n        \"--stack=\"+GetPulumiStackName(project, stage),\n        \"up\",\n        \"--yes\",\n        \"--emoji\",\n    )\n}\n</code></pre>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"Pulumi-round-2/#my-feedback","title":"My Feedback","text":"<ul> <li> Provide several examples for using the flags, such as <code>--config</code> as I couldn't figure out how to pass the string array in a way that made sense.</li> <li> I've seen others post on issues as well about confusion with the provider. Much of the examples rely on the default provider. For me that was problematic when I wanted to try and work locally but then use a service account credential only for pulumi to test. Make some more examples on how to easily do that.</li> <li> Provide help on Stack Overflow, Github Discussions, or a dedicated discourse to promote visibility and knowledge resharing on solutions. Preference would be Github discussions as it's easy to find help inline with issues. Slack is a terrible place to find info in comparison and responses can be erratic and slow (which I get considering how stretched everyone must be!)</li> <li> Maybe the logger makes sense for automation api or something else, but I'd love to see the default logger support structured logging (say through zerolog) or something a bit less clunky. Having to replace the string values like this made from some clunky logging code. <code>_ = ctx.Log.Error(fmt.Sprintf(\"Unable to read Kubeconfig override: %q, %v\", kubeConfigOverride, err), nil)</code>. I'd like to just call: <code>pulumi.Log.Info().Str(\"myval\",strval).Msg(\"this is important\")</code>.</li> </ul> <ol> <li> <p>They are making progress on this with Crosswalk, but Go isn't in there at this time.\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","microblog","kubernetes","go"]},{"location":"my-job/","title":"My Job","text":"<p>Acted like I wasn't going to go to work today with my 4 year old daughter. She looked over at me and told me I don't have to go somewhere, I just walk up the stairs. \"Your boss will want you to go to work today\". Very astitute, I observed.</p> <p>But what do I do when I go to work? Do I fix cars, construction, or other things?</p> <p>She looked at at me and said I put on my headset and talk to people about important stuff.</p> <p>No cool computer hacker cred for me today.</p>","tags":["ramblings","microblog","dadlife"]},{"location":"reflex-for-quick-filewatching-commands/","title":"Reflex for Quick Filewatching Commands","text":"<pre><code>go install github.com/cespare/reflex@latest\n</code></pre> <p>Then you can run a command like:</p> <pre><code>reflex -r 'nginx.conf' -- curl -v -L http://127.0.0.1:8080 2&gt;&amp;1 | grep -i \"^&lt; location:\\|HTTP/1.1\"\n</code></pre> <p>You should see triggered output from the command whenever the file is saved.</p> <p>Nice work @cespare. Found this pretty useful to speed up testing cycles with some cli tools.</p> <p>Follow: Caleb and twitter for more cool Go magic.</p>","tags":["tech","development","microblog"]},{"location":"go-templates-with-kubectl/","title":"Go Templates With Kubectl","text":"<p>An alternative to using jsonpath with kubectl is go templates!</p> <p>Try switching this:</p> <p><code>`shell kubectl get serviceaccount myserviceaccount --context supercoolcontext --namespace themagicalcloud -o jsonpath='{.secrets[0].name}'</code></p> <p>To this and it should work just the same. Since I know go templates pretty well, this is a good alternative for jsonpath syntax.</p> <p><code>`shell kubectl get serviceaccount myserviceaccount --context supercoolcontext --namespace themagicalcloud -o go-template='{{range .secrets }}{{.name}}{{end}}'</code></p> <p>Further reading:</p> <ul> <li>List Container images using a go-template instead of jsonpath</li> </ul>","tags":["tech","development","microblog","go","kubernetes"]},{"location":"night-owl-for-slack/","title":"Night Owl for Slack","text":"","tags":["tech","development","microblog","slack"]},{"location":"night-owl-for-slack/#night-owl-for-slack","title":"Night Owl for Slack","text":"<p>Open Slack preferences, sidebar, and paste this to get your Night Owl theme going:</p> <pre><code>#011627,#1D3B53,#1D3B53,#82AAFF,#1D3B53,#D6DEEB,#ADDB67,#7E57C2,#0A151F,#D6DEEB\n</code></pre> <p>Thanks to night-owl-slack for providing the colors!</p>","tags":["tech","development","microblog","slack"]},{"location":"pulumi-secrets/","title":"Pulumi Secrets","text":"","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#chicken-or-the-egg","title":"Chicken or The Egg","text":"<p>Secrets with infrastructure are a problem.</p> <ul> <li>I want to source control my secrets, and deploy via CD.</li> <li>I want to use infrastructure-as-code to define my configuration, including secrets.</li> </ul> <p>But I don't want to expose the values at any stage during this. I want to keep them in a secret store, and only expose them when I need them, but I still want to manage them via code.</p> <p>For a while I've done a blend of loading local environment variables or creating manually in a vault, and then using in other ways.</p> <p>Terraform makes this tricky IMO, as if you want to create a secret like an Azure DevOps library variable, SSM parameter, or other value, you need to go and add this into Terraform cloud as a secret value, use a third party cli that helps do this for you, or some other work around.</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#other-alternatives","title":"Other alternatives","text":"<p>There are tools that allow you to encrypt your files in git, but it adds another layer of complexity, and often isn't cross-platform compatible.</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#pulumi-secret-encryption","title":"Pulumi Secret Encryption","text":"<p>Pulumi's solution seems like a non-brainer ingenious way to solve this.</p> <p>When using their remote state storage, you can use the cli to easily flag a value as a secret, and the value is encrypted in your yaml configuration (you can configure one outside of the yaml too if you wish I think).</p> <pre><code>pulumi config set --secret --secret-key=my-secret-key my-secret-value\n</code></pre>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#some-practical-examples","title":"Some Practical Examples","text":"<p>I wanted to embed some encrypted kubeconfigs for designated clusters to do deployments.</p> <p>Assuming you have a directory of some service account kubeconfigs, you could run this shell script and add encrypted configurations for each of your clusters that pulumi could then read.</p> <pre><code>pulumicwd=pulumi/myfancyproject\npulumistack=myfancyproject/dev\nClusterIndex=0\nfor i in $(ls .mykubeconfigs/kubeconfig-svc-account--*.yaml); do\n  echo \"Processing $i: ClusterIndex: $ClusterIndex\"\n  kubecontent=$(cat $i | gojq --compact-output --yaml-input)\n  clustername=$(echo $kubecontent | gojq '.clusters[0].name' --raw-output)\n  pulumi --cwd $pulumicwd --stack $pulumistack config set --path \"clusters[${ClusterIndex}].name\"  $clustername\n  cat $i | gojq --compact-output --yaml-input | pulumi --cwd $pulumicwd --stack $pulumistack config set --secret --path \"clusters[${ClusterIndex}].kubeconfig\"\n  ClusterIndex=$((ClusterIndex+1))\ndone\necho \"All done, the magic has been done, toast your kubeconfigs and send Sheldon a coffee. \u2615 (and copilot for helping write this so quickly)\"\n</code></pre> <p>This would result in the following pulumi yaml being part of your stack:</p> <pre><code>---\nconfig:\n  myfancyproject:clusters:\n    - clustername: clusteeeeergobrrrr01\n      kubeconfig:\n        secure: mumbojumbencryptedtexthere\n    - clustername: clusteeeeergobrrrr02\n      kubeconfig:\n        secure: mumbojumbencryptedtexthere\n</code></pre>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#consuming-this-in-go","title":"Consuming This in Go","text":"<p>What's cool is it's so freaking easy to work with it still in Pulumi. Since the encryption is per stack, as long as you are in the right stack, and as long as you specify explicitly that the value you want to load is a secret, you can just work with it almost as usual.</p> <p>In fact, all it takes is flipping <code>RequireObject</code> to <code>RequiredSecretObject</code>.</p> <p>Not everything is easy in Pulumi... for sure, but they freaking nailed this.</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#create-a-config-object","title":"Create A Config Object","text":"<pre><code>type Clusters struct {\n  Name       string `json:\"name,omitempty\"`\n  Kubeconfig string `json:\"kubeconfig,omitempty\"`\n}\n</code></pre>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#use-structured-secret-configuration-in-pulumi-plan","title":"Use Structured Secret Configuration In Pulumi Plan","text":"<p>Now that we have a config object, use Pulumi's configuration package to load the config directly into a pointer to a struct.</p> <pre><code>import  (\n    kubernetes \"github.com/pulumi/pulumi-kubernetes/sdk/v3/go/kubernetes\"\n    pulumi \"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n  config \"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\nfunc main() {\n  pulumi.Run(func(ctx *pulumi.Context) error {\n    configClusters := []Clusters{}\n    // config.RequireObject(\"clusters\", &amp;configClusters) // wouldn't give encrypted values, so we do the next line\n    config.RequireSecretObject(\"clusters\", &amp;configClusters) // wouldn't give encrypted values, so we do the next line\n\n    for _, cluster := range configClusters {\n      _ = ctx.Log.Info(fmt.Sprintf(\"starting up on cluster: %s... it go brrrr\", cluster.Name), &amp;pulumi.LogArgs{})\n      pargs := &amp;kubernetes.ProviderArgs{}\n      pargs.Kubeconfig = cluster.Kubeconfig\n      myProvider, err := kubernetes.NewProvider(ctx, \"k8\", pargs)\n      // now pass this myProvider to resources as the provider to use.\n    }\n  }\n}\n</code></pre> <p>Passing the provider is done per resources like this:</p> <pre><code>_, err = appsv1.NewDeployment(\n    ctx,\n    \"my-deployment-name\",\n    &amp;appsv1.DeploymentArgs{},\n    pulumi.Provider(prov),\n)\n</code></pre>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#security-is-per-stack","title":"Security Is Per Stack","text":"<p>Now... let's say you get worried about someone cloning this file and doing some devious with it.</p> <p>They go along and generate a new yaml file, grab your encrypted values, use pulumi to decrypt and then go and drop your cluster.</p> <p>Ain't gonna happen. The paranoid part of my DevOpsy brain thinks like this.</p> <pre><code>error: failed to decrypt encrypted configuration value 'cluster:kubeconfig': [400] Message authentication failed\nThis can occur when a secret is copied from one stack to another. Encryption of secrets is done per-stack and\nit is not possible to share an encrypted configuration value across stacks.\n</code></pre> <p>To me this felt like a failure I really was happy to see. Thank you Pulumi!</p> <p>Best part is no freaking <code>echo $val | base64 -d</code>... <code>&lt;looking at you kubernetes secret values&gt;</code>.</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#side-note-on-configuration","title":"Side Note On Configuration","text":"<p>I used a more complex config struct for the main naming convention enforcement and it worked great to simplify naming consistency. Might do an article on this sometime if anyone finds it useful.</p> <p>Example:</p> <pre><code>// ServiceDeployName returns a string formatted like `myapi-dev-myapp-{SUFFIX}` where suffix is provided on call.\nfunc (d *SvcConfig) ServiceDeployName(sharedConfig *SharedConfig, suffix string) string {\n    return strings.ToLower(\n        strings.Join([]string{d.Rolename, sharedConfig.Environment, d.Deployment, suffix}, \"-\"),\n    )\n}\n</code></pre> <p>This provided solid intellisense all the way through it and made it really easy to refactor naming all by updating my config package.</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#disclaimer","title":"Disclaimer","text":"<p>I'm a fan of role based access like AWS Role assumption, using limited generated access tokens, etc (see article on Leapp for a great example for local dev workflows)</p> <p>However, practically, you might have services or apps that you need to manage that the cost of setting all that up is very high, for example you do the majority of your work in AWS, but you have a development Kubernetes cluster and want to setup a limited service account to do some Gitpod stuff. You use this service account for limited scope permissions for deployments only, and this might be a great case of just embedding the kubeconfig directly into a Pulumi plan.</p> <p>Maybe you need your PAT embedded for a provider, now just add as an encrypted value to your stack and get work shipped.</p> <p>Improve later with more robust handling, but this will cut down the overhead of getting infra-as-code for a lot of the smaller projects!</p>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#futher-reading","title":"Futher Reading","text":"<ul> <li>Pulumi &amp; Secrets</li> </ul>","tags":["tech","development","pulumi","go"]},{"location":"pulumi-secrets/#tools-used-or-mentioned","title":"Tools Used Or Mentioned","text":"<ul> <li>Pulumi</li> <li>Gojq</li> <li>Copilot - It actually wrote 50% of the shell script example. I \u2764\ufe0f Copilot</li> <li>base64 cli</li> <li>Kubernetes</li> <li>Pulumi Structured Configuration &amp; associated Go Package: Config</li> </ul>","tags":["tech","development","pulumi","go"]},{"location":"remove-pending-operations-from-pulumi-state/","title":"Remove Pending Operations From Pulumi State","text":"<p>If you need to work through some issues with a deployment and remove a pending operation in Pulumi state file, you can do this pretty easily manually, or save a quick bit of scrolling and use <code>gojq</code> (or <code>jq</code> if you want).</p> <ul> <li>Export: <code>pulumi stack export --file state.json</code></li> <li>Align formatting: <code>cat state.json | gojq &gt; stateFormatted.json</code></li> <li>Remove pending operation: <code>cat stateFormatted.json | gojq '.deployment.pending_operations = []' &gt; stateNew.json</code></li> <li>Now you can compare the results without any whitespace variance.</li> <li>Import: <code>pulumi stack import --file stateNew.json</code></li> </ul>","tags":["tech","development","microblog","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/","title":"Using Randomization for Pulumi Kubernetes Resources","text":"","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#logical-names","title":"Logical Names","text":"<p>Pulumi requires unique logical names (URN) for tracking the state of resources. I ran into an issue with this once I expanded to a multi-cluster deployment since Pulumi began erroring on the logical name not being unique. Let's say you are defining a service:</p> <pre><code>_, err = corev1.NewService(\n        ctx,\n        \"myservice\", // \ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48 this is the logical name\n        &amp;corev1.ServiceArgs{\n            ApiVersion: pulumi.String(\"v1\"),\n            Kind:       pulumi.String(\"Service\"),\n            Metadata:   &amp;metav1.ObjectMetaArgs{\n                Namespace: pulumi.String(myNamespace),\n                Name:      pulumi.String(myAppName), // \ud83d\udc48\ud83d\udc48\ud83d\udc48\ud83d\udc48 This is the physical name\n            },\n)\n</code></pre> <p>You can normally omit the <code>Metadata.Name</code> and Pulumi will autogenerate this with a randomized suffix for you. This allows a style of \"create before destroy\" deployments you don't get with native kubectl apply style commands. Things get a little messy here as overriding the logical name requires a normal <code>String</code>.</p> <p>To allow maximum uniqueness, you can concatenate values in the logical name so that it's unique as you desire, such as <code>resourcename + cluster</code> for example.</p>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#using-random","title":"Using Random","text":"<p>Having used Terraform's random provider in the past, and found it incredibly helpful to stop relying on the uniqueness of names that I have to manage, I tried initially to do this with the Pulumi terraform version of random. It didn't work out too well. In the end, I realized this is where just having the power of the language itself makes perfect sense. I had a pointer from someone in this github discussion about using the <code>petname</code> package itself, but I didn't like that idea as it was a bit unwieldy and not designed for importing as a package. Trying to use the resource results in a problem as <code>pulumi.StringOutput</code> can't be used with string concantenation. Instead, you have to use the <code>ApplyT</code> and pass around the string output to other inputs accepting <code>pulumi.StringPtr</code> type.</p> <pre><code>petname.ID().ApplyT(func(id pulumi.ID) string {\n                return fmt.Sprintf(\"%v\", id)\n}).(pulumi.StringOutput)\n</code></pre> <p>This doesn't work because the output is still a <code>pulumi.StringOutput</code> and not a <code>string</code>.</p> <p>This would work for things like the physical name, but you can't get the string output as it's to be considered like a \"promise\" and not resolved till the end of the plan. Logical names require strings, not <code>pulumi.String</code>.</p>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#go-makes-it-simple","title":"Go Makes it Simple","text":"<p>I did a little searching for correctly converting strings into int hashes, and with the volume of deployments, a collision risk is ridiculously low (something like 1 in 1 billion?).</p> <p>Here's how I went about it. You can adapt this for your Pulumi plan. I went back to one of my favorites, gofakeit which provides a fantastic package for generating data. What's cool about this is that the generators offer a global <code>Seed</code> option so you can reliably regenerate the same random data.</p>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#setup","title":"Setup","text":"<p>@brianvoe on github did a great job with this <code>gofakeit</code> package.</p> <pre><code>go get \"github.com/brianvoe/gofakeit/v6\"\n</code></pre> <p>Add this to your imports</p> <pre><code>import (\n    \"github.com/brianvoe/gofakeit/v6\"\n)\n</code></pre> <p>Now for the hashing, I found a great MIT licensed library I grabbed two functions from here: util by @shomali11 on github</p> <pre><code>// FNV64a hashes using fnv64a algorithm\n//\n// Sourced from: https://github.com/shomali11/util/blob/master/xhashes/xhashes.go\nfunc FNV64a(text string) uint64 {\n    algorithm := fnv.New64a()\n    return uint64Hasher(algorithm, text)\n}\n\n// uint64Hasher returns a uint64\n//\n// Sourced from: https://github.com/shomali11/util/blob/master/xhashes/xhashes.go\nfunc uint64Hasher(algorithm hash.Hash64, text string) uint64 {\n    algorithm.Write([]byte(text))\n    return algorithm.Sum64()\n}\n</code></pre> <p>I set up a few methods on a configuration struct.</p> <pre><code>// Clusters returns a list of clusters.\ntype Clusters struct {\n    Name string `json:\"name,omitempty\"`\n}\n// setSeed sets the gofakeit global state to a specific\n// seed value based on the string input.\nfunc setSeed(s string) {\n    calcSeed := FNV64a(s)\n    v := int64(calcSeed)\n    gofakeit.Seed(v)\n}\n// animalString returns a string formatted with `{DescriptiveAdjective}-{Animal}`.\nfunc animalString() string {\n    animal := gofakeit.Animal()\n    adjective := gofakeit.AdjectiveDescriptive()\n    return strings.ToLower(strings.Join([]string{adjective, animal}, \"-\"))\n}\n</code></pre> <p>Now, once you load a configuration into the struct using the Pulumi configuration package, you can obtain a randomized petname on demand, that will be repeatable and only change if the cluster name is different.</p> <p>{{&lt; admonition type=\"Tip\" title=\"If your uniqueness requirements change...\" open=true &gt;}}</p> <p>If your business requirements for uniqueness change, such as a combination of <code>resource+cluster</code> now needing to be deployed in duplication across another namespace (for example for provisioning development environments on demand)... you can just change the input seed from cluster to a combination of other values and you'll generate new unique seeds from there.</p> <p>{{&lt; /admonition &gt;}}</p> <pre><code>// PetName returns a unique petname for logical resources to be uniquely named in the Pulumi state file.\n// This is formatted as `adjective-animal`.\n// For logical name purposes, use the PetNameSuffix method instead.\nfunc (c *Clusters) PetName() string {\n    // Calculate a seed value based on cluster name and then generate a random petname for the reosurces so that the logical names stay truly unique even in multi-cluster environments.\n  setSeed(c.Name)\n    randomPet := animalString() // Random based on seed, so should be repeatable for different deploys.\n    return randomPet\n}\n\n// PetName returns a unique petname suffix for easy string concantenation for logical resources.\n// This is formatted as `-adjective-animal` with a preceding.\n// You'd join like `_, err = appsv1.NewDeployment(ctx, \"myapp\" + config.PetNameSuffix(), nil)\nfunc (c *Clusters) PetNameSuffix() string {\n    // Calculate a seed value based on cluster name and then generate a random petname for the reosurces so that the logical names stay truly unique even in multi-cluster environments.\n  setSeed(c.Name)\n    return \"-\" + animalString() // Random based on seed, so should be repeatable for different deploys.\n}\n</code></pre>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#quick-and-dirty-option","title":"Quick and Dirty Option","text":"<p>If you just want to do it all in <code>main()</code> and ignore the frowning of the \"best practice police\" just inline it.</p> <pre><code>calcSeed := FNV64a(cluster.Name)\nv := int64(calcSeed)\ngofakeit.Seed(v)\nanimal := gofakeit.Animal()\nadjective := gofakeit.AdjectiveDescriptive()\nrandomPetCalc := strings.ToLower(strings.Join([]string{adjective, animal}, \"-\"))\nfmt.Printf(\"Random Pet Calculated at Runtime: %s\\n\", randomPetCalc)\n</code></pre>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#using-in-loop","title":"Using in Loop","text":"<p>Note that this would probably have issues if you were trying to update the seed in goroutines as I believe it's a global variable. However, it works great when you need to do something like this:</p> <pre><code>for _, cluster := range clusterList {\n    suffix := cluster.PetNameSuffix()\n    err = ingress.NetworkingIngress(ctx, suffix) // ... etc\n}\n</code></pre>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#wrap-up","title":"Wrap-Up","text":"<p>I got here thanks to the help of folks in the Pulumi slack + Github discussions. I've found it's a common question. I recommended they beef up some good examples of using the random provider like this. However, I'm not certain it fits Pulumi's \"promise\" model quite the same as it was with Terraform. I'm not versed enough in the architecture to understand why it worked for Terraform but not with Pulumi, but this \"workaround\" using normal Go code seems to work fine. I'm really appreciating the value of having access to a full fledged programming language in my infrastructure work, including Kubernetes, even if this entails a little more complexity up front.</p>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"using-randomization-for-pulumi-kubernetes-resources/#further-reading","title":"Further Reading","text":"<ul> <li>Using random resource in plans with Go? \u00b7 Discussion #9207 \u00b7 pulumi/pulumi \u00b7 GitHub</li> <li>How do you approach passing around ID's for resources without it becoming a hot mess? \u00b7 Discussion #9205 \u00b7 pulumi/pulumi \u00b7 GitHub</li> <li>Unique ComponentResource Names With Random Suffix \u00b7 Discussion #9216 \u00b7 pulumi/pulumi \u00b7 GitHub</li> <li>Using ComponentResource as logical namespace \u00b7 Discussion #9250 \u00b7 pulumi/pulumi \u00b7 GitHub</li> <li>Cannot use Output as logical resource name \u00b7 Issue #5234 \u00b7 pulumi/pulumi \u00b7 GitHub</li> <li>Inputs and Outputs | Pulumi Docs</li> </ul>","tags":["tech","development","pulumi","golang","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/","title":"The Pulumi Verdict So Far","text":"","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#what-pulumi-is","title":"What Pulumi Is","text":"<ul> <li>Pulumi is like a muscle car. Great if you want full control, power, and flexibility.</li> <li>Pulumi is fast.</li> <li>Has actual loops. Sorry <code>HCL</code>... Your loops are just confusing.<sup>1</sup></li> </ul>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#what-pulumi-is-not","title":"What Pulumi Is Not","text":"<ul> <li>Pulumi is like a rally car. You will be traveling a gravel road that might result in a few bumps and potholes. It can handle it. However, there are things (just like <code>HCL</code> had/has over time) you'll run into that don't make sense or are bugs, but since it's younger there might not be immediate fixes.</li> </ul>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#when-would-i-choose-terraform-over-pulumi","title":"When Would I Choose Terraform Over Pulumi?","text":"<ul> <li>If you prefer minivans over rally cars. (j/k)</li> <li>If I want to leverage a prebuilt complex module, like those from the Terraform registry made by CloudPosse, Terraform could provide a better value for the time.<ul> <li>I hope that Pulumi eventually has a full \"Crosswalk\" support where folks begin sharing prebuilt stacks with best practices, but I feel it's an uphill road as a latecomer from Terraform.</li> </ul> </li> <li>When there is a module that provides the functionality you want, it might make sense to use it over rebuilding in Pulumi.</li> <li>If you expect no one in your org will support Pulumi, you might use it for a few things here and there, but it's a tough road unless others are interested and willing to try it out.</li> </ul>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#when-would-i-choose-pulumi-over-terraform","title":"When Would I choose Pulumi over Terraform?","text":"<ul> <li>If you lean towards \"developer\" over \"infrastructure\" engineering in your skillset, meaning you are comfortable writing in the primary languages Pulumi supports. I feel it requires a bit more coding (esp with Typed languages) understanding upfront, while <code>HCL</code> is something you can pick up without requiring general-purpose coding knowledge.</li> <li>If you are solid with Go, Python, C#, or Typescript, but not quite as advanced as <code>HCL</code>.</li> <li>If you understand infrastructure. This is key. Terraform modules tend to hold your hand and do a lot of good things for you out of the box. Pulumi is like giving you the lego pieces to build whatever you want, but not a step-by-step assembly guide.</li> <li>When you have the autonomy to select a combination of tools in your role.</li> <li>When you want to use looping constructs and other language functions without dealing with the limits of confusion of <code>HCL</code>.</li> <li>If you want to deviate from very basic <code>yaml</code> and <code>tfvar</code> inputs, then Pulumi can be more flexible with the range of libraries and built-in configuration functionality Pulumi offers.</li> <li>If you want to store encrypted secrets in your <code>yaml</code> for simplicity and velocity, Pulumi does this very elegantly.</li> <li>If you want to manage complex naming conventions, then using a struct with methods is fantastic, allowing you to enforce naming and self-document using Go's documentation functionality (and IntelliSense).</li> </ul>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#other-notes","title":"Other Notes","text":"","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#support","title":"Support","text":"<ul> <li>Pulumi: Expect delays. I believe many of those helping are not doing support in Slack or GitHub full-time. This can make the unique challenges faced in edge cases difficult to allocate time to support. I believe this would change if the enterprise support tier was engaged, so if it's an org-wide rollout, then consider this.</li> <li>Terraform: In contrast, I've had absymally low engagement from Terraform in forums and GitHub. I think both prioritize (rightly) the Enterprise clients, which leaves the lower tier subscribers a bit on their own/crowdsourced support at times. They should close their forums down and rely on GitHub unless engagement changes. The best part about Terraform, is you often don't need support since so many community members use it.</li> </ul>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"the-pulumi-verdict-so-far/#components","title":"Components","text":"<p>Components allow you to provide similar functionality to the org as Terraform modules. If you are rolling this out to an org, consider becoming familiar with this and simplify other development teams' usage by having the core components provided with all the best practices, tagging, and naming convention preset.</p> <ol> <li> <p>My top post of all time to this day is a post on using terraform <code>for_each</code>.\u00a0\u21a9</p> </li> </ol>","tags":["tech","development","pulumi","infrastructure-as-code"]},{"location":"check-azure-devops-pr-for-conventional-commit-title/","title":"Check Azure Devops Pr for Conventional Commit Title","text":"","tags":["tech","development","azure-devops","powershell","devops","pre-commit","ci"]},{"location":"check-azure-devops-pr-for-conventional-commit-title/#conventional-commit","title":"Conventional Commit","text":"<ul> <li>Is it needed... No.</li> <li>Is it the only way... No.</li> <li>Is consistency better than inconsistentcy, mostly yes.</li> </ul> <p>I've used conventional commit most of my career and it's a fine basic standard, even if sometimes it feels strange as you get used to it. I think it's far better than inconsistency and is common enough with tooling you can do some nice release, changelog, and versioning with it.</p> <p>However, getting folks used to it can be tricky. I'm a big believer in don't ask folks to remember a bunch of \"conventions\" and just codify with linters, pre-commit, and CI checks. This eliminates policing and changing a standard just becomes a PR with proposed changes.</p> <p>GitHub has a lot of actions that help check and prompt for fixes on this, but Azure DevOps doesn't.</p> <p>I created an Azure Pipeline task with a little adhoc powershell that works well and helps prompt for more consistency in the creation of consistent titles.</p>","tags":["tech","development","azure-devops","powershell","devops","pre-commit","ci"]},{"location":"check-azure-devops-pr-for-conventional-commit-title/#powershell-code","title":"PowerShell code","text":"<pre><code>try { &amp;commitlint --version } catch { npm install commitlint -g }\n\nWrite-Host \"Validating PR Title matches what's acceptable in project (should have a .commitlintrc.yml in your project to use this)...\"\n\n$result = $($ENV:PR_TITLE | commitlint)\nif ($LASTEXITCODE -eq 1) {\n  $result | Select-Object -Skip 1 |  ForEach-Object {\n    if ($_ -match '\u2716') {\n      Write-Host \"##vso[task.logissue type=error]$_\"\n    } else {\n      Write-Host \"##[info]$_\"\n    }\n  }\n  Write-Host \"##vso[task.logissue type=error]Topic should be in the form of 'type(scope): lower case title &lt; 120 characters' (please note colon after scope has no spaces)\"\n\n  exit 1\n} else {\n  Write-Host \"\u2705 PR Title looks good. Nice work! \ud83d\udc4d\"\n}\n</code></pre> <p>Plug this into a template in an dedicated azure pipelines template repository for easy reuse in many repos.</p> <pre><code>---\njobs:\n  - job: checkconventionalcommitprtitle\n    displayName: check-conventional-commit-pr-title\n    timeoutInMinutes: 5\n    cancelTimeoutInMinutes: 2\n    steps:\n      - checkout: self\n        fetchDepth: 1 # Shallow fetch to optimize performance if template repo gets larger\n      - bash: |\n          PR_TITLE=\"$(curl --silent -u azdo:$SYSTEM_ACCESSTOKEN \\\n          $(System.CollectionUri)_apis/git/repositories/$(Build.Repository.ID)/pullRequests/$(System.PullRequest.PullRequestId)?api-version=5.1 \\\n          | jq -r .title)\"\n          echo \"##vso[task.setvariable variable=Pr.Title]$PR_TITLE\"\n        env:\n          SYSTEM_ACCESSTOKEN: $(System.AccessToken)\n        displayName: get-pull-request-title\n        condition: and(succeeded(), eq(variables['Build.Reason'], 'PullRequest'))\n      - pwsh: |\n          &lt;PUT THE POWERSHELL CODE HERE&gt;\n        displayName: check-conventional-commit-pr-title\n        failOnStderr: true\n        ignoreLASTEXITCODE: true\n</code></pre> <p>To require this on a PR, you can setup a policy on merges to your trunk. I normally manage this with terraform, so here's a hint to get you started. Managing your Azure DevOps configuration, builds, and pipelines with Terraform or Pulumi is a far nicer way to keep things maintainable and scale up as more are added.</p> <p>This sets up your base configuration.</p> <pre><code>terraform {\n  required_providers {\n    azuredevops = {\n      source  = \"microsoft/azuredevops\"\n      version = \"0.2.1\"\n    }\n  }\n}\ndata \"azuredevops_project\" \"projname\" {\n  name = \"MyProjectName\"\n}\n\ndata \"azuredevops_git_repository\" \"myrepo\" {\n  project_id = data.azuredevops_project.projname.id\n  name       = \"MyRepoName\"\n}\n</code></pre> <p>Next, you'll want to register the pipeline as a valid pipeline to show up linked to the yaml.</p> <pre><code>resource \"azuredevops_build_definition\" \"ci-projname-check-pr-title\" {\n  project_id = data.azuredevops_project.projname.id\n  name       = \"ci-projname-check-pr-title\"\n  path       = \"\\\\pull-request-checks\"\n  ci_trigger {\n    use_yaml = true\n  }\n\n  repository {\n    repo_type   = \"TfsGit\"\n    repo_id     = data.azuredevops_git_repository.myrepo.id\n    branch_name = data.azuredevops_git_repository.myrepo.default_branch\n    yml_path    = \"pathto/azure-pipelines.checks.prtitle.yml\" # Match this to whatever you call your template\n  }\n}\n</code></pre> <p>Finally, you'll link your pipeline to a policy that requires the run of this to be successful to merge.</p> <pre><code>resource \"azuredevops_branch_policy_build_validation\" \"projname-check-pr-title\" {\n  project_id = data.azuredevops_project.projname.id\n\n  enabled  = true\n  blocking = true # This means to bypass you'd have to have permissions and document the override.\n\n  settings {\n    display_name        = \"\ud83e\uddea PR Title Adheres to Conventional commit\"\n    build_definition_id = azuredevops_build_definition.ci-projname-check-pr-title.id\n\n    # Set to however long before it has to be rerun. This is fine to be at a high duration as title shouldn't be changing constantly after it passes\n    valid_duration      = 720\n    filename_patterns = [\n      \"*\",\n    ]\n    scope {\n      repository_id  = data.azuredevops_git_repository.myrepo.id\n      repository_ref = data.azuredevops_git_repository.myrepo.default_branch\n      match_type     = \"Exact\"\n    }\n  }\n}\n</code></pre> <p>Definitely a bit more work than GitHub actions, but you can still get around some of this by using this approach for any PR merge validations you want. There's also PR Status policies, but they are a bit more involved, and often involve running some Azure Functions or other things to post back a status.</p> <p>In my opinion, that is worth investing in as you scale, but initially it's just too much plumbing so I just stick with fast small pipelines like this.</p> <p>Good luck! \ud83d\udc4d</p>","tags":["tech","development","azure-devops","powershell","devops","pre-commit","ci"]},{"location":"to-get-a-cup-of-coffee/","title":"To Get a Cup of Coffee","text":"<ul> <li>Take the car into shop to get state inspection that's overdue, since I don't drive it much.</li> <li>Find out a check engine light fails the test.</li> <li>Drive towards grocery store near house, and hit a long line of traffic.</li> <li>Decide to be a dilligent budgeter and pick up milk instead of buying coffee.</li> <li>Train decides to park at road I have to cross.</li> <li>Go to gas station, grab milk.</li> <li>Arrive home.</li> <li>Make coffee.</li> </ul> <p>Sometimes working from an office has it's advantages, especially those fancy places with free lattes (looking at you LinkedIn!)</p>","tags":["tech","development","working-from-home"]},{"location":"amply-repaid/","title":"Amply Repaid","text":"<p>That's how children reward you for having them. They are such a responsibility and such a miracle at the same time, that the miracle of the relationship you can have with them amply repays you for the responsibility, if you can only see it. But then you have to go into the depths and take that relationship as a sacred reality and ethical requirement. And you deal with those things casually at your great peril. - Jordan Peterson<sup>1</sup></p> <p>This was a beautiful quote that I had to capture from Jordan Peterson. It wasn't the main topic, but the discussion of religous terminology came up as the guest speaker said she was raised Catholic, but had rejected this faith. Yet, even with the rejection of faith, both her and Dr Peterson found that expressions of deep and core concepts often require borrowing from the domain of the sacred. Not only does it provide weight, but without it can be difficult to find the words to convey concepts that have more tremendous substance to them.</p> <ol> <li> <p>pocketcasts link \u21a9</p> </li> </ol>","tags":["children","microblog"]},{"location":"digital-garden-released/","title":"Digital Garden Released","text":"<p>Split my blog into notes.sheldonhull.com for evergreen note taking and reference content. I'll still maintain this blog for posts.</p> <p>Mkdocs allows a minimal friction to publishing and previewing. The directory structure is built automatically from directories.</p> <p>While I love Hugo, it's more work to maintain small reference pages so I'm going to split up my notes more like a \"digital garden\" approach.</p> <ul> <li>Free hosting through GitHub.</li> <li>60 seconds from push to page refresh.</li> </ul> <p>In comparison, Obsidian Publish and others can run $100+ a year.</p>","tags":["tech","development","microblog"]},{"location":"oreilly/","title":"O'Reilly","text":"<p>Creating a new chrome alias so my typo doesn't keep annoying me. I'm lucky enough to have access to O'Reilly Books through <code>$work</code>.</p> <p>Problem is everytime I google it I keep letting my autocomplete show me O'Reilly Auto. \ud83d\ude04</p>","tags":["tech","development","microblog"]},{"location":"refined-github/","title":"Refined Github","text":"<p>After using this for a while, I figured I'd add this to my my \"cool-tools\" list.</p> <p></p> <p>The main features I've found useful:</p> <ul> <li>Use keyboard to flip straight to code review with approve/reject focused.</li> <li>Highlight the code most recently changed in the browser.</li> </ul> <p>There's a whole list on the repo page of small usability tweaks for those living in GitHub, for example Repositories Tweaks.</p> <p>Give it a try!</p>","tags":["tech","development","microblog","cool-tools"]},{"location":"how-many-commits-did-you-make-in-last-month/","title":"How Many Commits Did You Make in Last Month","text":"<p>Been playing with opencommit and wanted to assess how much using it on every single commit I still could track locally might cost me.</p> <p>This script loops through all the <code>${HOME}/git</code> directories and totals the commits made in last month.</p> <p>Looks like using OpenAI's api would cost me about $4-$5 if I'd used on every single commit and the size of the commits was smaller.</p> <p>Not as bad as I'd thought it would be.</p> <pre><code>#!/usr/bin/env bash\n# Use https://github.com/sharkdp/fd\nset -e\n# Initialize variables\ntotal_commits=0\nusername=$(whoami)\n\n# Capture the directories found by fd to a variable\ndirectories=$(fd -H --max-depth 6 \"^\\.git$\" \"$HOME/git\")\n\n# Loop through each directory\n\nfor directory in $directories; do\n    # Change to the git directory\n    printf \"...\u2699\ufe0f $(dirname $directory)\" &amp;&amp;\n    pushd \"$(dirname \"$directory\")\" &amp;&amp;\n    # Get the repo name\n    repo=$(basename \"$PWD\") &amp;&amp;\n    # Get the count of commits made by a user with the partial name match of \"username\"\n    commits=$(git log \\\n                --branches \\\n                --author=\".*${username}.*\" \\\n                --since=\"1 month ago\" \\\n                --no-merges \\\n                --format=\"%H\" | wc -l | awk \"{print \\$1}\"\n    ) &amp;&amp;\n    # Output the repo name and count of commits\n    printf \"\\t%-50s %-50s \\n\" \"$repo\", \"$commits\" &amp;&amp;\n    # Add to the total count of commits\n    total_commits=$((total_commits + commits)) &amp;&amp;\n    popd\ndone\n\n# Output the total count of commits\necho \"Total commits: $total_commits\"\n</code></pre>","tags":["tech","development","microblog","openai","shell"]},{"location":"brain-noodles/","title":"Brain Noodles","text":"<p>I'm sitting here singing: \"I've got brain noodles in my noggin\" and I'm not sure if that's normal.</p> <p></p>","tags":["microblog"]},{"location":"alexa-customization/","title":"Alexa Customization","text":"<p>This bugged me for a while, as it random notifications were intrusive in when I was working.</p> <ul> <li>Remove the marking notications by going to Alexa app, settings, notifications, and then turn off any categories you don't want to be annoyed with.</li> </ul>","tags":["tech"]},{"location":"2023/","title":"2023","text":""},{"location":"2022/","title":"2022","text":""},{"location":"2021/","title":"2021","text":""},{"location":"2020/","title":"2020","text":""},{"location":"2019/","title":"2019","text":""},{"location":"2018/","title":"2018","text":""},{"location":"2017/","title":"2017","text":""},{"location":"2016/","title":"2016","text":""},{"location":"2015/","title":"2015","text":""},{"location":"2014/","title":"2014","text":""},{"location":"2013/","title":"2013","text":""},{"location":"page/2/","title":"posts","text":""},{"location":"page/3/","title":"posts","text":""},{"location":"page/4/","title":"posts","text":""},{"location":"page/5/","title":"posts","text":""},{"location":"page/6/","title":"posts","text":""},{"location":"page/7/","title":"posts","text":""},{"location":"page/8/","title":"posts","text":""},{"location":"page/9/","title":"posts","text":""},{"location":"page/10/","title":"posts","text":""},{"location":"page/11/","title":"posts","text":""},{"location":"page/12/","title":"posts","text":""},{"location":"page/13/","title":"posts","text":""},{"location":"page/14/","title":"posts","text":""},{"location":"page/15/","title":"posts","text":""},{"location":"page/16/","title":"posts","text":""},{"location":"page/17/","title":"posts","text":""},{"location":"page/18/","title":"posts","text":""},{"location":"page/19/","title":"posts","text":""},{"location":"page/20/","title":"posts","text":""},{"location":"page/21/","title":"posts","text":""},{"location":"page/22/","title":"posts","text":""},{"location":"page/23/","title":"posts","text":""},{"location":"page/24/","title":"posts","text":""},{"location":"page/25/","title":"posts","text":""},{"location":"page/26/","title":"posts","text":""},{"location":"page/27/","title":"posts","text":""},{"location":"page/28/","title":"posts","text":""},{"location":"page/29/","title":"posts","text":""},{"location":"page/30/","title":"posts","text":""},{"location":"page/31/","title":"posts","text":""},{"location":"page/32/","title":"posts","text":""},{"location":"page/33/","title":"posts","text":""},{"location":"page/34/","title":"posts","text":""},{"location":"page/35/","title":"posts","text":""},{"location":"page/36/","title":"posts","text":""},{"location":"page/37/","title":"posts","text":""},{"location":"2022/page/2/","title":"2022","text":""},{"location":"2021/page/2/","title":"2021","text":""},{"location":"2021/page/3/","title":"2021","text":""},{"location":"2021/page/4/","title":"2021","text":""},{"location":"2021/page/5/","title":"2021","text":""},{"location":"2021/page/6/","title":"2021","text":""},{"location":"2021/page/7/","title":"2021","text":""},{"location":"2021/page/8/","title":"2021","text":""},{"location":"2021/page/9/","title":"2021","text":""},{"location":"2021/page/10/","title":"2021","text":""},{"location":"2021/page/11/","title":"2021","text":""},{"location":"2020/page/2/","title":"2020","text":""},{"location":"2020/page/3/","title":"2020","text":""},{"location":"2020/page/4/","title":"2020","text":""},{"location":"2020/page/5/","title":"2020","text":""},{"location":"2020/page/6/","title":"2020","text":""},{"location":"2020/page/7/","title":"2020","text":""},{"location":"2020/page/8/","title":"2020","text":""},{"location":"2019/page/2/","title":"2019","text":""},{"location":"2019/page/3/","title":"2019","text":""},{"location":"2017/page/2/","title":"2017","text":""},{"location":"2017/page/3/","title":"2017","text":""},{"location":"2017/page/4/","title":"2017","text":""},{"location":"2016/page/2/","title":"2016","text":""},{"location":"2016/page/3/","title":"2016","text":""},{"location":"2016/page/4/","title":"2016","text":""},{"location":"2016/page/5/","title":"2016","text":""},{"location":"2016/page/6/","title":"2016","text":""},{"location":"2015/page/2/","title":"2015","text":""},{"location":"2015/page/3/","title":"2015","text":""},{"location":"2013/page/2/","title":"2013","text":""}]}